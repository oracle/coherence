///////////////////////////////////////////////////////////////////////////////
    Copyright (c) 2000, 2021, Oracle and/or its affiliates.

    Licensed under the Universal Permissive License v 1.0 as shown at
    http://oss.oracle.com/licenses/upl.
///////////////////////////////////////////////////////////////////////////////
= Durable Events
:description: A feature that ensures events are never lost
:keywords: coherence, events, persistence, documentation

// DO NOT remove this header - it might look like a duplicate of the header above, but
// both they serve a purpose, and the docs will look wrong if it is removed.
== Durable Events

Coherence has provided the ability for clients to asynchronously observe data
changes for almost two decades. This has proven to be an incredibly powerful
feature allowing Coherence to offer components such as
{commercial-docs-base-url}/develop-applications/introduction-coherence-caches.html#GUID-5C066CC9-575F-4D7D-9D53-7BB674D69FD1[NearCaches]
and {commercial-docs-base-url}/develop-applications/introduction-coherence-caches.html#GUID-0A4E65E0-1E92-4B8B-8681-AD9A8CA6D06D[ViewCaches/CQCs],
in addition to allowing customers to build truly event driven systems.

A comprehensive overview of MapEvents in terms of the call back interface in
addition to the registration mechanisms is provided in the
{commercial-docs-base-url}/develop-applications/using-map-events.html[official documentation].
However, it is worth drawing attention to some of the guarantees offered by MapEvents
to provide context of why the DurableEvents feature is useful.

=== MapEvent Guarantees

Coherence guarantees a `MapEvent`, which represents a change to an `Entry` in the
source `Map`, will be delivered exactly once given the client remains a member
of the associated service.

[NOTE]
====
Each `NamedMap` is associated to a `Service` and typically this service is a `PartitionedService`
therefore provides distributed storage and partitioned/sharded access. The remaining
description of `MapEvent` guarantees will assume the use of a `PartitionedService`
and therefore providing resilience to process, machine, rack, or site failure.
====

Importantly this guarantee is maintained regardless of failures in the system
that the services are configured to handle. Therefore a `backup-count` of 1 results
in the service being able to tolerate the loss of a single unit where unit could
be a node (JVM/member), machine, rack or site. Upon encountering a fault Coherence
will restore data and continue service for the affected partitions. This data
redundancy is extended to MapEvent delivery to clients. Therefore if a member
hosting primary partitions was to die, and said member had sent the backup message
for some change but failed to deliver the MapEvent to the client, the new primary
member (that was a backup member and went through the automatic promotion protocol)
would emit MapEvent messages that had not been confirm by clients. The client
is aware of MapEvent messages it had already processed, therefore if the MapEvent
was received by the client but the primary did not receive the ACK thus causing
the backup to send a duplicate, the client will simply not replay the event.

=== Raison D'etre

This level of redundancy and guarantee is sufficient for many applications and
users of Coherence have been able to live with/workaround a particular shortcoming
such that the product has not attempted to address it.

The aforementioned guarantee of MapEvent delivery are all valid under the assumption
that the member, that has registered for MapEvents, does *not* leave the service.
If it does leave the associated service then these guarantees no longer apply.
This can be problematic under a few contexts (described below) and has led to
the need deliver a more general feature of event replay.

==== Abnormal Service Termination

While it is rare, there are some scenarios that result in abnormal service
termination. This has affected Coherence users by causing references to ``Service``s
or ``NamedMap``s to become invalid and therefore unusable. Instead of having
numerous applications, and internal call sites, be defensive to these invalid handles
Coherence chose to introduce a 'safe' layer between the call site and the raw/internal
service. The 'safe' layer remains valid when services abnormally terminate and
additionally may cause the underlying service to restart.

In the case of a 'safe' `NamedMap` any `MapEvent` listeners registered will automatically
be re-registered. However, any events that had occurred after the member left
the service and before it re-joined will be lost, or more formally not observed
by this member's listener. This has been worked around in many cases by these
members/clients re-synchronizing their local state with the remote data; this
is the approach taken for ``ContinuousQueryCache``s.

==== Extend Proxy Failover

Coherence Extend provides a means for a client to connect to a cluster via a
conduit referred to as a proxy. An extend client wraps up the intended request,
forwards to a proxy which executes said request with the results either streamed
back to the client or sent as a single response. There are many reason why one
would chose using extend over being a member of the cluster; further documentation
on extend can be found {commercial-docs-base-url}/develop-remote-clients/introduction-coherenceextend.html[here].

The liveness of the proxy is incredibly important to the extend clients that
are connected to it. If the proxy does become unresponsive, or simply dies the
extend client will transparently reconnect to another proxy. Similar to the
aforementioned 'abnormal service termination' use case there is a potential of
not observing MapEvents that had occurred at the source due to the proxy leaving
the associated service or the extend client reconnecting to a different proxy
and therefore re-registering the listener.

Once again, there are means to workaround this situation by observing the proxy
disconnect / re-connect and causing a re-synchronization of extend client and
the source. However, extend proxy failover is a significantly more likely event
and has been raised by Coherence users.

==== Process Death

A logical client receiving MapEvents may experience a process restart and it
may be desirable to continue receiving MapEvents after the last received event,
opposed to only events after registration. For example, a client may be responsible
for updating some auxiliary system by applying the contents of each MapEvent
to that system. Additionally it may be tracking the last received events and
therefore _could_ inform the source of the last event it received. A capable
source could replay all events that were missed.

=== Generic Event Replay

In considering how to address these various scenarios that result in event loss
it became evident that the product needs an ability to retain events beyond
receipt of delivery such that they can be replayed on request. This pushes
certain requirements on different parts of the system:

* storage nodes
** [x] must track, by storing, the ``MapEvent``s as they are generated
** [x] expose a monotonically increasing version per partition within a single cache
** [x] ensure version semantics are maintained regardless of faults
** [ ] [TODO] ensure `MapEvent` storage is redundant
** [ ] [TODO] provide a `MapEvent` storage retention policy
* client nodes
** [x] have a trivial facility to suggest received `MapEvent` versions are tracked
and therefore events replayed when faced with restart
** [x] provide a more advanced means such that the client can control the tracking
of event versions

With the above a `MapListener` can opt in by suggesting they are version aware
and Coherence will automatically start tracking versions on the client. This
does require a complementing server-side configuration in which the storage servers
are tracking ``MapEvent``s. For example:

1. Start a storage server that is tracking events:
[source,bash]
----
$ java -Dcoherence.distributed.persistence.events.dir=/tmp/events-dir -jar coherence.jar
----
2. Start a client that registers a version aware `MapEvent` listener. A snippet
taken from a functional test in the repo has been provided below:
[source,java]
----
include::../../test/functional/persistence/src/test/java/persistence/events/DurableEventsTests.java[tag=simple-registration]
----

The above registration results in the client receiving events and tracking the
latest version per partition. Upon abnormal service restart the client will
automatically re-register itself and request versions after the last received
version.

More advanced clients can implement the {javadoc-root}/com/tangosol/net/partition/VersionAwareMapListener.html[VersionAwareMapListener]
interface directly. Implementors must return an implementation of {javadoc-root}/com/tangosol/net/partition/VersionedPartitions.html[VersionedPartitions]
which will be interrogated by Coherence upon registration either directly due
to a call to `NamedMap.addMapListener` or indirectly due to a service restart.
These versions are sent to the relevant storage servers and if a version is returned for
a partition Coherence will return all known versions larger than the specified
version. Additionally, certain formal constants are defined to allow a client
to request storage servers to send:

* all known events
* current head and all future events (previously known as priming events)
* all future events (the current behavior)

[NOTE]
====
There is a natural harmony between the registration mechanism and the partitions
returned from `VersionedPartitions` that occurs but is worth noting.
For example, when registering a `MapListener` against a specific key only ``MapEvent``s
for said key will be received by this `MapListener` and therefore only versions
for the associated partition will be tracked. The `VersionedPartitions` returned
by this `VersionAwareMapListener` will only return a version for a single partition.
However this is worth being aware of if you do implement your own `VersionAwareMapListener`
or `VersionedPartitions` data structure.
====

=== Production Ready?

This feature is NOT production ready and we do NOT recommend using it in production
at this point. There are some features that we believe are required prior to
this feature graduating to production ready status that we will detail below
for transparency. However, we are making this feature available in its current
form to garner feedback from the community prior to locking down the APIs and
semantics.

The following improvements will be required prior to this feature being production
ready:

* Redundant MapEvent storage
* MapEvent retention policy
* MapEvent delivery flow control
* Extend and gRPC client support
* Snapshots of MapEvent storage
* Allow a logical client to store its `VersionedPartitions` in Coherence
* Monitoring metrics

=== Get Started

To get started please take a look at our guides and tutorials.