{
    "docs": [
        {
            "location": "/examples/guides/460-topics/README",
            "text": " What You Will Build What You Need Review the Initial Project Maven Configuration Data Model Topics Cache Configuration The Chat Application Build and Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " You will review, build and run a simple chat client which showcases using Coherence Topics. When running the chat client, the user can send a message in two ways: Send to all connected users using a publish/ subscribe model. For this functionality we create a topic called public-messages and all users are anonymous subscribers. Any messages to this topic will only be received by subscribers that are active. Send a private message to an individual user using a subscriber group. This uses a separate topic called private-messages and each subscriber to the topic specifies their userId as a subscriber group. Each value is only delivered to one of its subscriber group members, meaning the message will only be received by the individual user. We do not cover all features in Coherence Topics, so if you wish to read more about Coherence Topics, please see the Coherence Documentation . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; ",
            "title": "Maven Configuration"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; ",
            "title": "Data Model"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. ",
            "title": "Topics Cache Configuration"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Publisher for public messages. */ private final Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private final Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); if (coherence == null) { Coherence.clusterMember().start().join(); coherence = Coherence.getInstance(); } Session session = coherence.getSession(); NamedTopic&lt;ChatMessage&gt; publicTopic = session.getTopic(\"public-messages\"); NamedTopic&lt;ChatMessage&gt; privateTopic = session.getTopic(\"private-messages\"); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish public messages publisherPublic = publicTopic.createPublisher(); // create a subscriber to receive public messages subscriberPublic = publicTopic.createSubscriber(); Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish private messages publisherPrivate = privateTopic.createPublisher(); // create a subscriber to receive private messages subscriberPrivate = privateTopic.createSubscriber(inGroup(userId)); Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handle((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handle((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); getMessageLog(chatMessage) .ifPresent(message -&gt; { messagesReceived.incrementAndGet(); log(message); }); element.commit(); subscriber.receive().handle((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation of the message and display it Commit the element so that we do not receive the message again Receive the next message Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.publish(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (publisherPublic.isActive()) { publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); } if (subscriberPublic.isActive()) { subscriberPublic.close(); } if (publisherPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); } if (subscriberPrivate.isActive()) { subscriberPrivate.close(); } } ",
            "title": "The Chat Application"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Data Model The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; Topics Cache Configuration The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. The Chat Application The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Publisher for public messages. */ private final Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private final Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); if (coherence == null) { Coherence.clusterMember().start().join(); coherence = Coherence.getInstance(); } Session session = coherence.getSession(); NamedTopic&lt;ChatMessage&gt; publicTopic = session.getTopic(\"public-messages\"); NamedTopic&lt;ChatMessage&gt; privateTopic = session.getTopic(\"private-messages\"); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish public messages publisherPublic = publicTopic.createPublisher(); // create a subscriber to receive public messages subscriberPublic = publicTopic.createSubscriber(); Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish private messages publisherPrivate = privateTopic.createPublisher(); // create a subscriber to receive private messages subscriberPrivate = privateTopic.createSubscriber(inGroup(userId)); Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handle((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handle((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); getMessageLog(chatMessage) .ifPresent(message -&gt; { messagesReceived.incrementAndGet(); log(message); }); element.commit(); subscriber.receive().handle((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation of the message and display it Commit the element so that we do not receive the message again Receive the next message Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.publish(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (publisherPublic.isActive()) { publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); } if (subscriberPublic.isActive()) { subscriberPublic.close(); } if (publisherPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); } if (subscriberPrivate.isActive()) { subscriberPrivate.close(); } } ",
            "title": "Review the Initial Project"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Build the project using either of the following: <markup lang=\"bash\" >./mvnw clean package or <markup lang=\"bash\" >./gradlew clean build Start one or more Coherence Cache Servers using the following: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer Start the first chat client with the user Tim <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar Tim or <markup lang=\"bash\" >./gradlew runClient -PuserId=Tim --console=plain You will notice output similar to the following: <markup lang=\"bash\" >Oracle Coherence Version 20.12 Build demo Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. User: Tim Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (Tim)&gt; Start a second second client with the name Helen . You will see a message on Tim&#8217;s chat application indicating Helen has joined the chat. <markup lang=\"bash\" >Chat (Tim)&gt; 14:14:30 Helen joined the chat Use send hello from Helen&#8217;s chat and you will notice that the message is dispalyed on Tim&#8217;s chat. To show how subscriber groups work, send a private message using the following from Tim to JK . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK Hello JK Also send a private message to admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Please ping me when you get in as i have an issue with my Laptop Start a third chat application with JK as the user: <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar JK User: JK Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (JK)&gt; You will notice that the private message for JK was not delivered as the subscriber group JK was only created when he joined and therefore messages send previously are not stored. You will also see join messages on the other terminals. Type quit in Helen&#8217;s terminal and restart the client as admin <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar admin User: admin Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (admin)&gt; 14:18:29 Tim (Private) - Please ping me when you get in as i have an issue with my Laptop You will notice that the message sent before admin joined is now delivered as the admin subscriber group was created in configuration and add on server startup. Type a message send Got to go, bye on JK&#8217;s chat application and then quit . The message along with the leave notification will be shown on the other terminals. <markup lang=\"bash\" >Chat (JK)&gt; send Got to go, bye Now that JK has quit the application, send a private message from Tim to JK using sendpm JK please ping me . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK please ping me Start the client as JK and you will see the message displayed now as the subscriber group is created. Finally send a private messge from Tim to admin using sendpm admin Are you free for lunch? . You will notice this message is only displayed for admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Are you free for lunch? ",
            "title": "Build and Run the Example"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " In this tutorial you have learned how use Coherence Topics. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Topics Overview and Configuration Performing Topics Operations ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " This tutorial walks through the steps to use Coherence Topics using a simple Chat Application. Table of Contents What You Will Build What You Need Review the Initial Project Maven Configuration Data Model Topics Cache Configuration The Chat Application Build and Run the Example Summary See Also What You Will Build You will review, build and run a simple chat client which showcases using Coherence Topics. When running the chat client, the user can send a message in two ways: Send to all connected users using a publish/ subscribe model. For this functionality we create a topic called public-messages and all users are anonymous subscribers. Any messages to this topic will only be received by subscribers that are active. Send a private message to an individual user using a subscriber group. This uses a separate topic called private-messages and each subscriber to the topic specifies their userId as a subscriber group. Each value is only delivered to one of its subscriber group members, meaning the message will only be received by the individual user. We do not cover all features in Coherence Topics, so if you wish to read more about Coherence Topics, please see the Coherence Documentation . What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Review the Initial Project Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Data Model The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; Topics Cache Configuration The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. The Chat Application The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Publisher for public messages. */ private final Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private final Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); if (coherence == null) { Coherence.clusterMember().start().join(); coherence = Coherence.getInstance(); } Session session = coherence.getSession(); NamedTopic&lt;ChatMessage&gt; publicTopic = session.getTopic(\"public-messages\"); NamedTopic&lt;ChatMessage&gt; privateTopic = session.getTopic(\"private-messages\"); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish public messages publisherPublic = publicTopic.createPublisher(); // create a subscriber to receive public messages subscriberPublic = publicTopic.createSubscriber(); Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish private messages publisherPrivate = privateTopic.createPublisher(); // create a subscriber to receive private messages subscriberPrivate = privateTopic.createSubscriber(inGroup(userId)); Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handle((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handle((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); getMessageLog(chatMessage) .ifPresent(message -&gt; { messagesReceived.incrementAndGet(); log(message); }); element.commit(); subscriber.receive().handle((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation of the message and display it Commit the element so that we do not receive the message again Receive the next message Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.publish(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (publisherPublic.isActive()) { publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); } if (subscriberPublic.isActive()) { subscriberPublic.close(); } if (publisherPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); } if (subscriberPrivate.isActive()) { subscriberPrivate.close(); } } Build and Run the Example Build the project using either of the following: <markup lang=\"bash\" >./mvnw clean package or <markup lang=\"bash\" >./gradlew clean build Start one or more Coherence Cache Servers using the following: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer Start the first chat client with the user Tim <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar Tim or <markup lang=\"bash\" >./gradlew runClient -PuserId=Tim --console=plain You will notice output similar to the following: <markup lang=\"bash\" >Oracle Coherence Version 20.12 Build demo Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. User: Tim Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (Tim)&gt; Start a second second client with the name Helen . You will see a message on Tim&#8217;s chat application indicating Helen has joined the chat. <markup lang=\"bash\" >Chat (Tim)&gt; 14:14:30 Helen joined the chat Use send hello from Helen&#8217;s chat and you will notice that the message is dispalyed on Tim&#8217;s chat. To show how subscriber groups work, send a private message using the following from Tim to JK . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK Hello JK Also send a private message to admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Please ping me when you get in as i have an issue with my Laptop Start a third chat application with JK as the user: <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar JK User: JK Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (JK)&gt; You will notice that the private message for JK was not delivered as the subscriber group JK was only created when he joined and therefore messages send previously are not stored. You will also see join messages on the other terminals. Type quit in Helen&#8217;s terminal and restart the client as admin <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar admin User: admin Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (admin)&gt; 14:18:29 Tim (Private) - Please ping me when you get in as i have an issue with my Laptop You will notice that the message sent before admin joined is now delivered as the admin subscriber group was created in configuration and add on server startup. Type a message send Got to go, bye on JK&#8217;s chat application and then quit . The message along with the leave notification will be shown on the other terminals. <markup lang=\"bash\" >Chat (JK)&gt; send Got to go, bye Now that JK has quit the application, send a private message from Tim to JK using sendpm JK please ping me . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK please ping me Start the client as JK and you will see the message displayed now as the subscriber group is created. Finally send a private messge from Tim to admin using sendpm admin Are you free for lunch? . You will notice this message is only displayed for admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Are you free for lunch? Summary In this tutorial you have learned how use Coherence Topics. See Also Topics Overview and Configuration Performing Topics Operations ",
            "title": "Topics"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " What You Will Build What You Need Building The Example Code The Power of CohQL Query Caches Programmatically Create the Test Class Bootstrap Coherence Filter ValueExtractor Aggregate Results Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " The example code is written as a set of unit tests, showing you how can simply executed sophisticated queries against your Coherence data. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " Before we start querying caches programmatically, you should be aware of the power of the Coherence Query Language (CohQL). CohQL is inspired by SQL and is a quick and easy way to interact with your caches. Commonly it is used as a command-line tool. Let&#8217;s assume we have a cache called countries that contains a map of Country classes with the 2-letter country code being the key of each cache entry. The Country class will have some basic properties such as name , capital and population . The simplest CohQL query you could write is a query that will return all countries is: <markup lang=\"sql\" >select * from countries As you can see, if you&#8217;re familiar with SQL, you will feel right at home. And of course from here we can make the query more sophisticated. For detailed information on how to use CohQL, please visit the chapter Using Coherence Query Language in the Coherence reference guide. In order to give you a way experiment with the cache using CohQL, we provide a simple sample app that pre-populates a Coherence cache countries wih countries and starts the CohQL Console. To get started, execute com.oracle.coherence.guides.queries.StartCohQLConsole . Typically, you would want to start the CohQL Console as a stand-alone application. Please see the following instructions to learn more. Once the console application is started, let&#8217;s execute: <markup lang=\"sql\" >select * from countries The result should be a list of 5 countries: <markup lang=\"java\" >Results Country{name='Colombia', capital='Bogotá', population=50.4} Country{name='Australia', capital='Canberra', population=26.0} Country{name='Ukraine', capital='Kyiv', population=41.2} Country{name='France', capital='Paris', population=67.4} Country{name='Germany', capital='Berlin', population=83.2} What if you would like to just retrieve the list of capitals? We can achieve that by selecting just the capital: <markup lang=\"sql\" >select capital from countries which yields: <markup lang=\"java\" >Results \"Bogotá\" \"Paris\" \"Canberra\" \"Kyiv\" \"Berlin\" Of course, you can also apply where clauses to further limit the results. For example, if you like to retrieve the countries with a population that is greater than 60 million you may add the following where clause: <markup lang=\"sql\" >select capital from countries c where population &gt; 60.0 which results in: <markup lang=\"java\" >Results \"Paris\" \"Berlin\" Another option is to aggregate results. For example, let&#8217;s calculate the total population of countries with a population larger than 60 million: <markup lang=\"sql\" >select sum(population) from countries c where population &gt; 60.0 which yields a value of 150.6 . CohQL is not merely a tool for query caches. It can also be used to create and delete caches, to insert , delete and update cache value, to create indices and more. For more information please see the official reference documentation. ",
            "title": "The Power of CohQL"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " So how would we create queries programmatically to retrieve the same results? The key here is to understand the following concepts: Filter ValueExtractor Aggregator ",
            "title": "Query Caches Programmatically"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence 20.12 . As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We will also populate the cache with several countries and thus let&#8217;s create a small helper class CoherenceHelper : <markup lang=\"java\" >public static void startCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); Session session = coherence.getSession(); NamedCache&lt;String, Country&gt; countries = session.getCache(\"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); countries.put(\"fr\", new Country(\"France\", \"Paris\", 67.4)); countries.put(\"ua\", new Country(\"Ukraine\", \"Kyiv\", 41.2)); countries.put(\"co\", new Country(\"Colombia\", \"Bogotá\", 50.4)); countries.put(\"au\", new Country(\"Australia\", \"Canberra\", 26)); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Obtain the default Session Get the countries cache Populate the countries cache with several new Country instances We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide . <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { CoherenceHelper.startCoherence(); } Call CoherenceHelper and start the Coherence instance and populate the country data. Lastly, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different querying operations. ",
            "title": "Bootstrap Coherence"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " To get started, we would like to retrieve all countries that have a population of more than 60 million people. For that we will use a Filter : <markup lang=\"java\" >@Test void testGreaterEqualsFilter() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); final Set&lt;Map.Entry&lt;String, Country&gt;&gt; results = map.entrySet(filter); assertThat(results).hasSize(2); map.entrySet(filter).forEach(entry -&gt; { assertThat(entry.getKey()).containsAnyOf(\"de\", \"fr\"); assertThat(entry.getValue().getPopulation()).isGreaterThan(60.0); }); } Get the countries Map We create a Filter that will select only countries with more than 60 million people using the Filters helper class via Filters.greaterEqual . Apply the Filter by invoking entrySet(filter) on the Map The result should be 2 countries only Assert that only France and Germany were selected The best practice for ValueExtractors is to use the method reference, e.g. Country::getPopulation , to extract falues as this provides compile time type checking. ",
            "title": "Filter"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " What if we don&#8217;t want to return Countries but just the collection of country names for which the population is 60 million people or higher? This is where we can use a ValueExtractor in combination with a ReducerAggregator . A value extractor is used to extract a property from a given object. In most instances developers would use the ReflectionExtractor as an implementation. The ReducerAggregator on the other hand, is used to run a ValueExtractor against cache entries, and it returns the extracted value. The result returned by the ReducerAggregator is a Map where the key is the key of the cache entry and the value is the extracted value. <markup lang=\"java\" >@Test void testValueExtractor() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); ReducerAggregator&lt;String, Country, Country, String&gt; aggregator = new ReducerAggregator&lt;&gt;(Country::getName); Map&lt;String, String&gt; result = map.aggregate(filter, aggregator); result.forEach((key, value) -&gt; { assertThat(key).containsAnyOf(\"de\", \"fr\"); assertThat(value).containsAnyOf(\"Germany\", \"France\"); }); } Get the countries Map We create the same filter as in the previous test (Select countries with more than 60 million people, only) Create a ReducerAggregator instance and specify that we only want the name of the countries returned Apply the Filter and Aggregator Verify that only the two country names France and Germany are returned as filtered values ",
            "title": "ValueExtractor"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " What if we want to group queried data together? Let&#8217;s query for countries, where the population is greater than 60 million but instead of returning the countries, we will return the sum of the population of thsoe 2 countries instead. <markup lang=\"java\" >@Test void testAggregate() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); BigDecimalSum&lt;BigDecimal&gt; aggregator = new BigDecimalSum&lt;&gt;(\"getPopulation\"); BigDecimal result = map.aggregate(filter, aggregator); String resultAsString = result.setScale(2, RoundingMode.HALF_UP) .stripTrailingZeros() .toPlainString(); assertThat(resultAsString).isEqualTo(\"150.6\"); } Get the countries Map We create the same filter as in the previous test (Select countries with more than 60 million people, only) We will use a different Aggregator . BigDecimalSum will aggregate the population and return a Bigecimal value. This shows that you can use a method name (not recommended) as well as method reference Apply the Filter and Aggregator For assertion purposes we will convert the BigDecimal value to a String The generated String shall not have any trailing zeros Return the String Verify that the returned value is 150.6 To learn much more about built-in Aggregators, please take a look at the respective guide . ",
            "title": "Aggregate Results"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " The first step is to create the test class that will show and test the various query operations, we&#8217;ll call this class QueryTests . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class QueryTests { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence 20.12 . As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We will also populate the cache with several countries and thus let&#8217;s create a small helper class CoherenceHelper : <markup lang=\"java\" >public static void startCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); Session session = coherence.getSession(); NamedCache&lt;String, Country&gt; countries = session.getCache(\"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); countries.put(\"fr\", new Country(\"France\", \"Paris\", 67.4)); countries.put(\"ua\", new Country(\"Ukraine\", \"Kyiv\", 41.2)); countries.put(\"co\", new Country(\"Colombia\", \"Bogotá\", 50.4)); countries.put(\"au\", new Country(\"Australia\", \"Canberra\", 26)); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Obtain the default Session Get the countries cache Populate the countries cache with several new Country instances We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide . <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { CoherenceHelper.startCoherence(); } Call CoherenceHelper and start the Coherence instance and populate the country data. Lastly, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different querying operations. Filter To get started, we would like to retrieve all countries that have a population of more than 60 million people. For that we will use a Filter : <markup lang=\"java\" >@Test void testGreaterEqualsFilter() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); final Set&lt;Map.Entry&lt;String, Country&gt;&gt; results = map.entrySet(filter); assertThat(results).hasSize(2); map.entrySet(filter).forEach(entry -&gt; { assertThat(entry.getKey()).containsAnyOf(\"de\", \"fr\"); assertThat(entry.getValue().getPopulation()).isGreaterThan(60.0); }); } Get the countries Map We create a Filter that will select only countries with more than 60 million people using the Filters helper class via Filters.greaterEqual . Apply the Filter by invoking entrySet(filter) on the Map The result should be 2 countries only Assert that only France and Germany were selected The best practice for ValueExtractors is to use the method reference, e.g. Country::getPopulation , to extract falues as this provides compile time type checking. ValueExtractor What if we don&#8217;t want to return Countries but just the collection of country names for which the population is 60 million people or higher? This is where we can use a ValueExtractor in combination with a ReducerAggregator . A value extractor is used to extract a property from a given object. In most instances developers would use the ReflectionExtractor as an implementation. The ReducerAggregator on the other hand, is used to run a ValueExtractor against cache entries, and it returns the extracted value. The result returned by the ReducerAggregator is a Map where the key is the key of the cache entry and the value is the extracted value. <markup lang=\"java\" >@Test void testValueExtractor() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); ReducerAggregator&lt;String, Country, Country, String&gt; aggregator = new ReducerAggregator&lt;&gt;(Country::getName); Map&lt;String, String&gt; result = map.aggregate(filter, aggregator); result.forEach((key, value) -&gt; { assertThat(key).containsAnyOf(\"de\", \"fr\"); assertThat(value).containsAnyOf(\"Germany\", \"France\"); }); } Get the countries Map We create the same filter as in the previous test (Select countries with more than 60 million people, only) Create a ReducerAggregator instance and specify that we only want the name of the countries returned Apply the Filter and Aggregator Verify that only the two country names France and Germany are returned as filtered values Aggregate Results What if we want to group queried data together? Let&#8217;s query for countries, where the population is greater than 60 million but instead of returning the countries, we will return the sum of the population of thsoe 2 countries instead. <markup lang=\"java\" >@Test void testAggregate() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); BigDecimalSum&lt;BigDecimal&gt; aggregator = new BigDecimalSum&lt;&gt;(\"getPopulation\"); BigDecimal result = map.aggregate(filter, aggregator); String resultAsString = result.setScale(2, RoundingMode.HALF_UP) .stripTrailingZeros() .toPlainString(); assertThat(resultAsString).isEqualTo(\"150.6\"); } Get the countries Map We create the same filter as in the previous test (Select countries with more than 60 million people, only) We will use a different Aggregator . BigDecimalSum will aggregate the population and return a Bigecimal value. This shows that you can use a method name (not recommended) as well as method reference Apply the Filter and Aggregator For assertion purposes we will convert the BigDecimal value to a String The generated String shall not have any trailing zeros Return the String Verify that the returned value is 150.6 To learn much more about built-in Aggregators, please take a look at the respective guide . ",
            "title": "Create the Test Class"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " In this guide we showed how easy it is to query Coherence caches either using CohQL or programmatically using Filters, ValueExtractors and Aggregators. Please see the Coherence reference guide, specifically the chapter Querying Data In a Cache for more details. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " Using Coherence Query Language Querying Data In a Cache ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " This guide walks you through the basic concepts of querying Coherence caches. We will provide a quick overview and examples of using Coherence Query Language (CohQL) before learning more about Filters , ValueExtractors and Aggregators to query caches programmatically. Table of Contents What You Will Build What You Need Building The Example Code The Power of CohQL Query Caches Programmatically Create the Test Class Bootstrap Coherence Filter ValueExtractor Aggregate Results Summary See Also What You Will Build The example code is written as a set of unit tests, showing you how can simply executed sophisticated queries against your Coherence data. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build The Power of CohQL Before we start querying caches programmatically, you should be aware of the power of the Coherence Query Language (CohQL). CohQL is inspired by SQL and is a quick and easy way to interact with your caches. Commonly it is used as a command-line tool. Let&#8217;s assume we have a cache called countries that contains a map of Country classes with the 2-letter country code being the key of each cache entry. The Country class will have some basic properties such as name , capital and population . The simplest CohQL query you could write is a query that will return all countries is: <markup lang=\"sql\" >select * from countries As you can see, if you&#8217;re familiar with SQL, you will feel right at home. And of course from here we can make the query more sophisticated. For detailed information on how to use CohQL, please visit the chapter Using Coherence Query Language in the Coherence reference guide. In order to give you a way experiment with the cache using CohQL, we provide a simple sample app that pre-populates a Coherence cache countries wih countries and starts the CohQL Console. To get started, execute com.oracle.coherence.guides.queries.StartCohQLConsole . Typically, you would want to start the CohQL Console as a stand-alone application. Please see the following instructions to learn more. Once the console application is started, let&#8217;s execute: <markup lang=\"sql\" >select * from countries The result should be a list of 5 countries: <markup lang=\"java\" >Results Country{name='Colombia', capital='Bogotá', population=50.4} Country{name='Australia', capital='Canberra', population=26.0} Country{name='Ukraine', capital='Kyiv', population=41.2} Country{name='France', capital='Paris', population=67.4} Country{name='Germany', capital='Berlin', population=83.2} What if you would like to just retrieve the list of capitals? We can achieve that by selecting just the capital: <markup lang=\"sql\" >select capital from countries which yields: <markup lang=\"java\" >Results \"Bogotá\" \"Paris\" \"Canberra\" \"Kyiv\" \"Berlin\" Of course, you can also apply where clauses to further limit the results. For example, if you like to retrieve the countries with a population that is greater than 60 million you may add the following where clause: <markup lang=\"sql\" >select capital from countries c where population &gt; 60.0 which results in: <markup lang=\"java\" >Results \"Paris\" \"Berlin\" Another option is to aggregate results. For example, let&#8217;s calculate the total population of countries with a population larger than 60 million: <markup lang=\"sql\" >select sum(population) from countries c where population &gt; 60.0 which yields a value of 150.6 . CohQL is not merely a tool for query caches. It can also be used to create and delete caches, to insert , delete and update cache value, to create indices and more. For more information please see the official reference documentation. Query Caches Programmatically So how would we create queries programmatically to retrieve the same results? The key here is to understand the following concepts: Filter ValueExtractor Aggregator Create the Test Class The first step is to create the test class that will show and test the various query operations, we&#8217;ll call this class QueryTests . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class QueryTests { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence 20.12 . As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We will also populate the cache with several countries and thus let&#8217;s create a small helper class CoherenceHelper : <markup lang=\"java\" >public static void startCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); Session session = coherence.getSession(); NamedCache&lt;String, Country&gt; countries = session.getCache(\"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); countries.put(\"fr\", new Country(\"France\", \"Paris\", 67.4)); countries.put(\"ua\", new Country(\"Ukraine\", \"Kyiv\", 41.2)); countries.put(\"co\", new Country(\"Colombia\", \"Bogotá\", 50.4)); countries.put(\"au\", new Country(\"Australia\", \"Canberra\", 26)); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Obtain the default Session Get the countries cache Populate the countries cache with several new Country instances We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide . <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { CoherenceHelper.startCoherence(); } Call CoherenceHelper and start the Coherence instance and populate the country data. Lastly, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different querying operations. Filter To get started, we would like to retrieve all countries that have a population of more than 60 million people. For that we will use a Filter : <markup lang=\"java\" >@Test void testGreaterEqualsFilter() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); final Set&lt;Map.Entry&lt;String, Country&gt;&gt; results = map.entrySet(filter); assertThat(results).hasSize(2); map.entrySet(filter).forEach(entry -&gt; { assertThat(entry.getKey()).containsAnyOf(\"de\", \"fr\"); assertThat(entry.getValue().getPopulation()).isGreaterThan(60.0); }); } Get the countries Map We create a Filter that will select only countries with more than 60 million people using the Filters helper class via Filters.greaterEqual . Apply the Filter by invoking entrySet(filter) on the Map The result should be 2 countries only Assert that only France and Germany were selected The best practice for ValueExtractors is to use the method reference, e.g. Country::getPopulation , to extract falues as this provides compile time type checking. ValueExtractor What if we don&#8217;t want to return Countries but just the collection of country names for which the population is 60 million people or higher? This is where we can use a ValueExtractor in combination with a ReducerAggregator . A value extractor is used to extract a property from a given object. In most instances developers would use the ReflectionExtractor as an implementation. The ReducerAggregator on the other hand, is used to run a ValueExtractor against cache entries, and it returns the extracted value. The result returned by the ReducerAggregator is a Map where the key is the key of the cache entry and the value is the extracted value. <markup lang=\"java\" >@Test void testValueExtractor() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); ReducerAggregator&lt;String, Country, Country, String&gt; aggregator = new ReducerAggregator&lt;&gt;(Country::getName); Map&lt;String, String&gt; result = map.aggregate(filter, aggregator); result.forEach((key, value) -&gt; { assertThat(key).containsAnyOf(\"de\", \"fr\"); assertThat(value).containsAnyOf(\"Germany\", \"France\"); }); } Get the countries Map We create the same filter as in the previous test (Select countries with more than 60 million people, only) Create a ReducerAggregator instance and specify that we only want the name of the countries returned Apply the Filter and Aggregator Verify that only the two country names France and Germany are returned as filtered values Aggregate Results What if we want to group queried data together? Let&#8217;s query for countries, where the population is greater than 60 million but instead of returning the countries, we will return the sum of the population of thsoe 2 countries instead. <markup lang=\"java\" >@Test void testAggregate() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); BigDecimalSum&lt;BigDecimal&gt; aggregator = new BigDecimalSum&lt;&gt;(\"getPopulation\"); BigDecimal result = map.aggregate(filter, aggregator); String resultAsString = result.setScale(2, RoundingMode.HALF_UP) .stripTrailingZeros() .toPlainString(); assertThat(resultAsString).isEqualTo(\"150.6\"); } Get the countries Map We create the same filter as in the previous test (Select countries with more than 60 million people, only) We will use a different Aggregator . BigDecimalSum will aggregate the population and return a Bigecimal value. This shows that you can use a method name (not recommended) as well as method reference Apply the Filter and Aggregator For assertion purposes we will convert the BigDecimal value to a String The generated String shall not have any trailing zeros Return the String Verify that the returned value is 150.6 To learn much more about built-in Aggregators, please take a look at the respective guide . Summary In this guide we showed how easy it is to query Coherence caches either using CohQL or programmatically using Filters, ValueExtractors and Aggregators. Please see the Coherence reference guide, specifically the chapter Querying Data In a Cache for more details. See Also Using Coherence Query Language Querying Data In a Cache ",
            "title": "Querying Caches"
        },
        {
            "location": "/coherence-mp/health/README",
            "text": " To use Coherence MP Health, you should first declare it as a dependency in the project&#8217;s pom.xml file. You can declare Coherence MP Health as follows: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-health&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;/dependency&gt; Where ${coherence.groupId} is the Maven groupId for the Coherence edition being used, com.oracle.coherence for the commercial edition or com.oracle.coherence.ce for the community edition. And ${coherence.version} is the version of Coherence being used. After the module becomes available in the class path, the Coherence HealthCheck producer CDI bean will be automatically discovered and be registered as a Microprofile health check provider. The Coherence health checks will then be available via any health endpoints served by the application and included in started, readiness and liveness checks. ",
            "title": "Enabling the Use of Coherence MP Health"
        },
        {
            "location": "/coherence-mp/health/README",
            "text": " Coherence MicroProfile (MP) Health provides support for Eclipse MicroProfile Health within the Coherence cluster members. See the documentation on the Coherence Health Check API and MicroProfile Health . Coherence MP Health is a very simple module that enables you to publish Coherence health checks into the MicroProfile Health Check Registries available at runtime. Enabling the Use of Coherence MP Health To use Coherence MP Health, you should first declare it as a dependency in the project&#8217;s pom.xml file. You can declare Coherence MP Health as follows: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-health&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;/dependency&gt; Where ${coherence.groupId} is the Maven groupId for the Coherence edition being used, com.oracle.coherence for the commercial edition or com.oracle.coherence.ce for the community edition. And ${coherence.version} is the version of Coherence being used. After the module becomes available in the class path, the Coherence HealthCheck producer CDI bean will be automatically discovered and be registered as a Microprofile health check provider. The Coherence health checks will then be available via any health endpoints served by the application and included in started, readiness and liveness checks. ",
            "title": "Coherence MicroProfile Health"
        },
        {
            "location": "/docs/core/07_sorted_views",
            "text": " Sorted Views can be created programmatically, using the existing ViewBuilder API: <markup lang=\"java\" >NamedMap&lt;String, String&gt; states = session.getMap(\"states\"); NamedMap&lt;String, String&gt; sortedStates = states.view().sorted().build(); Obtain a reference to a distributed cache that stores master copy of state names, keyed by two-letter state code Create a client-side view of states that will be sorted by the natural order of map values, in this case state name Just like with other views, the contents of the view will be kept in sync automatically by Coherence, so any changes made to the master list of states will be automatically reflected in each client-side view. You can also create a view for more complex data types by passing a custom Comparator to the sorted method: <markup lang=\"java\" >NamedMap&lt;Long, Person&gt; people = session.getMap(\"people\"); NamedMap&lt;Long, Person&gt; sortedPeople = people.view() .sorted(Comparator.comparing(Person::getAge).reversed()) .build(); The above will give you a view of all people, sorted by age from the oldest to the youngest person. Of course, you can also perform all other operations that the ViewBuilder API supports, such as filtering entries in a view before they are sorted. For example, to create a view of all women sorted by age from youngest to oldest, you would define a view like this: <markup lang=\"java\" >NamedMap&lt;Long, Person&gt; people = session.getMap(\"people\"); NamedMap&lt;Long, Person&gt; sortedPeople = people.view() .filter(Filters.equal(Person::getGender, Gender.FEMALE)) .sorted(Comparator.comparing(Person::getAge)) .build(); One thing to keep in mind is that the sorting is always performed on the client after the data is retrieved from the server. In most cases that doesn&#8217;t matter, and will happen regardless of the order that you specify the operations in. For example, the above example would work exactly the same if you reversed the order of filter and sorted operations and created a view like this: <markup lang=\"java\" >NamedMap&lt;Long, Person&gt; people = session.getMap(\"people\"); NamedMap&lt;Long, Person&gt; sortedPeople = people.view() .sorted(Comparator.comparing(Person::getAge)) .filter(Filters.equal(Person::getGender, Gender.FEMALE)) .build(); The only exception is map operation, as it changes the type of the values that are stored on the client by transforming them on the server, to reduce the amount of data that is transferred across the network. Because we can only sort what we have, the sort operation always has to be specified after the map operation, in order to use the correct value type: <markup lang=\"java\" >NamedMap&lt;Long, Person&gt; people = session.getMap(\"people\"); NamedMap&lt;Long, Name&gt; sortedNames = people.view() .filter(Filters.equal(Person::getGender, Gender.MALE)) .map(Person::getName) .sorted(Comparator.comparing(Name::getLast) .thenComparing(Name::getFirst)) .build(); The above will extract the Names of all men on the server, and sort them first by last and then by first name on the client. In general, a good rule to follow is to specify filter , map and sorted operations in that order, as that&#8217;s the order they are actually executed in. ",
            "title": "Creating a Sorted View"
        },
        {
            "location": "/docs/core/07_sorted_views",
            "text": " Sorted Views allow you to create a client-side views of data managed in Coherence that are sorted based either on the natural sort order of the entry values, or on the provided Comparator . Creating a Sorted View Sorted Views can be created programmatically, using the existing ViewBuilder API: <markup lang=\"java\" >NamedMap&lt;String, String&gt; states = session.getMap(\"states\"); NamedMap&lt;String, String&gt; sortedStates = states.view().sorted().build(); Obtain a reference to a distributed cache that stores master copy of state names, keyed by two-letter state code Create a client-side view of states that will be sorted by the natural order of map values, in this case state name Just like with other views, the contents of the view will be kept in sync automatically by Coherence, so any changes made to the master list of states will be automatically reflected in each client-side view. You can also create a view for more complex data types by passing a custom Comparator to the sorted method: <markup lang=\"java\" >NamedMap&lt;Long, Person&gt; people = session.getMap(\"people\"); NamedMap&lt;Long, Person&gt; sortedPeople = people.view() .sorted(Comparator.comparing(Person::getAge).reversed()) .build(); The above will give you a view of all people, sorted by age from the oldest to the youngest person. Of course, you can also perform all other operations that the ViewBuilder API supports, such as filtering entries in a view before they are sorted. For example, to create a view of all women sorted by age from youngest to oldest, you would define a view like this: <markup lang=\"java\" >NamedMap&lt;Long, Person&gt; people = session.getMap(\"people\"); NamedMap&lt;Long, Person&gt; sortedPeople = people.view() .filter(Filters.equal(Person::getGender, Gender.FEMALE)) .sorted(Comparator.comparing(Person::getAge)) .build(); One thing to keep in mind is that the sorting is always performed on the client after the data is retrieved from the server. In most cases that doesn&#8217;t matter, and will happen regardless of the order that you specify the operations in. For example, the above example would work exactly the same if you reversed the order of filter and sorted operations and created a view like this: <markup lang=\"java\" >NamedMap&lt;Long, Person&gt; people = session.getMap(\"people\"); NamedMap&lt;Long, Person&gt; sortedPeople = people.view() .sorted(Comparator.comparing(Person::getAge)) .filter(Filters.equal(Person::getGender, Gender.FEMALE)) .build(); The only exception is map operation, as it changes the type of the values that are stored on the client by transforming them on the server, to reduce the amount of data that is transferred across the network. Because we can only sort what we have, the sort operation always has to be specified after the map operation, in order to use the correct value type: <markup lang=\"java\" >NamedMap&lt;Long, Person&gt; people = session.getMap(\"people\"); NamedMap&lt;Long, Name&gt; sortedNames = people.view() .filter(Filters.equal(Person::getGender, Gender.MALE)) .map(Person::getName) .sorted(Comparator.comparing(Name::getLast) .thenComparing(Name::getFirst)) .build(); The above will extract the Names of all men on the server, and sort them first by last and then by first name on the client. In general, a good rule to follow is to specify filter , map and sorted operations in that order, as that&#8217;s the order they are actually executed in. ",
            "title": "Sorted Views"
        },
        {
            "location": "/coherence-mp/README",
            "text": " Coherence provides a number of additional modules that provide support for different Microprofile APIs. Microprofile Config Using Coherence as a Microprofile config source. Microprofile Metrics Configure Coherence to publish metrics via the Microprofile metrics API. ",
            "title": "Coherence MP"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Project Maven Configuration Federation Configuration Build and Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " You will review the operational and cache configuration required to set up Federated Coherence clusters and carry out the following: Start one or more cache servers for PrimaryCluster Start one or more cache servers for SecondaryCluster Start a CohQL session for PrimaryCluster Start a CohQL session for SecondaryCluster Carry out various data operations on each cluster and observe the data being replicated The default configuration for this example runs the following clusters: PrimaryCluster on 127.0.0.1:7574 SecondaryCluster on 127.0.0.1:7575 You can change these hosts/ports by changing the following in the pom.xml : <markup lang=\"xml\" >&lt;primary.cluster.host&gt;127.0.0.1&lt;/primary.cluster.host&gt; &lt;primary.cluster.port&gt;7574&lt;/primary.cluster.port&gt; &lt;secondary.cluster.host&gt;127.0.0.1&lt;/secondary.cluster.host&gt; &lt;secondary.cluster.port&gt;7575&lt;/secondary.cluster.port&gt; If you wish to know more about Coherence Federation, please see the Coherence Documentation . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Important Because Coherence Federation is only available in Grid Edition, you must carry out the following changes to the project before building and running: Update the coherence.version property in your pom.xml and gradle.properties to the Coherence Grid Edition version you are going to use. Change the coherence.group.id in the above files to com.oracle.coherence . Install Coherence Grid Edition into your local Maven repository by running the following: This example assumes you have Coherence 14.1.1. Please adjust for your Coherence version. <markup lang=\"bas\" >mvn install:install-file -Dfile=$COHERENCE_HOME/lib/coherence.jar \\ -DpomFile=$COHERENCE_HOME/plugins/maven/com/oracle/coherence/coherence/14.1.1/coherence.14.1.1.pom Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Important Because Coherence Federation is only available in Grid Edition, you must carry out the following changes to the project before building and running: Update the coherence.version property in your pom.xml and gradle.properties to the Coherence Grid Edition version you are going to use. Change the coherence.group.id in the above files to com.oracle.coherence . Install Coherence Grid Edition into your local Maven repository by running the following: This example assumes you have Coherence 14.1.1. Please adjust for your Coherence version. <markup lang=\"bas\" >mvn install:install-file -Dfile=$COHERENCE_HOME/lib/coherence.jar \\ -DpomFile=$COHERENCE_HOME/plugins/maven/com/oracle/coherence/coherence/14.1.1/coherence.14.1.1.pom Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. primary-storage - Runs a DefaultCacheServer for the PrimaryCluster primary-cohql - Runs a CohQL session for the PrimaryCluster secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster secondary-cohql - Runs a CohQL session for the SecondaryCluster primary-storage - Runs a DefaultCacheServer for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=PrimaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;PrimaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${primary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${primary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=SecondaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.log.level&lt;/key&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;SecondaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${secondary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${secondary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; ",
            "title": "Maven Configuration"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Federated caching is configured using Coherence configuration files and requires no changes to application code. There are two areas that require configuration for Federation: An operational override file is used to configure federation participants and the federation topology. A cache configuration file is used to create federated caches schemes. A federated cache is a type of partitioned cache service and is managed by a federated cache service instance. The following cache configuration file is used to define the Federated service: <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;key-type&gt;java.lang.Integer&lt;/key-type&gt; &lt;value-type&gt;java.lang.String&lt;/value-type&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;federated-scheme&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;service-name&gt;FederatedPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;topologies&gt; &lt;topology&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;/topology&gt; &lt;/topologies&gt; &lt;/federated-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A cache-mapping for all caches (*) to map to a scheme called federated The federated-scheme in a similar way to a distributed-scheme A topology for the federated-scheme. The default topology is active-active so this element is not required and just included for completeness. The following operational configuration file is used to define the participants and topology: <markup lang=\"xml\" >&lt;federation-config&gt; &lt;participants&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"primary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"primary.cluster.port\"&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;SecondaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"secondary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"secondary.cluster.port\"&gt;7575&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;/participants&gt; &lt;topology-definitions&gt; &lt;active-active&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;active system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/active&gt; &lt;active system-property=\"secondary.cluster\"&gt;SecondaryCluster&lt;/active&gt; &lt;/active-active&gt; &lt;/topology-definitions&gt; &lt;/federation-config&gt; PrimaryCluster participant with its host and port for the cluster Name Service SecondaryCluster participant with its host and port for the cluster Name Service Topology that defines an active-active configuration between clusters. This is the default and not strictly required. ",
            "title": "Federation Configuration"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. primary-storage - Runs a DefaultCacheServer for the PrimaryCluster primary-cohql - Runs a CohQL session for the PrimaryCluster secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster secondary-cohql - Runs a CohQL session for the SecondaryCluster primary-storage - Runs a DefaultCacheServer for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=PrimaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;PrimaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${primary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${primary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=SecondaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.log.level&lt;/key&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;SecondaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${secondary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${secondary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Federation Configuration Federated caching is configured using Coherence configuration files and requires no changes to application code. There are two areas that require configuration for Federation: An operational override file is used to configure federation participants and the federation topology. A cache configuration file is used to create federated caches schemes. A federated cache is a type of partitioned cache service and is managed by a federated cache service instance. The following cache configuration file is used to define the Federated service: <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;key-type&gt;java.lang.Integer&lt;/key-type&gt; &lt;value-type&gt;java.lang.String&lt;/value-type&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;federated-scheme&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;service-name&gt;FederatedPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;topologies&gt; &lt;topology&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;/topology&gt; &lt;/topologies&gt; &lt;/federated-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A cache-mapping for all caches (*) to map to a scheme called federated The federated-scheme in a similar way to a distributed-scheme A topology for the federated-scheme. The default topology is active-active so this element is not required and just included for completeness. The following operational configuration file is used to define the participants and topology: <markup lang=\"xml\" >&lt;federation-config&gt; &lt;participants&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"primary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"primary.cluster.port\"&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;SecondaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"secondary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"secondary.cluster.port\"&gt;7575&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;/participants&gt; &lt;topology-definitions&gt; &lt;active-active&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;active system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/active&gt; &lt;active system-property=\"secondary.cluster\"&gt;SecondaryCluster&lt;/active&gt; &lt;/active-active&gt; &lt;/topology-definitions&gt; &lt;/federation-config&gt; PrimaryCluster participant with its host and port for the cluster Name Service SecondaryCluster participant with its host and port for the cluster Name Service Topology that defines an active-active configuration between clusters. This is the default and not strictly required. ",
            "title": "Review the Project"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P primary-storage and <markup lang=\"bash\" >./mvnw exec:exec -P secondary-storage Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:java -P primary-cohql and <markup lang=\"bash\" >./mvnw exec:java -P secondary-cohql ",
            "title": "Maven"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runServerPrimary and <markup lang=\"bash\" >./gradlew runServerSecondary Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runCohQLPrimary --console=plain and <markup lang=\"bash\" >./gradlew runCohQLSecondary --console=plain ",
            "title": "Gradle"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " In each of the CohQL sessions, run the following command to verify the caches are empty in each cluster: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 In the first (PrimaryCluster) CohQL session, add an entry to the cache test : <markup lang=\"bash\" >CohQL&gt; insert into test key(1) value(\"Tim\") CohQL&gt; select key(), value() from test Results [1, \"Tim\"] In the second (SecondaryCluster) CohQL session, verify the entry was sent from the PrimaryCluster and then update the name to Timothy . As the clusters are active-active , the changes will be sent back to the primary cluster. <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Tim\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] In the first (PrimaryCluster) CohQL session, verify the entry was changed via the change in the SecondaryCluster , then delete the entry and confirm it was deleted in the SecondaryCluster <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; delete from test Results CohQL&gt; select key(), value() from test Results Continue experimenting: You can continue to experiment by inserting, updating or removing data using various CohQL commands. For detailed information on how to use CohQL, please visit the chapter Using Coherence Query Language in the Coherence reference guide. Monitor Federation If you want to monitor Federation, you can do this via the Coherence VisualVM Plugin. See here for how to install the Plugin if you have VisualVM already, otherwise visit https://visualvm.github.io/ to download and install VisualVM . Once you have installed the plugin, you can click on one of the DefaultCacheServer process, and you will see the Federation tab as shown below: ",
            "title": "Run the following commands to exercise Federation"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " After you have built the project as described earlier in this document you can run via Maven or Gradle. Maven Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P primary-storage and <markup lang=\"bash\" >./mvnw exec:exec -P secondary-storage Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:java -P primary-cohql and <markup lang=\"bash\" >./mvnw exec:java -P secondary-cohql Gradle Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runServerPrimary and <markup lang=\"bash\" >./gradlew runServerSecondary Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runCohQLPrimary --console=plain and <markup lang=\"bash\" >./gradlew runCohQLSecondary --console=plain Run the following commands to exercise Federation In each of the CohQL sessions, run the following command to verify the caches are empty in each cluster: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 In the first (PrimaryCluster) CohQL session, add an entry to the cache test : <markup lang=\"bash\" >CohQL&gt; insert into test key(1) value(\"Tim\") CohQL&gt; select key(), value() from test Results [1, \"Tim\"] In the second (SecondaryCluster) CohQL session, verify the entry was sent from the PrimaryCluster and then update the name to Timothy . As the clusters are active-active , the changes will be sent back to the primary cluster. <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Tim\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] In the first (PrimaryCluster) CohQL session, verify the entry was changed via the change in the SecondaryCluster , then delete the entry and confirm it was deleted in the SecondaryCluster <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; delete from test Results CohQL&gt; select key(), value() from test Results Continue experimenting: You can continue to experiment by inserting, updating or removing data using various CohQL commands. For detailed information on how to use CohQL, please visit the chapter Using Coherence Query Language in the Coherence reference guide. Monitor Federation If you want to monitor Federation, you can do this via the Coherence VisualVM Plugin. See here for how to install the Plugin if you have VisualVM already, otherwise visit https://visualvm.github.io/ to download and install VisualVM . Once you have installed the plugin, you can click on one of the DefaultCacheServer process, and you will see the Federation tab as shown below: ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " In this guide you walked through the steps to use Coherence Federation by using Coherence Query Language (CohQL) to insert, update and remove data in Federated clusters. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Federation Documentation Using Coherence Query Language ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " This guide walks through the steps to use Coherence Federation by using Coherence Query Language (CohQL) to insert, update and remove data in Federated clusters. Federated caching federates cache data asynchronously across multiple geographically dispersed clusters. Cached data is federated across clusters to provide redundancy, off-site backup, and multiple points of access for application users in different geographical locations. Federated caching supports multiple federation topologies. These include: active-active, active-passive, hub-spoke, and central-federation. The topologies define common federation strategies between clusters and support a wide variety of use cases. Custom federation topologies can also be created as required. Federated caching provides applications with the ability to accept, reject, or modify cache entries being stored locally or remotely. Conflict resolution is application specific to allow the greatest amount of flexibility when defining federation rules. Federation is only available when using Coherence Grid Edition (GE) 12.2.1.4.X and above, and is not available in the open-source Coherence Community Edition (CE). As Coherence Grid Edition JAR&#8217;s and not available in Maven central, to build and run this example you, must first install the Coherence JAR into your Maven Repository from your local Grid Edition Install. See here for instructions on how to complete this. Table of Contents What You Will Build What You Need Building the Example Code Review the Project Maven Configuration Federation Configuration Build and Run the Example Summary See Also What You Will Build You will review the operational and cache configuration required to set up Federated Coherence clusters and carry out the following: Start one or more cache servers for PrimaryCluster Start one or more cache servers for SecondaryCluster Start a CohQL session for PrimaryCluster Start a CohQL session for SecondaryCluster Carry out various data operations on each cluster and observe the data being replicated The default configuration for this example runs the following clusters: PrimaryCluster on 127.0.0.1:7574 SecondaryCluster on 127.0.0.1:7575 You can change these hosts/ports by changing the following in the pom.xml : <markup lang=\"xml\" >&lt;primary.cluster.host&gt;127.0.0.1&lt;/primary.cluster.host&gt; &lt;primary.cluster.port&gt;7574&lt;/primary.cluster.port&gt; &lt;secondary.cluster.host&gt;127.0.0.1&lt;/secondary.cluster.host&gt; &lt;secondary.cluster.port&gt;7575&lt;/secondary.cluster.port&gt; If you wish to know more about Coherence Federation, please see the Coherence Documentation . What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Important Because Coherence Federation is only available in Grid Edition, you must carry out the following changes to the project before building and running: Update the coherence.version property in your pom.xml and gradle.properties to the Coherence Grid Edition version you are going to use. Change the coherence.group.id in the above files to com.oracle.coherence . Install Coherence Grid Edition into your local Maven repository by running the following: This example assumes you have Coherence 14.1.1. Please adjust for your Coherence version. <markup lang=\"bas\" >mvn install:install-file -Dfile=$COHERENCE_HOME/lib/coherence.jar \\ -DpomFile=$COHERENCE_HOME/plugins/maven/com/oracle/coherence/coherence/14.1.1/coherence.14.1.1.pom Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Review the Project Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. primary-storage - Runs a DefaultCacheServer for the PrimaryCluster primary-cohql - Runs a CohQL session for the PrimaryCluster secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster secondary-cohql - Runs a CohQL session for the SecondaryCluster primary-storage - Runs a DefaultCacheServer for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=PrimaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;PrimaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${primary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${primary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=SecondaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.log.level&lt;/key&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;SecondaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${secondary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${secondary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Federation Configuration Federated caching is configured using Coherence configuration files and requires no changes to application code. There are two areas that require configuration for Federation: An operational override file is used to configure federation participants and the federation topology. A cache configuration file is used to create federated caches schemes. A federated cache is a type of partitioned cache service and is managed by a federated cache service instance. The following cache configuration file is used to define the Federated service: <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;key-type&gt;java.lang.Integer&lt;/key-type&gt; &lt;value-type&gt;java.lang.String&lt;/value-type&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;federated-scheme&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;service-name&gt;FederatedPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;topologies&gt; &lt;topology&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;/topology&gt; &lt;/topologies&gt; &lt;/federated-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A cache-mapping for all caches (*) to map to a scheme called federated The federated-scheme in a similar way to a distributed-scheme A topology for the federated-scheme. The default topology is active-active so this element is not required and just included for completeness. The following operational configuration file is used to define the participants and topology: <markup lang=\"xml\" >&lt;federation-config&gt; &lt;participants&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"primary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"primary.cluster.port\"&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;SecondaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"secondary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"secondary.cluster.port\"&gt;7575&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;/participants&gt; &lt;topology-definitions&gt; &lt;active-active&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;active system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/active&gt; &lt;active system-property=\"secondary.cluster\"&gt;SecondaryCluster&lt;/active&gt; &lt;/active-active&gt; &lt;/topology-definitions&gt; &lt;/federation-config&gt; PrimaryCluster participant with its host and port for the cluster Name Service SecondaryCluster participant with its host and port for the cluster Name Service Topology that defines an active-active configuration between clusters. This is the default and not strictly required. Run the Example After you have built the project as described earlier in this document you can run via Maven or Gradle. Maven Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P primary-storage and <markup lang=\"bash\" >./mvnw exec:exec -P secondary-storage Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:java -P primary-cohql and <markup lang=\"bash\" >./mvnw exec:java -P secondary-cohql Gradle Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runServerPrimary and <markup lang=\"bash\" >./gradlew runServerSecondary Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runCohQLPrimary --console=plain and <markup lang=\"bash\" >./gradlew runCohQLSecondary --console=plain Run the following commands to exercise Federation In each of the CohQL sessions, run the following command to verify the caches are empty in each cluster: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 In the first (PrimaryCluster) CohQL session, add an entry to the cache test : <markup lang=\"bash\" >CohQL&gt; insert into test key(1) value(\"Tim\") CohQL&gt; select key(), value() from test Results [1, \"Tim\"] In the second (SecondaryCluster) CohQL session, verify the entry was sent from the PrimaryCluster and then update the name to Timothy . As the clusters are active-active , the changes will be sent back to the primary cluster. <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Tim\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] In the first (PrimaryCluster) CohQL session, verify the entry was changed via the change in the SecondaryCluster , then delete the entry and confirm it was deleted in the SecondaryCluster <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; delete from test Results CohQL&gt; select key(), value() from test Results Continue experimenting: You can continue to experiment by inserting, updating or removing data using various CohQL commands. For detailed information on how to use CohQL, please visit the chapter Using Coherence Query Language in the Coherence reference guide. Monitor Federation If you want to monitor Federation, you can do this via the Coherence VisualVM Plugin. See here for how to install the Plugin if you have VisualVM already, otherwise visit https://visualvm.github.io/ to download and install VisualVM . Once you have installed the plugin, you can click on one of the DefaultCacheServer process, and you will see the Federation tab as shown below: Summary In this guide you walked through the steps to use Coherence Federation by using Coherence Query Language (CohQL) to insert, update and remove data in Federated clusters. See Also Federation Documentation Using Coherence Query Language ",
            "title": "Federation"
        },
        {
            "location": "/plugins/maven/pof-maven-plugin/README",
            "text": " The POF Maven Plugin provides automated instrumentation of classes with the @PortableType annotation to generate consistent (and correct) implementations of Evolvable POF serialization methods. It is a far from a trivial exercise to manually write serialization methods that support serializing inheritance hierarchies that support the Evolvable concept. However, with static type analysis these methods can be deterministically generated. This allows developers to focus on business logic rather than implementing boilerplate code for the above-mentioned methods. Please see Portable Types documentation for more information and detailed instructions on Portable Types creation and usage. ",
            "title": "POF Maven Plugin"
        },
        {
            "location": "/plugins/maven/pof-maven-plugin/README",
            "text": " In order to use the POF Maven Plugin, you need to declare it as a plugin dependency in your pom.xml : <markup lang=\"xml\" > &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;pof-maven-plugin&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;instrument&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;instrument-tests&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument-tests&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; An example Person class (below) when processed with the plugin is below results in the bytecode shown below. <markup lang=\"java\" >@PortableType(id=1000) public class Person { public Person() { } public Person(int id, String name, Address address) { super(); this.id = id; this.name = name; this.address = address; } int id; String name; Address address; // getters and setters omitted for brevity } Generated bytecode: <markup lang=\"bash\" >$ javap Person.class Compiled from \"Person.java\" public class demo.Person implements com.tangosol.io.pof.PortableObject,com.tangosol.io.pof.EvolvableObject { int id; java.lang.String name; demo.Address address; public demo.Person(); public demo.Person(int, java.lang.String, demo.Address); public int getId(); public void setId(int); public java.lang.String getName(); public void setName(java.lang.String); public demo.Address getAddress(); public void setAddress(demo.Address); public java.lang.String toString(); public int hashCode(); public boolean equals(java.lang.Object); public void readExternal(com.tangosol.io.pof.PofReader) throws java.io.IOException; public void writeExternal(com.tangosol.io.pof.PofWriter) throws java.io.IOException; public com.tangosol.io.Evolvable getEvolvable(int); public com.tangosol.io.pof.EvolvableHolder getEvolvableHolder(); } Additional methods generated by Coherence POF plugin. ",
            "title": "Usage"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " The example code is written as a set of unit tests, showing you how to use CDI Caching Response annotations. ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " The data model for this guide consists of a single class named Message . It represents a message for a user and has a single property: message. ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " The first step is to create root JAX-RS resource class that will be used to test the various response caching operations. Resource will be using messages-cache to store cached messages. <markup lang=\"java\" >@Path(\"/\") @RequestScoped @CacheName(\"messages-cache\") public class GreetResource { /** * This is used to track the count of invocations of a method annotated with {@link CacheGet}. */ public static final AtomicInteger GET_CALLS = new AtomicInteger(); /** * This is used to track the count of invocations of a method annotated with {@link CacheAdd}. */ public static final AtomicInteger ADD_CALLS = new AtomicInteger(); /** * This is used to track the count of invocations of a method annotated with {@link CachePut}. */ public static final AtomicInteger PUT_CALLS = new AtomicInteger(); /** * This is used to track the count of invocations of a method annotated with {@link CacheRemove}. */ public static final AtomicInteger REMOVE_CALLS = new AtomicInteger(); /** * This is used to track the count of invocations of a method with multiple parameters that * are used to build cache key. */ public static final AtomicInteger MULTI_PARAM_CALLS = new AtomicInteger(); and test class: <markup lang=\"java\" >@HelidonTest public class GreetResourceTest { @Inject private WebTarget target; @Inject @Name(\"messages-cache\") private NamedMap cache; @Inject @Name(\"another-cache\") private NamedMap anotherCache; @BeforeAll static void boot() { System.setProperty(\"coherence.wka\", \"127.0.0.1\"); } @BeforeEach void setup() { cache.clear(); anotherCache.clear(); GreetResource.GET_CALLS.set(0); GreetResource.ADD_CALLS.set(0); GreetResource.PUT_CALLS.set(0); GreetResource.REMOVE_CALLS.set(0); } Inject cache so we can verify its content Reset cache content and counters before each test ",
            "title": "Create the Resource Class and JUnit Test Class"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " @CacheGet gets the value from the cache if present; invokes the target method and caches the result otherwise. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @GET @Produces(MediaType.APPLICATION_JSON) @CacheGet public Message getMessage(@PathParam(\"name\") String name) { GET_CALLS.incrementAndGet(); return new Message(\"Hello \" + name); } We&#8217;ll test @CacheGet annotation processing Cache key will be name argument We&#8217;ll count number of method invocations Result of the method invocation that will be cached Add test method for @CacheGet operation: <markup lang=\"java\" >@Test void testGet() { Message getResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); final Message expected = new Message(\"Hello John\"); assertThat(getResponse, is(expected)); assertThat(GreetResource.GET_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(expected)); getResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); assertThat(getResponse, is(expected)); assertThat(GreetResource.GET_CALLS.get(), is(1)); } Invoke caching resource method Verify that response is the expected one Verify that target method was invoked Verify that response was cached Verify that repeated invocation of the caching resource method won&#8217;t result in the method execution as result will be returned from the cache ",
            "title": "@CacheGet"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " @CacheAdd always calls the target method and then caches the result. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @POST @Produces(MediaType.APPLICATION_JSON) @CacheAdd public Message addMessage(@PathParam(\"name\") String name) { ADD_CALLS.incrementAndGet(); return new Message(\"ADD executed\"); } We&#8217;ll test @CacheAdd annotation processing Cache key will be name argument We&#8217;ll count number of method invocations Result of the method invocation that will be cached Test method for @CacheAdd operation: <markup lang=\"java\" >@Test void testAdd() { Message getResponse = target.path(\"/greet/John\") .request() .get(Message.class); final Message expectedGetResponse = new Message(\"Hello John\"); assertThat(getResponse, is(expectedGetResponse)); assertThat(GreetResource.GET_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(expectedGetResponse)); final Message addedMessage = new Message(\"ADD executed\"); Message addResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .post(null, Message.class); assertThat(addResponse, is(addedMessage)); assertThat(GreetResource.ADD_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(addedMessage)); addResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .post(null, Message.class); assertThat(addResponse, is(addedMessage)); assertThat(GreetResource.ADD_CALLS.get(), is(2)); assertThat(cache.get(\"John\"), is(addedMessage)); } Populate cache by invoking caching resource method Invoke resource method annotated with @CacheAdd Verify that the target method was invoked and its returning value was cached Invoke @CacheAdd annotated method again Verify that the target method was executed once again ",
            "title": "@CacheAdd"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " @CachePut stores the value annotated with @CacheValue in the cache and calls the target method Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @PUT @Consumes(MediaType.APPLICATION_JSON) @Produces(MediaType.APPLICATION_JSON) @CachePut public Message putMessage(@CacheKey @PathParam(\"name\") String name, @CacheValue Message message) { PUT_CALLS.incrementAndGet(); return new Message(\"PUT executed\"); } We&#8217;ll test @CachePut annotation processing Cache key will be name argument and message will be new cache value We&#8217;ll count number of method invocations Result of the method invocation that won&#8217;t be cached Test method for @CachePut operation: <markup lang=\"java\" >@Test void testPut() { final Message messageToCache = new Message(\"Hola\"); final Message expectedPutResponse = new Message(\"PUT executed\"); Message putResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .put(Entity.entity(messageToCache, MediaType.APPLICATION_JSON_TYPE), Message.class); assertThat(putResponse, is(expectedPutResponse)); assertThat(GreetResource.PUT_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(messageToCache)); putResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .put(Entity.entity(messageToCache, MediaType.APPLICATION_JSON_TYPE), Message.class); assertThat(putResponse, is(expectedPutResponse)); assertThat(GreetResource.PUT_CALLS.get(), is(2)); } Pass new cache value to the caching resource method Verify that passed value is stored in the cache Invoke the same @CachePut annotated method again Verify that the target method was executed once again ",
            "title": "@CachePut"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " @CacheRemove removes the key from the cache and calls the target method Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @DELETE @Produces(MediaType.APPLICATION_JSON) @CacheRemove public Message removeMessage(@PathParam(\"name\") String name) { REMOVE_CALLS.incrementAndGet(); return new Message(\"Deleted cached value for \" + name); } We&#8217;ll test @CacheRemove annotation processing Cache key to remove from the cache We&#8217;ll count number of method invocations Result of the method invocation that will be returned Test method for @CacheRemove operation: <markup lang=\"java\" >@Test void testRemove() { final Message hola = new Message(\"Hola\"); Message putResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .put(Entity.entity(hola, MediaType.APPLICATION_JSON_TYPE), Message.class); assertThat(putResponse, is(new Message(\"PUT executed\"))); assertThat(GreetResource.PUT_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(hola)); Message deleteResponse = target.path(\"/greet/John\") .request() .delete(Message.class); assertThat(deleteResponse, is(new Message(\"Deleted cached value for John\"))); assertThat(GreetResource.REMOVE_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(nullValue())); assertThat(cache.size(), is(0)); } Store initial value in the cache Verify that cache is populated Remove key from the cache by invoking resource method marked with @CacheRemove Verify that key was removed from the cache ",
            "title": "@CacheRemove"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " The @CacheName annotation defines the cache that will be used for response caching. If both the class and methods are annotated with @CacheName, the value from the method annotation takes precedence. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"another\") @GET @Produces(MediaType.APPLICATION_JSON) @CacheGet @CacheName(\"another-cache\") public Message getFromAnotherCache(@QueryParam(\"name\") @CacheKey String name) { return new Message(\"Another \" + name + \"?\"); } Specify cache name that will override cache name defined on a class (message-cache) Test method for @CacheName annotation: <markup lang=\"java\" >@Test void testCacheName() { Message anotherGetResponse = target.path(\"/another\") .queryParam(\"name\", \"John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); assertThat(anotherGetResponse, is(new Message(\"Another John?\"))); assertThat(cache.size(), is(0)); assertThat(anotherCache.size(), is(1)); assertThat(anotherCache.get(\"John\"), is(new Message(\"Another John?\"))); } Populate cache Verify that cache specified by class @CacheName is not populated Verify that cache specified by method @CacheName is populated ",
            "title": "@CacheName"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " Unless the @CacheKey annotation is applied to a parameter, all parameters except for one marked with @CacheValue will be used as a part of the cache key. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"parameters\") @GET @Produces(MediaType.APPLICATION_JSON) @CacheGet public Message get(@QueryParam(\"firstName\") String firstName, @QueryParam(\"lastName\") String lastName) { MULTI_PARAM_CALLS.incrementAndGet(); return new Message(\"Message for \" + firstName + \" \" + lastName); } Cache key will be assembled from both firstName and lastName arguments. Test method: <markup lang=\"java\" >@Test void testCacheName() { Message anotherGetResponse = target.path(\"/another\") .queryParam(\"name\", \"John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); assertThat(anotherGetResponse, is(new Message(\"Another John?\"))); assertThat(cache.size(), is(0)); assertThat(anotherCache.size(), is(1)); assertThat(anotherCache.get(\"John\"), is(new Message(\"Another John?\"))); } Store initial value in the cache Verify that cache is populated correctly Verify that value was fetched from the cache ",
            "title": "Multiple arguments as a cache key"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " Let&#8217;s add resource method and test for each response caching operation: @CacheGet @CacheGet gets the value from the cache if present; invokes the target method and caches the result otherwise. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @GET @Produces(MediaType.APPLICATION_JSON) @CacheGet public Message getMessage(@PathParam(\"name\") String name) { GET_CALLS.incrementAndGet(); return new Message(\"Hello \" + name); } We&#8217;ll test @CacheGet annotation processing Cache key will be name argument We&#8217;ll count number of method invocations Result of the method invocation that will be cached Add test method for @CacheGet operation: <markup lang=\"java\" >@Test void testGet() { Message getResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); final Message expected = new Message(\"Hello John\"); assertThat(getResponse, is(expected)); assertThat(GreetResource.GET_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(expected)); getResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); assertThat(getResponse, is(expected)); assertThat(GreetResource.GET_CALLS.get(), is(1)); } Invoke caching resource method Verify that response is the expected one Verify that target method was invoked Verify that response was cached Verify that repeated invocation of the caching resource method won&#8217;t result in the method execution as result will be returned from the cache @CacheAdd @CacheAdd always calls the target method and then caches the result. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @POST @Produces(MediaType.APPLICATION_JSON) @CacheAdd public Message addMessage(@PathParam(\"name\") String name) { ADD_CALLS.incrementAndGet(); return new Message(\"ADD executed\"); } We&#8217;ll test @CacheAdd annotation processing Cache key will be name argument We&#8217;ll count number of method invocations Result of the method invocation that will be cached Test method for @CacheAdd operation: <markup lang=\"java\" >@Test void testAdd() { Message getResponse = target.path(\"/greet/John\") .request() .get(Message.class); final Message expectedGetResponse = new Message(\"Hello John\"); assertThat(getResponse, is(expectedGetResponse)); assertThat(GreetResource.GET_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(expectedGetResponse)); final Message addedMessage = new Message(\"ADD executed\"); Message addResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .post(null, Message.class); assertThat(addResponse, is(addedMessage)); assertThat(GreetResource.ADD_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(addedMessage)); addResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .post(null, Message.class); assertThat(addResponse, is(addedMessage)); assertThat(GreetResource.ADD_CALLS.get(), is(2)); assertThat(cache.get(\"John\"), is(addedMessage)); } Populate cache by invoking caching resource method Invoke resource method annotated with @CacheAdd Verify that the target method was invoked and its returning value was cached Invoke @CacheAdd annotated method again Verify that the target method was executed once again @CachePut @CachePut stores the value annotated with @CacheValue in the cache and calls the target method Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @PUT @Consumes(MediaType.APPLICATION_JSON) @Produces(MediaType.APPLICATION_JSON) @CachePut public Message putMessage(@CacheKey @PathParam(\"name\") String name, @CacheValue Message message) { PUT_CALLS.incrementAndGet(); return new Message(\"PUT executed\"); } We&#8217;ll test @CachePut annotation processing Cache key will be name argument and message will be new cache value We&#8217;ll count number of method invocations Result of the method invocation that won&#8217;t be cached Test method for @CachePut operation: <markup lang=\"java\" >@Test void testPut() { final Message messageToCache = new Message(\"Hola\"); final Message expectedPutResponse = new Message(\"PUT executed\"); Message putResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .put(Entity.entity(messageToCache, MediaType.APPLICATION_JSON_TYPE), Message.class); assertThat(putResponse, is(expectedPutResponse)); assertThat(GreetResource.PUT_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(messageToCache)); putResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .put(Entity.entity(messageToCache, MediaType.APPLICATION_JSON_TYPE), Message.class); assertThat(putResponse, is(expectedPutResponse)); assertThat(GreetResource.PUT_CALLS.get(), is(2)); } Pass new cache value to the caching resource method Verify that passed value is stored in the cache Invoke the same @CachePut annotated method again Verify that the target method was executed once again @CacheRemove @CacheRemove removes the key from the cache and calls the target method Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @DELETE @Produces(MediaType.APPLICATION_JSON) @CacheRemove public Message removeMessage(@PathParam(\"name\") String name) { REMOVE_CALLS.incrementAndGet(); return new Message(\"Deleted cached value for \" + name); } We&#8217;ll test @CacheRemove annotation processing Cache key to remove from the cache We&#8217;ll count number of method invocations Result of the method invocation that will be returned Test method for @CacheRemove operation: <markup lang=\"java\" >@Test void testRemove() { final Message hola = new Message(\"Hola\"); Message putResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .put(Entity.entity(hola, MediaType.APPLICATION_JSON_TYPE), Message.class); assertThat(putResponse, is(new Message(\"PUT executed\"))); assertThat(GreetResource.PUT_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(hola)); Message deleteResponse = target.path(\"/greet/John\") .request() .delete(Message.class); assertThat(deleteResponse, is(new Message(\"Deleted cached value for John\"))); assertThat(GreetResource.REMOVE_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(nullValue())); assertThat(cache.size(), is(0)); } Store initial value in the cache Verify that cache is populated Remove key from the cache by invoking resource method marked with @CacheRemove Verify that key was removed from the cache @CacheName The @CacheName annotation defines the cache that will be used for response caching. If both the class and methods are annotated with @CacheName, the value from the method annotation takes precedence. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"another\") @GET @Produces(MediaType.APPLICATION_JSON) @CacheGet @CacheName(\"another-cache\") public Message getFromAnotherCache(@QueryParam(\"name\") @CacheKey String name) { return new Message(\"Another \" + name + \"?\"); } Specify cache name that will override cache name defined on a class (message-cache) Test method for @CacheName annotation: <markup lang=\"java\" >@Test void testCacheName() { Message anotherGetResponse = target.path(\"/another\") .queryParam(\"name\", \"John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); assertThat(anotherGetResponse, is(new Message(\"Another John?\"))); assertThat(cache.size(), is(0)); assertThat(anotherCache.size(), is(1)); assertThat(anotherCache.get(\"John\"), is(new Message(\"Another John?\"))); } Populate cache Verify that cache specified by class @CacheName is not populated Verify that cache specified by method @CacheName is populated Multiple arguments as a cache key Unless the @CacheKey annotation is applied to a parameter, all parameters except for one marked with @CacheValue will be used as a part of the cache key. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"parameters\") @GET @Produces(MediaType.APPLICATION_JSON) @CacheGet public Message get(@QueryParam(\"firstName\") String firstName, @QueryParam(\"lastName\") String lastName) { MULTI_PARAM_CALLS.incrementAndGet(); return new Message(\"Message for \" + firstName + \" \" + lastName); } Cache key will be assembled from both firstName and lastName arguments. Test method: <markup lang=\"java\" >@Test void testCacheName() { Message anotherGetResponse = target.path(\"/another\") .queryParam(\"name\", \"John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); assertThat(anotherGetResponse, is(new Message(\"Another John?\"))); assertThat(cache.size(), is(0)); assertThat(anotherCache.size(), is(1)); assertThat(anotherCache.get(\"John\"), is(new Message(\"Another John?\"))); } Store initial value in the cache Verify that cache is populated correctly Verify that value was fetched from the cache ",
            "title": "Response Caching Operations"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " You have seen how to use CDI Caching Response annotations. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/600-response-caching/README",
            "text": " CDI Response Caching allows you to cache the results of method invocations. Each time a target method is invoked, CDI interceptors check whether the method has already been invoked for the given arguments. If the method has been invoked, the cached result is returned without invoking the target method again. If there are no cached results because the method hasn&#8217;t been invoked yet or because the result was removed from the cache, the target method is invoked, the result is cached, and then returned to the caller. What You Will Build The example code is written as a set of unit tests, showing you how to use CDI Caching Response annotations. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Example Data Model The data model for this guide consists of a single class named Message . It represents a message for a user and has a single property: message. Create the Resource Class and JUnit Test Class The first step is to create root JAX-RS resource class that will be used to test the various response caching operations. Resource will be using messages-cache to store cached messages. <markup lang=\"java\" >@Path(\"/\") @RequestScoped @CacheName(\"messages-cache\") public class GreetResource { /** * This is used to track the count of invocations of a method annotated with {@link CacheGet}. */ public static final AtomicInteger GET_CALLS = new AtomicInteger(); /** * This is used to track the count of invocations of a method annotated with {@link CacheAdd}. */ public static final AtomicInteger ADD_CALLS = new AtomicInteger(); /** * This is used to track the count of invocations of a method annotated with {@link CachePut}. */ public static final AtomicInteger PUT_CALLS = new AtomicInteger(); /** * This is used to track the count of invocations of a method annotated with {@link CacheRemove}. */ public static final AtomicInteger REMOVE_CALLS = new AtomicInteger(); /** * This is used to track the count of invocations of a method with multiple parameters that * are used to build cache key. */ public static final AtomicInteger MULTI_PARAM_CALLS = new AtomicInteger(); and test class: <markup lang=\"java\" >@HelidonTest public class GreetResourceTest { @Inject private WebTarget target; @Inject @Name(\"messages-cache\") private NamedMap cache; @Inject @Name(\"another-cache\") private NamedMap anotherCache; @BeforeAll static void boot() { System.setProperty(\"coherence.wka\", \"127.0.0.1\"); } @BeforeEach void setup() { cache.clear(); anotherCache.clear(); GreetResource.GET_CALLS.set(0); GreetResource.ADD_CALLS.set(0); GreetResource.PUT_CALLS.set(0); GreetResource.REMOVE_CALLS.set(0); } Inject cache so we can verify its content Reset cache content and counters before each test Response Caching Operations Let&#8217;s add resource method and test for each response caching operation: @CacheGet @CacheGet gets the value from the cache if present; invokes the target method and caches the result otherwise. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @GET @Produces(MediaType.APPLICATION_JSON) @CacheGet public Message getMessage(@PathParam(\"name\") String name) { GET_CALLS.incrementAndGet(); return new Message(\"Hello \" + name); } We&#8217;ll test @CacheGet annotation processing Cache key will be name argument We&#8217;ll count number of method invocations Result of the method invocation that will be cached Add test method for @CacheGet operation: <markup lang=\"java\" >@Test void testGet() { Message getResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); final Message expected = new Message(\"Hello John\"); assertThat(getResponse, is(expected)); assertThat(GreetResource.GET_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(expected)); getResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); assertThat(getResponse, is(expected)); assertThat(GreetResource.GET_CALLS.get(), is(1)); } Invoke caching resource method Verify that response is the expected one Verify that target method was invoked Verify that response was cached Verify that repeated invocation of the caching resource method won&#8217;t result in the method execution as result will be returned from the cache @CacheAdd @CacheAdd always calls the target method and then caches the result. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @POST @Produces(MediaType.APPLICATION_JSON) @CacheAdd public Message addMessage(@PathParam(\"name\") String name) { ADD_CALLS.incrementAndGet(); return new Message(\"ADD executed\"); } We&#8217;ll test @CacheAdd annotation processing Cache key will be name argument We&#8217;ll count number of method invocations Result of the method invocation that will be cached Test method for @CacheAdd operation: <markup lang=\"java\" >@Test void testAdd() { Message getResponse = target.path(\"/greet/John\") .request() .get(Message.class); final Message expectedGetResponse = new Message(\"Hello John\"); assertThat(getResponse, is(expectedGetResponse)); assertThat(GreetResource.GET_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(expectedGetResponse)); final Message addedMessage = new Message(\"ADD executed\"); Message addResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .post(null, Message.class); assertThat(addResponse, is(addedMessage)); assertThat(GreetResource.ADD_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(addedMessage)); addResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .post(null, Message.class); assertThat(addResponse, is(addedMessage)); assertThat(GreetResource.ADD_CALLS.get(), is(2)); assertThat(cache.get(\"John\"), is(addedMessage)); } Populate cache by invoking caching resource method Invoke resource method annotated with @CacheAdd Verify that the target method was invoked and its returning value was cached Invoke @CacheAdd annotated method again Verify that the target method was executed once again @CachePut @CachePut stores the value annotated with @CacheValue in the cache and calls the target method Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @PUT @Consumes(MediaType.APPLICATION_JSON) @Produces(MediaType.APPLICATION_JSON) @CachePut public Message putMessage(@CacheKey @PathParam(\"name\") String name, @CacheValue Message message) { PUT_CALLS.incrementAndGet(); return new Message(\"PUT executed\"); } We&#8217;ll test @CachePut annotation processing Cache key will be name argument and message will be new cache value We&#8217;ll count number of method invocations Result of the method invocation that won&#8217;t be cached Test method for @CachePut operation: <markup lang=\"java\" >@Test void testPut() { final Message messageToCache = new Message(\"Hola\"); final Message expectedPutResponse = new Message(\"PUT executed\"); Message putResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .put(Entity.entity(messageToCache, MediaType.APPLICATION_JSON_TYPE), Message.class); assertThat(putResponse, is(expectedPutResponse)); assertThat(GreetResource.PUT_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(messageToCache)); putResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .put(Entity.entity(messageToCache, MediaType.APPLICATION_JSON_TYPE), Message.class); assertThat(putResponse, is(expectedPutResponse)); assertThat(GreetResource.PUT_CALLS.get(), is(2)); } Pass new cache value to the caching resource method Verify that passed value is stored in the cache Invoke the same @CachePut annotated method again Verify that the target method was executed once again @CacheRemove @CacheRemove removes the key from the cache and calls the target method Add resource method to the resource class: <markup lang=\"java\" >@Path(\"greet/{name}\") @DELETE @Produces(MediaType.APPLICATION_JSON) @CacheRemove public Message removeMessage(@PathParam(\"name\") String name) { REMOVE_CALLS.incrementAndGet(); return new Message(\"Deleted cached value for \" + name); } We&#8217;ll test @CacheRemove annotation processing Cache key to remove from the cache We&#8217;ll count number of method invocations Result of the method invocation that will be returned Test method for @CacheRemove operation: <markup lang=\"java\" >@Test void testRemove() { final Message hola = new Message(\"Hola\"); Message putResponse = target.path(\"/greet/John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .put(Entity.entity(hola, MediaType.APPLICATION_JSON_TYPE), Message.class); assertThat(putResponse, is(new Message(\"PUT executed\"))); assertThat(GreetResource.PUT_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(hola)); Message deleteResponse = target.path(\"/greet/John\") .request() .delete(Message.class); assertThat(deleteResponse, is(new Message(\"Deleted cached value for John\"))); assertThat(GreetResource.REMOVE_CALLS.get(), is(1)); assertThat(cache.get(\"John\"), is(nullValue())); assertThat(cache.size(), is(0)); } Store initial value in the cache Verify that cache is populated Remove key from the cache by invoking resource method marked with @CacheRemove Verify that key was removed from the cache @CacheName The @CacheName annotation defines the cache that will be used for response caching. If both the class and methods are annotated with @CacheName, the value from the method annotation takes precedence. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"another\") @GET @Produces(MediaType.APPLICATION_JSON) @CacheGet @CacheName(\"another-cache\") public Message getFromAnotherCache(@QueryParam(\"name\") @CacheKey String name) { return new Message(\"Another \" + name + \"?\"); } Specify cache name that will override cache name defined on a class (message-cache) Test method for @CacheName annotation: <markup lang=\"java\" >@Test void testCacheName() { Message anotherGetResponse = target.path(\"/another\") .queryParam(\"name\", \"John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); assertThat(anotherGetResponse, is(new Message(\"Another John?\"))); assertThat(cache.size(), is(0)); assertThat(anotherCache.size(), is(1)); assertThat(anotherCache.get(\"John\"), is(new Message(\"Another John?\"))); } Populate cache Verify that cache specified by class @CacheName is not populated Verify that cache specified by method @CacheName is populated Multiple arguments as a cache key Unless the @CacheKey annotation is applied to a parameter, all parameters except for one marked with @CacheValue will be used as a part of the cache key. Add resource method to the resource class: <markup lang=\"java\" >@Path(\"parameters\") @GET @Produces(MediaType.APPLICATION_JSON) @CacheGet public Message get(@QueryParam(\"firstName\") String firstName, @QueryParam(\"lastName\") String lastName) { MULTI_PARAM_CALLS.incrementAndGet(); return new Message(\"Message for \" + firstName + \" \" + lastName); } Cache key will be assembled from both firstName and lastName arguments. Test method: <markup lang=\"java\" >@Test void testCacheName() { Message anotherGetResponse = target.path(\"/another\") .queryParam(\"name\", \"John\") .request() .acceptEncoding(MediaType.APPLICATION_JSON) .get(Message.class); assertThat(anotherGetResponse, is(new Message(\"Another John?\"))); assertThat(cache.size(), is(0)); assertThat(anotherCache.size(), is(1)); assertThat(anotherCache.get(\"John\"), is(new Message(\"Another John?\"))); } Store initial value in the cache Verify that cache is populated correctly Verify that value was fetched from the cache Summary You have seen how to use CDI Caching Response annotations. ",
            "title": "CDI Response Caching"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " To support arbitrary vector types, Coherence provides com.oracle.coherence.ai.Vector&lt;T&gt; interface, with three built-in implementations: BitVector , which internally uses a java.util.Bitset to represent each vector element using a single bit, Int8Vector , which internally uses a byte[] , and Float32Vector , which internally uses a float[] . These types allow users to add a vector property to their own classes the same way they would add any other property: by simply creating a field and accessors for it: <markup lang=\"java\" title=\"Book.java\" >@PortableType(id = 2001) public class Book { @Portable private String isbn; @Portable private String title; @Portable private String author; @Portable private String summary; @Portable private Vector&lt;float[]&gt; summaryEmbedding; // constructors, getters and setters omitted } In the example above, the summaryEmbedding field is used to store vector representation of the summary field, so we can use vector similarity to search book summaries. In the subsequent sections, we&#8217;ll discuss how the summaryEmbedding property can be used to define both standard and vector indexes, and to perform similarity search against them. ",
            "title": "Vector Types"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " It is possible to improve performance of a brute-force search by creating a forward-only index on the vector attribute using DeserializationAccelerator : <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new DeserializationAccelerator(Book::getSummaryEmbedding)); This will avoid repeated deserialization of Book values when performing brute-force search, at the cost of additional memory consumed by the indexed vector instances. The search will still perform the exact distance calculation, so the results will be exact, just like with the non-indexed brute-force search. ",
            "title": "Indexed Brute-Force Search"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " By default, if no index is defined for the vector attribute, Coherence will perform a brute-force search by deserializing every entry, extracting the vector attribute from it, and performing distance calculation between the extracted vector and the search vector using specified distance algorithm. This is fine for small or medium-sized data sets, because Coherence will still perform search in parallel across cluster members and aggregate the results, but can be very inefficient as the data sets get larger and larger, in which case using one of supported index types (described below) is recommended. However, even when using indexes, it may be beneficial to execute the same query using brute force, in order to test recall by comparing the results returned by the (approximate) index-based search, and the (exact) brute-force search. To accomplish that, you can configure SimilaritySearch aggregator to ignore any configured index and to perform brute-force search anyway, by calling bruteForce method on the aggregator instance: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 10) .bruteForce(); Indexed Brute-Force Search It is possible to improve performance of a brute-force search by creating a forward-only index on the vector attribute using DeserializationAccelerator : <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new DeserializationAccelerator(Book::getSummaryEmbedding)); This will avoid repeated deserialization of Book values when performing brute-force search, at the cost of additional memory consumed by the indexed vector instances. The search will still perform the exact distance calculation, so the results will be exact, just like with the non-indexed brute-force search. ",
            "title": "Brute-force Search"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " HNSW index performs approximate vector search using Hierarchical Navigable Small World graphs , as described by Malkov and Yashunin. Coherence uses embedded native implementation of hnswlib for HNSW index implementation, so in order to use HNSW index you need to add a dependency on coherence-hnsw module, which contains all Java code and pre-built native libraries for Linux (ARM and x86), Mac (ARM and x86) and Windows (x86 only) that you need: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-hnsw&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;/dependency&gt; Once you add the dependency above, creating HNSW index is as simple as <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, 768)); The first argument to HnswIndex constructor is the extractor for the vector attribute to index, and the second is the number of dimensions each indexed vector will have (which must be identical), which will allow native index implementation to pre-allocate memory required for index. By default, HnswIndex will use cosine distance to calculate vector distances, but this can be overridden by specifying spaceName argument ina constructor: <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, \"L2\", 768)); The valid values for space name are COSINE , L2 and IP (inner product). HnswIndex also provides a number of options that can be used to fine-tune its behavior, which can be specified using fluent API: <markup lang=\"java\" >var hnsw = new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, 768) .setEfConstr(200) .setEfSearch(50) .setM(16) .setRandomSeed(100); books.addIndex(hnsw); The algorithm parameters above are described in more detail in hnswlib documentation . You can also specify maximum index size by calling setMaxElements method. By default, the index will be created with a maximum size of 4,096 elements, and will be resized as necessary to accommodate data set growth. However, resize operation is somewhat costly and can be avoided if you know ahead of time how many entries will be stored in a Coherence map you are creating the index on, in which case you should configure the index size accordingly. Note Remember that Coherence partitions indexes, so there will be as many instances of HNSW index as there are partitions. This means that the ideal maxElements settings is just a bit over mapSize / partitionCount , and not the actual map size, which would be way too big. Once you have HNSW index configured and created, you can simply perform searches the same way as we did earlier using brute-force search. Coherence will automatically detect and use HNSW index, if one is available. ",
            "title": "HNSW Index"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " Coherence also supports Binary Quantization -based index, which provides significant space savings (32x) compared to vector indexes that use float32 vectors, such as HNSW. It does this by converting each 32-bit float in the original vector into either 0 or 1, and representing it using a single bit in a BitSet . The downside is that the recall may not be as accurate, especially with smaller vectors, but that can be largely addressed by oversampling and re-scoring of the results, which Coherence automatically performs. BinaryQuantIndex is implemented in pure Java, and is a part of the main Coherence distribution, so it requires no additional dependencies. To create it, simply call NamedMap.addIndex method: <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new BinaryQuantIndex&lt;&gt;(Book::getSummaryEmbedding)); The only option you can specify is the oversamplingFactor , which is the multiplier for the maximum number of the results to return, and is 3 by default, meaning that if your search aggregator is configured to return 10 results, binary quantization search will initially return 30 results based on the Hamming distance between the binary representation of the search vector and index vectors, re-score all 30 results using exact distance calculation and then re-order and return top 10 results based on the calculated exact distance. To change oversamplingFactor , you can specify it using fluent API when creating an index <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new BinaryQuantIndex&lt;&gt;(Book::getSummaryEmbedding).oversamplingFactor(5)); which will cause SimilaritySearch aggregator to return and re-score 50 results initially instead of 30, in the example above. Just like with HNSW index, once you have Binary Quantization index configured and created, you can simply perform searches the same way as we did earlier using brute-force search. Coherence will automatically detect it and use it. ",
            "title": "Binary Quantization"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " While the brute force searches work fine with small data sets, as the data set gets larger it is highly recommended to create a vector index for a vector property. Coherence supports two vector index types out of the box: HNSW index and Binary Quantization index. HNSW Index HNSW index performs approximate vector search using Hierarchical Navigable Small World graphs , as described by Malkov and Yashunin. Coherence uses embedded native implementation of hnswlib for HNSW index implementation, so in order to use HNSW index you need to add a dependency on coherence-hnsw module, which contains all Java code and pre-built native libraries for Linux (ARM and x86), Mac (ARM and x86) and Windows (x86 only) that you need: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-hnsw&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;/dependency&gt; Once you add the dependency above, creating HNSW index is as simple as <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, 768)); The first argument to HnswIndex constructor is the extractor for the vector attribute to index, and the second is the number of dimensions each indexed vector will have (which must be identical), which will allow native index implementation to pre-allocate memory required for index. By default, HnswIndex will use cosine distance to calculate vector distances, but this can be overridden by specifying spaceName argument ina constructor: <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, \"L2\", 768)); The valid values for space name are COSINE , L2 and IP (inner product). HnswIndex also provides a number of options that can be used to fine-tune its behavior, which can be specified using fluent API: <markup lang=\"java\" >var hnsw = new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, 768) .setEfConstr(200) .setEfSearch(50) .setM(16) .setRandomSeed(100); books.addIndex(hnsw); The algorithm parameters above are described in more detail in hnswlib documentation . You can also specify maximum index size by calling setMaxElements method. By default, the index will be created with a maximum size of 4,096 elements, and will be resized as necessary to accommodate data set growth. However, resize operation is somewhat costly and can be avoided if you know ahead of time how many entries will be stored in a Coherence map you are creating the index on, in which case you should configure the index size accordingly. Note Remember that Coherence partitions indexes, so there will be as many instances of HNSW index as there are partitions. This means that the ideal maxElements settings is just a bit over mapSize / partitionCount , and not the actual map size, which would be way too big. Once you have HNSW index configured and created, you can simply perform searches the same way as we did earlier using brute-force search. Coherence will automatically detect and use HNSW index, if one is available. Binary Quantization Coherence also supports Binary Quantization -based index, which provides significant space savings (32x) compared to vector indexes that use float32 vectors, such as HNSW. It does this by converting each 32-bit float in the original vector into either 0 or 1, and representing it using a single bit in a BitSet . The downside is that the recall may not be as accurate, especially with smaller vectors, but that can be largely addressed by oversampling and re-scoring of the results, which Coherence automatically performs. BinaryQuantIndex is implemented in pure Java, and is a part of the main Coherence distribution, so it requires no additional dependencies. To create it, simply call NamedMap.addIndex method: <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new BinaryQuantIndex&lt;&gt;(Book::getSummaryEmbedding)); The only option you can specify is the oversamplingFactor , which is the multiplier for the maximum number of the results to return, and is 3 by default, meaning that if your search aggregator is configured to return 10 results, binary quantization search will initially return 30 results based on the Hamming distance between the binary representation of the search vector and index vectors, re-score all 30 results using exact distance calculation and then re-order and return top 10 results based on the calculated exact distance. To change oversamplingFactor , you can specify it using fluent API when creating an index <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new BinaryQuantIndex&lt;&gt;(Book::getSummaryEmbedding).oversamplingFactor(5)); which will cause SimilaritySearch aggregator to return and re-score 50 results initially instead of 30, in the example above. Just like with HNSW index, once you have Binary Quantization index configured and created, you can simply perform searches the same way as we did earlier using brute-force search. Coherence will automatically detect it and use it. ",
            "title": "Index-based Search"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " In addition to vector-based similarity search, you can use standard Coherence filters to perform metadata-based filtering of the results. For example, if we only wanted to search books by a specific author, we could specify a metadata filter SimilaritySearch aggregator should use in conjunction with a vector similarity search: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 3) .filter(Filters.equal(Book::getAuthor, \"Jules Verne\")); var results = books.aggregate(search); The above should return only top 3 books written by Jules Verne, sorted according to vector similarity. Metadata filtering works the same regardless of whether you use brute-force or index-based search, and will use any indexes you may have on the metadata attributes you are filtering on, such as Book::getAuthor in this case, to speed up filter evaluation. If you are a long-time Coherence user, you may be wondering why we are setting the filter on the aggregator itself and performing filter evaluation inside the aggregator, instead of using aggregate method that accepts a filter and allows us to pre-filter the set of entries to aggregate. The reason is that both vector index implementations need to evaluate the filter internally, and only include the result if it evaluates to true , so the example above will work in all situations. However, if you are using brute-force search, you may achieve the same result, and likely improve performance, by pre-filtering the entries: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 3); var results = books.aggregate(Filters.equal(Book::getAuthor, \"Jules Verne\"), search); ",
            "title": "Metadata Filtering"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " To perform similarity search against vectors stored in Coherence you can use SimilaritySearch aggregator. The easiest way to construct one is by using Aggregators.similaritySearch factory method. You need to specify three arguments when constructing the SimilaritySearch aggregator: A ValueExtractor that should be used to retrieve the vector attribute from the map entries The search vector to compare the extracted values with, and The maximum number of the results to return For example, to search the map containing Book objects, and return up to 10 most similar books, you would create SimilaritySearch aggregator instance like this: <markup lang=\"java\" >var searchVector = createEmbedding(searchQuery); // outside of Coherence var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 10); By default, the aggregator will use cosine distance to calculate distance between vectors, but you can change that by calling fluent algorithm method on the created aggregator instance and passing an instance of a different DistanceAlgorithm implementation: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 10) .algorithm(new L2SquaredDistance()); Out of the box Coherence provides CosineDistance , L2SquaredDistance and InnerProductDistance implementation, but you can easily add support for additional algorithms by implementing DistanceAlgorithm interface yourself. Once you have an instance of a SimilaritySearch aggregator, you can perform similarity search by calling NamedMap.aggregate method like you normally would: <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); List&lt;QueryResult&lt;String, Book&gt;&gt; results = books.aggregate(search); The result of the search is a list of up to maximum specified QueryResult objects (10, in the example above), which contain entry key, value, and calculated distance between the search vector and a vector extracted from the specified entry. The results are sorted by distance, in ascending order, from closest to farthest. Brute-force Search By default, if no index is defined for the vector attribute, Coherence will perform a brute-force search by deserializing every entry, extracting the vector attribute from it, and performing distance calculation between the extracted vector and the search vector using specified distance algorithm. This is fine for small or medium-sized data sets, because Coherence will still perform search in parallel across cluster members and aggregate the results, but can be very inefficient as the data sets get larger and larger, in which case using one of supported index types (described below) is recommended. However, even when using indexes, it may be beneficial to execute the same query using brute force, in order to test recall by comparing the results returned by the (approximate) index-based search, and the (exact) brute-force search. To accomplish that, you can configure SimilaritySearch aggregator to ignore any configured index and to perform brute-force search anyway, by calling bruteForce method on the aggregator instance: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 10) .bruteForce(); Indexed Brute-Force Search It is possible to improve performance of a brute-force search by creating a forward-only index on the vector attribute using DeserializationAccelerator : <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new DeserializationAccelerator(Book::getSummaryEmbedding)); This will avoid repeated deserialization of Book values when performing brute-force search, at the cost of additional memory consumed by the indexed vector instances. The search will still perform the exact distance calculation, so the results will be exact, just like with the non-indexed brute-force search. Index-based Search While the brute force searches work fine with small data sets, as the data set gets larger it is highly recommended to create a vector index for a vector property. Coherence supports two vector index types out of the box: HNSW index and Binary Quantization index. HNSW Index HNSW index performs approximate vector search using Hierarchical Navigable Small World graphs , as described by Malkov and Yashunin. Coherence uses embedded native implementation of hnswlib for HNSW index implementation, so in order to use HNSW index you need to add a dependency on coherence-hnsw module, which contains all Java code and pre-built native libraries for Linux (ARM and x86), Mac (ARM and x86) and Windows (x86 only) that you need: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-hnsw&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;/dependency&gt; Once you add the dependency above, creating HNSW index is as simple as <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, 768)); The first argument to HnswIndex constructor is the extractor for the vector attribute to index, and the second is the number of dimensions each indexed vector will have (which must be identical), which will allow native index implementation to pre-allocate memory required for index. By default, HnswIndex will use cosine distance to calculate vector distances, but this can be overridden by specifying spaceName argument ina constructor: <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, \"L2\", 768)); The valid values for space name are COSINE , L2 and IP (inner product). HnswIndex also provides a number of options that can be used to fine-tune its behavior, which can be specified using fluent API: <markup lang=\"java\" >var hnsw = new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, 768) .setEfConstr(200) .setEfSearch(50) .setM(16) .setRandomSeed(100); books.addIndex(hnsw); The algorithm parameters above are described in more detail in hnswlib documentation . You can also specify maximum index size by calling setMaxElements method. By default, the index will be created with a maximum size of 4,096 elements, and will be resized as necessary to accommodate data set growth. However, resize operation is somewhat costly and can be avoided if you know ahead of time how many entries will be stored in a Coherence map you are creating the index on, in which case you should configure the index size accordingly. Note Remember that Coherence partitions indexes, so there will be as many instances of HNSW index as there are partitions. This means that the ideal maxElements settings is just a bit over mapSize / partitionCount , and not the actual map size, which would be way too big. Once you have HNSW index configured and created, you can simply perform searches the same way as we did earlier using brute-force search. Coherence will automatically detect and use HNSW index, if one is available. Binary Quantization Coherence also supports Binary Quantization -based index, which provides significant space savings (32x) compared to vector indexes that use float32 vectors, such as HNSW. It does this by converting each 32-bit float in the original vector into either 0 or 1, and representing it using a single bit in a BitSet . The downside is that the recall may not be as accurate, especially with smaller vectors, but that can be largely addressed by oversampling and re-scoring of the results, which Coherence automatically performs. BinaryQuantIndex is implemented in pure Java, and is a part of the main Coherence distribution, so it requires no additional dependencies. To create it, simply call NamedMap.addIndex method: <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new BinaryQuantIndex&lt;&gt;(Book::getSummaryEmbedding)); The only option you can specify is the oversamplingFactor , which is the multiplier for the maximum number of the results to return, and is 3 by default, meaning that if your search aggregator is configured to return 10 results, binary quantization search will initially return 30 results based on the Hamming distance between the binary representation of the search vector and index vectors, re-score all 30 results using exact distance calculation and then re-order and return top 10 results based on the calculated exact distance. To change oversamplingFactor , you can specify it using fluent API when creating an index <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new BinaryQuantIndex&lt;&gt;(Book::getSummaryEmbedding).oversamplingFactor(5)); which will cause SimilaritySearch aggregator to return and re-score 50 results initially instead of 30, in the example above. Just like with HNSW index, once you have Binary Quantization index configured and created, you can simply perform searches the same way as we did earlier using brute-force search. Coherence will automatically detect it and use it. Metadata Filtering In addition to vector-based similarity search, you can use standard Coherence filters to perform metadata-based filtering of the results. For example, if we only wanted to search books by a specific author, we could specify a metadata filter SimilaritySearch aggregator should use in conjunction with a vector similarity search: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 3) .filter(Filters.equal(Book::getAuthor, \"Jules Verne\")); var results = books.aggregate(search); The above should return only top 3 books written by Jules Verne, sorted according to vector similarity. Metadata filtering works the same regardless of whether you use brute-force or index-based search, and will use any indexes you may have on the metadata attributes you are filtering on, such as Book::getAuthor in this case, to speed up filter evaluation. If you are a long-time Coherence user, you may be wondering why we are setting the filter on the aggregator itself and performing filter evaluation inside the aggregator, instead of using aggregate method that accepts a filter and allows us to pre-filter the set of entries to aggregate. The reason is that both vector index implementations need to evaluate the filter internally, and only include the result if it evaluates to true , so the example above will work in all situations. However, if you are using brute-force search, you may achieve the same result, and likely improve performance, by pre-filtering the entries: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 3); var results = books.aggregate(Filters.equal(Book::getAuthor, \"Jules Verne\"), search); ",
            "title": "Performing Similarity Search"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " Coherence provides DocumentChunk class, which can be used to represent document chunks containing text, embedding and metadata, as typically represented in various RAG frameworks. While this class can certainly be used on its own, its main purpose is to support various RAG framework integrations in a consistent manner. ",
            "title": "Retrieval Augmented Generation (RAG) Support"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " TBD ",
            "title": "LangChain (Python)"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " langchain4j-coherence module provides implementation of EmbeddingStore and ChatMemoryStore interfaces, allowing Coherence to be easily used as either (or both) in LangChain4j applications. It also provides a Spring Boot Starter for LangChain4j applications via langchain4j-coherence-spring-boot-starter module. For more information, see documentation provided by the integration modules above. ",
            "title": "LangChain4j"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " spring-ai-coherence-store module provides implementation of VectorStore interface, allowing Coherence to be easily used as such in Spring AI applications. It also provides a Spring Boot Starter via spring-ai-starter-coherence-store module. For more information, see documentation provided by the integration modules above. ",
            "title": "Spring AI"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " The following sections describe integrations with popular AI frameworks that Coherence supports. These integrations are not part of Coherence itself, but the contributions we&#8217;ve made to the frameworks below to allow them to use Coherence as a Vector Store, Chat Memory, etc. LangChain (Python) TBD LangChain4j langchain4j-coherence module provides implementation of EmbeddingStore and ChatMemoryStore interfaces, allowing Coherence to be easily used as either (or both) in LangChain4j applications. It also provides a Spring Boot Starter for LangChain4j applications via langchain4j-coherence-spring-boot-starter module. For more information, see documentation provided by the integration modules above. Spring AI spring-ai-coherence-store module provides implementation of VectorStore interface, allowing Coherence to be easily used as such in Spring AI applications. It also provides a Spring Boot Starter via spring-ai-starter-coherence-store module. For more information, see documentation provided by the integration modules above. ",
            "title": "Integrations"
        },
        {
            "location": "/docs/core/08_vector_db",
            "text": " With the increased popularity of Gen AI, and Retrieval Augmented Generation (RAG) use cases in particular, the need for a way to store and search a large number of dense vector embeddings efficiently is larger than ever. Coherence already provides a number of features that make this possible, such as efficient serialization format, filters and aggregators, which allow users to search across large data sets in parallel by leveraging all the CPU cores in a cluster, and gRPC proxy, which allows remote clients written in any supported language, including Python, to access data in a Coherence cluster efficiently. This release adds the missing bits that turn Coherence into a full-fledged Vector Database: Built-in support for Vector Types float32 , int8 and bit dense vectors of arbitrary dimension Built-in support for Semantic Search, including HNSW indexing Binary Quantization Index-optimized Exact Searches Metadata Filtering Built-in support for Document Chunks, addressing a common RAG use case Integration with LangChain and LangChain4j Integration with Spring AI The following sections provide more details about each of the features above. Vector Types To support arbitrary vector types, Coherence provides com.oracle.coherence.ai.Vector&lt;T&gt; interface, with three built-in implementations: BitVector , which internally uses a java.util.Bitset to represent each vector element using a single bit, Int8Vector , which internally uses a byte[] , and Float32Vector , which internally uses a float[] . These types allow users to add a vector property to their own classes the same way they would add any other property: by simply creating a field and accessors for it: <markup lang=\"java\" title=\"Book.java\" >@PortableType(id = 2001) public class Book { @Portable private String isbn; @Portable private String title; @Portable private String author; @Portable private String summary; @Portable private Vector&lt;float[]&gt; summaryEmbedding; // constructors, getters and setters omitted } In the example above, the summaryEmbedding field is used to store vector representation of the summary field, so we can use vector similarity to search book summaries. In the subsequent sections, we&#8217;ll discuss how the summaryEmbedding property can be used to define both standard and vector indexes, and to perform similarity search against them. Performing Similarity Search To perform similarity search against vectors stored in Coherence you can use SimilaritySearch aggregator. The easiest way to construct one is by using Aggregators.similaritySearch factory method. You need to specify three arguments when constructing the SimilaritySearch aggregator: A ValueExtractor that should be used to retrieve the vector attribute from the map entries The search vector to compare the extracted values with, and The maximum number of the results to return For example, to search the map containing Book objects, and return up to 10 most similar books, you would create SimilaritySearch aggregator instance like this: <markup lang=\"java\" >var searchVector = createEmbedding(searchQuery); // outside of Coherence var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 10); By default, the aggregator will use cosine distance to calculate distance between vectors, but you can change that by calling fluent algorithm method on the created aggregator instance and passing an instance of a different DistanceAlgorithm implementation: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 10) .algorithm(new L2SquaredDistance()); Out of the box Coherence provides CosineDistance , L2SquaredDistance and InnerProductDistance implementation, but you can easily add support for additional algorithms by implementing DistanceAlgorithm interface yourself. Once you have an instance of a SimilaritySearch aggregator, you can perform similarity search by calling NamedMap.aggregate method like you normally would: <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); List&lt;QueryResult&lt;String, Book&gt;&gt; results = books.aggregate(search); The result of the search is a list of up to maximum specified QueryResult objects (10, in the example above), which contain entry key, value, and calculated distance between the search vector and a vector extracted from the specified entry. The results are sorted by distance, in ascending order, from closest to farthest. Brute-force Search By default, if no index is defined for the vector attribute, Coherence will perform a brute-force search by deserializing every entry, extracting the vector attribute from it, and performing distance calculation between the extracted vector and the search vector using specified distance algorithm. This is fine for small or medium-sized data sets, because Coherence will still perform search in parallel across cluster members and aggregate the results, but can be very inefficient as the data sets get larger and larger, in which case using one of supported index types (described below) is recommended. However, even when using indexes, it may be beneficial to execute the same query using brute force, in order to test recall by comparing the results returned by the (approximate) index-based search, and the (exact) brute-force search. To accomplish that, you can configure SimilaritySearch aggregator to ignore any configured index and to perform brute-force search anyway, by calling bruteForce method on the aggregator instance: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 10) .bruteForce(); Indexed Brute-Force Search It is possible to improve performance of a brute-force search by creating a forward-only index on the vector attribute using DeserializationAccelerator : <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new DeserializationAccelerator(Book::getSummaryEmbedding)); This will avoid repeated deserialization of Book values when performing brute-force search, at the cost of additional memory consumed by the indexed vector instances. The search will still perform the exact distance calculation, so the results will be exact, just like with the non-indexed brute-force search. Index-based Search While the brute force searches work fine with small data sets, as the data set gets larger it is highly recommended to create a vector index for a vector property. Coherence supports two vector index types out of the box: HNSW index and Binary Quantization index. HNSW Index HNSW index performs approximate vector search using Hierarchical Navigable Small World graphs , as described by Malkov and Yashunin. Coherence uses embedded native implementation of hnswlib for HNSW index implementation, so in order to use HNSW index you need to add a dependency on coherence-hnsw module, which contains all Java code and pre-built native libraries for Linux (ARM and x86), Mac (ARM and x86) and Windows (x86 only) that you need: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-hnsw&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;/dependency&gt; Once you add the dependency above, creating HNSW index is as simple as <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, 768)); The first argument to HnswIndex constructor is the extractor for the vector attribute to index, and the second is the number of dimensions each indexed vector will have (which must be identical), which will allow native index implementation to pre-allocate memory required for index. By default, HnswIndex will use cosine distance to calculate vector distances, but this can be overridden by specifying spaceName argument ina constructor: <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, \"L2\", 768)); The valid values for space name are COSINE , L2 and IP (inner product). HnswIndex also provides a number of options that can be used to fine-tune its behavior, which can be specified using fluent API: <markup lang=\"java\" >var hnsw = new HnswIndex&lt;&gt;(Book::getSummaryEmbedding, 768) .setEfConstr(200) .setEfSearch(50) .setM(16) .setRandomSeed(100); books.addIndex(hnsw); The algorithm parameters above are described in more detail in hnswlib documentation . You can also specify maximum index size by calling setMaxElements method. By default, the index will be created with a maximum size of 4,096 elements, and will be resized as necessary to accommodate data set growth. However, resize operation is somewhat costly and can be avoided if you know ahead of time how many entries will be stored in a Coherence map you are creating the index on, in which case you should configure the index size accordingly. Note Remember that Coherence partitions indexes, so there will be as many instances of HNSW index as there are partitions. This means that the ideal maxElements settings is just a bit over mapSize / partitionCount , and not the actual map size, which would be way too big. Once you have HNSW index configured and created, you can simply perform searches the same way as we did earlier using brute-force search. Coherence will automatically detect and use HNSW index, if one is available. Binary Quantization Coherence also supports Binary Quantization -based index, which provides significant space savings (32x) compared to vector indexes that use float32 vectors, such as HNSW. It does this by converting each 32-bit float in the original vector into either 0 or 1, and representing it using a single bit in a BitSet . The downside is that the recall may not be as accurate, especially with smaller vectors, but that can be largely addressed by oversampling and re-scoring of the results, which Coherence automatically performs. BinaryQuantIndex is implemented in pure Java, and is a part of the main Coherence distribution, so it requires no additional dependencies. To create it, simply call NamedMap.addIndex method: <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new BinaryQuantIndex&lt;&gt;(Book::getSummaryEmbedding)); The only option you can specify is the oversamplingFactor , which is the multiplier for the maximum number of the results to return, and is 3 by default, meaning that if your search aggregator is configured to return 10 results, binary quantization search will initially return 30 results based on the Hamming distance between the binary representation of the search vector and index vectors, re-score all 30 results using exact distance calculation and then re-order and return top 10 results based on the calculated exact distance. To change oversamplingFactor , you can specify it using fluent API when creating an index <markup lang=\"java\" >NamedMap&lt;String, Book&gt; books = session.getMap(\"books\"); books.addIndex(new BinaryQuantIndex&lt;&gt;(Book::getSummaryEmbedding).oversamplingFactor(5)); which will cause SimilaritySearch aggregator to return and re-score 50 results initially instead of 30, in the example above. Just like with HNSW index, once you have Binary Quantization index configured and created, you can simply perform searches the same way as we did earlier using brute-force search. Coherence will automatically detect it and use it. Metadata Filtering In addition to vector-based similarity search, you can use standard Coherence filters to perform metadata-based filtering of the results. For example, if we only wanted to search books by a specific author, we could specify a metadata filter SimilaritySearch aggregator should use in conjunction with a vector similarity search: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 3) .filter(Filters.equal(Book::getAuthor, \"Jules Verne\")); var results = books.aggregate(search); The above should return only top 3 books written by Jules Verne, sorted according to vector similarity. Metadata filtering works the same regardless of whether you use brute-force or index-based search, and will use any indexes you may have on the metadata attributes you are filtering on, such as Book::getAuthor in this case, to speed up filter evaluation. If you are a long-time Coherence user, you may be wondering why we are setting the filter on the aggregator itself and performing filter evaluation inside the aggregator, instead of using aggregate method that accepts a filter and allows us to pre-filter the set of entries to aggregate. The reason is that both vector index implementations need to evaluate the filter internally, and only include the result if it evaluates to true , so the example above will work in all situations. However, if you are using brute-force search, you may achieve the same result, and likely improve performance, by pre-filtering the entries: <markup lang=\"java\" >var search = Aggregators.similaritySearch(Book::getSummaryEmbedding, searchVector, 3); var results = books.aggregate(Filters.equal(Book::getAuthor, \"Jules Verne\"), search); Retrieval Augmented Generation (RAG) Support Coherence provides DocumentChunk class, which can be used to represent document chunks containing text, embedding and metadata, as typically represented in various RAG frameworks. While this class can certainly be used on its own, its main purpose is to support various RAG framework integrations in a consistent manner. Integrations The following sections describe integrations with popular AI frameworks that Coherence supports. These integrations are not part of Coherence itself, but the contributions we&#8217;ve made to the frameworks below to allow them to use Coherence as a Vector Store, Chat Memory, etc. LangChain (Python) TBD LangChain4j langchain4j-coherence module provides implementation of EmbeddingStore and ChatMemoryStore interfaces, allowing Coherence to be easily used as either (or both) in LangChain4j applications. It also provides a Spring Boot Starter for LangChain4j applications via langchain4j-coherence-spring-boot-starter module. For more information, see documentation provided by the integration modules above. Spring AI spring-ai-coherence-store module provides implementation of VectorStore interface, allowing Coherence to be easily used as such in Spring AI applications. It also provides a Spring Boot Starter via spring-ai-starter-coherence-store module. For more information, see documentation provided by the integration modules above. ",
            "title": "Vector DB"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " A Coherence application JVM can only be a member of zero or one Coherence cluster at any point in time. It can however, connect to zero, one or many Coherence clusters as a client. When building a Coherence client application (either an Extend client or a gRPC client) the application may need to connect to more than one Coherence cluster. Exactly how this is achieved, and the relative simplicity, depends on the version of Coherence being used. This example uses the Bootstrap API introduced in Coherence CE 20.12, and enhancements made to it in 22.06. ",
            "title": "Multi-Cluster Client"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The withParameter method on the SessionConfiguration.Builder is used to pass parameters to the cache configuration. Cache configuration files can be parameterized using the system-property attribute on elements. Typically, the values used for these elements are taken from corresponding system properties or environment variables. By using the withParameter method on a SessionConfiguration.Builder values for these elements can also be provided. For example, the &lt;remote-cache-scheme&gt; below has the &lt;address&gt; and &lt;port&gt; elements parameterized. The &lt;address&gt; element&#8217;s value will come from the coherence.extend.address System property (or COHERENCE_EXTEND_ADDRESS environment variable). The &lt;port&gt; element&#8217;s value will come from the coherence.extend.port System property (or COHERENCE_EXTEND_PORT environment variable). There are a number of alternative ways to configures the address for a remote gRPC scheme, which are covered on the Coherence documentation. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote-fixed&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.extend.address\"/&gt; &lt;port system-property=\"coherence.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; When creating a SessionConfiguration , those values can also be specified as configuration parameters. <markup lang=\"java\" >SessionConfiguration.builder() .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.extend.address\", \"127.0.0.1\") .withParameter(\"coherence.extend.port\", 20000) ",
            "title": "Session Configuration Parameters"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " Creating an Extend client could be as simple as the example below. This example will create an Extend client using the default Coherence cache configuration (as the withConfigUri has not been used to specify a cache configuration file). Both the session name and scope name are set to the tenant name. A number of parameters are also set in the configuration, this is explained further below. <markup lang=\"java\" >public Session getSession(String tenant) { Coherence coherence = Coherence.getInstance(); Optional&lt;Session&gt; optional = Coherence.findSession(tenant); if (optional.isPresent()) { return optional.get(); } coherence.addSessionIfAbsent(tenant, () -&gt; SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withParameter(\"coherence.client\", \"remote-fixed\") .withParameter(\"coherence.serializer\", \"java\") .withParameter(\"coherence.extend.address\", \"127.0.0.1\") .withParameter(\"coherence.extend.port\", 20000) .build()); return coherence.getSession(tenant); } Obtain the default Coherence instance Find the Session with the tenant name The Session has already been created, so use it Use the Coherence.addSessionIfAbsent() method to add a SessionConfiguration for the tenant. The addSessionIfAbsent method is used to be slightly more thread safe. Return the Session for the configuration name just added Session Configuration Parameters The withParameter method on the SessionConfiguration.Builder is used to pass parameters to the cache configuration. Cache configuration files can be parameterized using the system-property attribute on elements. Typically, the values used for these elements are taken from corresponding system properties or environment variables. By using the withParameter method on a SessionConfiguration.Builder values for these elements can also be provided. For example, the &lt;remote-cache-scheme&gt; below has the &lt;address&gt; and &lt;port&gt; elements parameterized. The &lt;address&gt; element&#8217;s value will come from the coherence.extend.address System property (or COHERENCE_EXTEND_ADDRESS environment variable). The &lt;port&gt; element&#8217;s value will come from the coherence.extend.port System property (or COHERENCE_EXTEND_PORT environment variable). There are a number of alternative ways to configures the address for a remote gRPC scheme, which are covered on the Coherence documentation. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote-fixed&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.extend.address\"/&gt; &lt;port system-property=\"coherence.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; When creating a SessionConfiguration , those values can also be specified as configuration parameters. <markup lang=\"java\" >SessionConfiguration.builder() .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.extend.address\", \"127.0.0.1\") .withParameter(\"coherence.extend.port\", 20000) ",
            "title": "Create an Extend Client Session"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " Since Coherence 22.06.2, creating a gRPC client session is a simple as creating an Extend client session. A &lt;remote-grpc-cache-scheme&gt; can be configured in a Coherence cache configuration file. The &lt;remote-grpc-cache-scheme&gt; can contain a &lt;grpc-channel&gt; element that configures the channel that the client will use to connect to the gRPC proxy in the Coherence cluster. There are a number of alternative ways to configure the &lt;remote-grpc-cache-scheme&gt; and &lt;grpc-channel&gt; elements, which are covered on the Coherence documentation. An example of a &lt;remote-grpc-cache-scheme&gt; is shown below. In this case the &lt;grpc-channel&gt; is configured with a single fixed address that the gRPC client connects to. The &lt;address&gt; and &lt;port&gt; elements below do not actually have values, the values of those elements will be supplied by the coherence.grpc.address and coherence.grpc.port system properties or by the COHERENCE_GRPC_ADDRESS and COHERENCE_GRPC_PORT environment variables, or by setting them in the Session configuration properties. <markup lang=\"xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;thin-grpc-fixed&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.grpc.address\"/&gt; &lt;port system-property=\"coherence.grpc.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; When creating a SessionConfiguration , the &lt;address&gt; and &lt;port&gt; values can also be specified as configuration parameters. For example, the SessionConfiguration below will configure the gRPC channel to connect to loopback ( 127.0.0.1 ) and port 1408 . <markup lang=\"java\" >SessionConfiguration.builder() .withMode(Coherence.Mode.GrpcFixed) .withParameter(\"coherence.grpc.address\", \"127.0.0.1\") .withParameter(\"coherence.grpc.port\", 1408) ",
            "title": "Create a gRPC Client Session"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " Using the bootstrap API, access to Coherence resources is via an instance of com.tangosol.net.Session where either the default Session is used, or one or more sessions are configured at start-up. In a multi-tenant client, where all the tenants are known ahead of time, this pattern is still usable, a client Session can be configured at start-up for each tenant. In this example the tenant list can be dynamic, to make the example a little more interesting and to show how to create a new Session at runtime. The bootstrap API starts one or more Coherence instances, each of which manages one or more Session instances. For the multi-tenant use case there is no need to have multiple Coherence instances. The default instance can be used, then new tenant specific client sessions are created as required. Every Session must have a unique name (and for an Extend client session, either a unique cache configuration URI, or a unique scope name). This example uses the same cache configuration file for all tenants (which is how most multi-tenant applications would work) so the session&#8217;s name and scope name are both set to the tenant name to ensure uniqueness. Create an Extend Client Session Creating an Extend client could be as simple as the example below. This example will create an Extend client using the default Coherence cache configuration (as the withConfigUri has not been used to specify a cache configuration file). Both the session name and scope name are set to the tenant name. A number of parameters are also set in the configuration, this is explained further below. <markup lang=\"java\" >public Session getSession(String tenant) { Coherence coherence = Coherence.getInstance(); Optional&lt;Session&gt; optional = Coherence.findSession(tenant); if (optional.isPresent()) { return optional.get(); } coherence.addSessionIfAbsent(tenant, () -&gt; SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withParameter(\"coherence.client\", \"remote-fixed\") .withParameter(\"coherence.serializer\", \"java\") .withParameter(\"coherence.extend.address\", \"127.0.0.1\") .withParameter(\"coherence.extend.port\", 20000) .build()); return coherence.getSession(tenant); } Obtain the default Coherence instance Find the Session with the tenant name The Session has already been created, so use it Use the Coherence.addSessionIfAbsent() method to add a SessionConfiguration for the tenant. The addSessionIfAbsent method is used to be slightly more thread safe. Return the Session for the configuration name just added Session Configuration Parameters The withParameter method on the SessionConfiguration.Builder is used to pass parameters to the cache configuration. Cache configuration files can be parameterized using the system-property attribute on elements. Typically, the values used for these elements are taken from corresponding system properties or environment variables. By using the withParameter method on a SessionConfiguration.Builder values for these elements can also be provided. For example, the &lt;remote-cache-scheme&gt; below has the &lt;address&gt; and &lt;port&gt; elements parameterized. The &lt;address&gt; element&#8217;s value will come from the coherence.extend.address System property (or COHERENCE_EXTEND_ADDRESS environment variable). The &lt;port&gt; element&#8217;s value will come from the coherence.extend.port System property (or COHERENCE_EXTEND_PORT environment variable). There are a number of alternative ways to configures the address for a remote gRPC scheme, which are covered on the Coherence documentation. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote-fixed&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.extend.address\"/&gt; &lt;port system-property=\"coherence.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; When creating a SessionConfiguration , those values can also be specified as configuration parameters. <markup lang=\"java\" >SessionConfiguration.builder() .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.extend.address\", \"127.0.0.1\") .withParameter(\"coherence.extend.port\", 20000) Create a gRPC Client Session Since Coherence 22.06.2, creating a gRPC client session is a simple as creating an Extend client session. A &lt;remote-grpc-cache-scheme&gt; can be configured in a Coherence cache configuration file. The &lt;remote-grpc-cache-scheme&gt; can contain a &lt;grpc-channel&gt; element that configures the channel that the client will use to connect to the gRPC proxy in the Coherence cluster. There are a number of alternative ways to configure the &lt;remote-grpc-cache-scheme&gt; and &lt;grpc-channel&gt; elements, which are covered on the Coherence documentation. An example of a &lt;remote-grpc-cache-scheme&gt; is shown below. In this case the &lt;grpc-channel&gt; is configured with a single fixed address that the gRPC client connects to. The &lt;address&gt; and &lt;port&gt; elements below do not actually have values, the values of those elements will be supplied by the coherence.grpc.address and coherence.grpc.port system properties or by the COHERENCE_GRPC_ADDRESS and COHERENCE_GRPC_PORT environment variables, or by setting them in the Session configuration properties. <markup lang=\"xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;thin-grpc-fixed&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.grpc.address\"/&gt; &lt;port system-property=\"coherence.grpc.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; When creating a SessionConfiguration , the &lt;address&gt; and &lt;port&gt; values can also be specified as configuration parameters. For example, the SessionConfiguration below will configure the gRPC channel to connect to loopback ( 127.0.0.1 ) and port 1408 . <markup lang=\"java\" >SessionConfiguration.builder() .withMode(Coherence.Mode.GrpcFixed) .withParameter(\"coherence.grpc.address\", \"127.0.0.1\") .withParameter(\"coherence.grpc.port\", 1408) ",
            "title": "Client Sessions"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The simplest way to run the example is to build the image and run the application web-server in a container. The example Maven and Gradle build files contain tasks to pull together all the dependencies and docker file into a directory. For Maven this will be target/docker and for Gradle this will be build/docker . The build then executes the Docker build command in that directory to build the image. Using Maven: <markup lang=\"bash\" >./mvnw clean package -DskipTests -P build-image Using Gradle <markup lang=\"bash\" >./gradlew clean buildImage Both of the commands above will create two images, one for the example server and one for the client Server image ghcr.io/coherence-community/multi-cluster-server:latest Client image ghcr.io/coherence-community/multi-cluster-client:latest ",
            "title": "Build the Example Image"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Build the Example Image The simplest way to run the example is to build the image and run the application web-server in a container. The example Maven and Gradle build files contain tasks to pull together all the dependencies and docker file into a directory. For Maven this will be target/docker and for Gradle this will be build/docker . The build then executes the Docker build command in that directory to build the image. Using Maven: <markup lang=\"bash\" >./mvnw clean package -DskipTests -P build-image Using Gradle <markup lang=\"bash\" >./gradlew clean buildImage Both of the commands above will create two images, one for the example server and one for the client Server image ghcr.io/coherence-community/multi-cluster-server:latest Client image ghcr.io/coherence-community/multi-cluster-client:latest ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The example application is a web-server that gets data to service requests from a specific Coherence cluster depending on a header value in the http request. This demonstrates a simple stateless multi-tenant web-server, where the tenant&#8217;s data is segregated into different Coherence clusters. The information about a tenant&#8217;s connection details are held in a meta-data cache in a separate admin cluster. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Build the Example Image The simplest way to run the example is to build the image and run the application web-server in a container. The example Maven and Gradle build files contain tasks to pull together all the dependencies and docker file into a directory. For Maven this will be target/docker and for Gradle this will be build/docker . The build then executes the Docker build command in that directory to build the image. Using Maven: <markup lang=\"bash\" >./mvnw clean package -DskipTests -P build-image Using Gradle <markup lang=\"bash\" >./gradlew clean buildImage Both of the commands above will create two images, one for the example server and one for the client Server image ghcr.io/coherence-community/multi-cluster-server:latest Client image ghcr.io/coherence-community/multi-cluster-client:latest ",
            "title": "Building the Example"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " So that the client can communicate with the cluster members, a Docker network is required. The command below will create a Docker network named coherence-net <markup lang=\"bash\" >docker network create --driver bridge coherence-net ",
            "title": "Create a Docker Network"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The example requires three clusters. The first is the tenant \"admin\" cluster that holds information about the tenants. Then there are two additional clusters for each tenant, in this case \"Marvel\" and \"Star Wars\" Start the admin cluster, this will hold tenant meta-data. <markup lang=\"bash\" >docker run -d --name tenants --network coherence-net \\ -e COHERENCE_CLUSTER=tenants \\ ghcr.io/coherence-community/multi-cluster-server:latest Start the cluster for the Marvel tenant. <markup lang=\"bash\" >docker run -d --name marvel --network coherence-net \\ -e COHERENCE_CLUSTER=marvel \\ ghcr.io/coherence-community/multi-cluster-server:latest Start the cluster for the Star Wars tenant. <markup lang=\"bash\" >docker run -d --name star-wars --network coherence-net \\ -e COHERENCE_CLUSTER=star-wars \\ ghcr.io/coherence-community/multi-cluster-server:latest After starting all three clusters, the docker ps command can be used to check their status. Eventually the STATUS colum of each container should say (healthy) . <markup >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4abdc735b7bd ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 2 minutes ago Up 2 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp star-wars 5df54737eb6a ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 2 minutes ago Up 2 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp marvel 87f9ee53dfc5 ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 3 minutes ago Up 3 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp tenants ",
            "title": "Start the Coherence Clusters"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " When all the clusters are running and healthy, the multi-tenant client can be started using the command below. This will start the webserver and expose the endpoints on http://127.0.0.1:8080 . <markup lang=\"bash\" >docker run -d --name webserver --network coherence-net \\ -e COHERENCE_EXTEND_ADDRESS=tenants \\ -e COHERENCE_EXTEND_PORT=20000 \\ -p 8080:8080 \\ ghcr.io/coherence-community/multi-cluster-client:latest Using docker ps the status of the webserver container should eventually be (healthy) too. ",
            "title": "Start the Web-Server"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " Once the webserver container is healthy the /tenants endpoint can be used to create the metadata for the two tenants. The curl command below will add the meta-data for the Marvel tenant. This will connect to the Marvel cluster using Coherence Extend on port 20000. The default extend proxy port in the server container is 20000. <markup lang=\"bash\" >curl -i -w '' -X POST http://127.0.0.1:8080/tenants \\ -d '{\"tenant\":\"marvel\",\"type\":\"extend\",\"hostName\":\"marvel\",\"port\":20000,\"serializer\":\"java\"}' This should return a 200 response as show below: <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:15:26 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.TenantMetaData\", \"hostName\":\"marvel\", \"port\":20000, \"serializer\":\"java\", \"tenant\":\"marvel\", \"type\":\"extend\" } The curl command below will add the meta-data for the Star Wars tenant. This will connect to the Star Wars cluster using Coherence gRPC API on port 1408. The default gRPC port in the server container is 1408. <markup lang=\"bash\" >curl -i -w '' -X POST http://127.0.0.1:8080/tenants \\ -d '{\"tenant\":\"star-wars\",\"type\":\"grpc\",\"hostName\":\"star-wars\",\"port\":1408,\"serializer\":\"java\"}' This should return a 200 response as show below: <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:17:49 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.TenantMetaData\", \"hostName\":\"star-wars\", \"port\":1408, \"serializer\":\"java\", \"tenant\":\"star-wars\", \"type\":\"grpc\" } ",
            "title": "Create the Tenant Meta-Data"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The demo is complete so everything can be cleaned up. <markup lang=\"bash\" >docker rm -f webserver tenants marvel star-wars docker network rm coherence-net ",
            "title": "Clean-Up"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " First, try a simple GET request without a tenant header value. <markup lang=\"bash\" >curl -i -w '' -X GET http://127.0.0.1:8080/users/foo This should return a 400 bad request response as shown below <markup lang=\"bash\" >HTTP/1.1 400 Bad Request Date: Thu, 07 Jul 2022 15:33:23 GMT Transfer-encoding: chunked {\"Error\":\"Missing tenant identifier\"} Now try the same get, with a valid tenant identifier in the header. <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X GET http://127.0.0.1:8080/users/foo This should return a 404, as no users have been created yet. <markup lang=\"bash\" >HTTP/1.1 404 Not Found Date: Thu, 07 Jul 2022 15:35:26 GMT Transfer-encoding: chunked {\"Error\":\"Unknown user foo\"} Create a User in the Marvel cluster with the command below, using the marvel tenant identifier in the header: <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X POST http://127.0.0.1:8080/users \\ -d '{\"firstName\":\"Iron\",\"lastName\":\"Man\",\"email\":\"iron.man@marvel.com\"}' The response should be a 200 response, with the json of the user created. This will include the ID of the new user, in this case the ID is Iron.Man . <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:37:04 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.User\", \"email\":\"iron.man@marvel.com\", \"firstName\":\"Iron\", \"id\":\"Iron.Man\", \"lastName\":\"Man\" } Now get the Iron.Man user from the Marvel cluster: <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X GET http://127.0.0.1:8080/users/Iron.Man This should respond with a 200 response code and the same json as above. Next, try to get the Iron.Man user from the Star Wars cluster by using the star-wars tenant ID in the header <markup lang=\"bash\" >curl -i -w '' -H 'tenant: star-wars' -X GET http://127.0.0.1:8080/users/Iron.Man The response should be a 404, not-found, as the Iron.Man user is not in the Star Wars tenant&#8217;s cluster. Clean-Up The demo is complete so everything can be cleaned up. <markup lang=\"bash\" >docker rm -f webserver tenants marvel star-wars docker network rm coherence-net ",
            "title": "Access the Multi-Tenant Endpoints"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The point of this example is to show a client connecting to multiple clusters, so running the examples requires also running a number of Coherence clusters. To make running simple, each clusters will just be a single member. Create a Docker Network So that the client can communicate with the cluster members, a Docker network is required. The command below will create a Docker network named coherence-net <markup lang=\"bash\" >docker network create --driver bridge coherence-net Start the Coherence Clusters The example requires three clusters. The first is the tenant \"admin\" cluster that holds information about the tenants. Then there are two additional clusters for each tenant, in this case \"Marvel\" and \"Star Wars\" Start the admin cluster, this will hold tenant meta-data. <markup lang=\"bash\" >docker run -d --name tenants --network coherence-net \\ -e COHERENCE_CLUSTER=tenants \\ ghcr.io/coherence-community/multi-cluster-server:latest Start the cluster for the Marvel tenant. <markup lang=\"bash\" >docker run -d --name marvel --network coherence-net \\ -e COHERENCE_CLUSTER=marvel \\ ghcr.io/coherence-community/multi-cluster-server:latest Start the cluster for the Star Wars tenant. <markup lang=\"bash\" >docker run -d --name star-wars --network coherence-net \\ -e COHERENCE_CLUSTER=star-wars \\ ghcr.io/coherence-community/multi-cluster-server:latest After starting all three clusters, the docker ps command can be used to check their status. Eventually the STATUS colum of each container should say (healthy) . <markup >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4abdc735b7bd ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 2 minutes ago Up 2 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp star-wars 5df54737eb6a ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 2 minutes ago Up 2 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp marvel 87f9ee53dfc5 ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 3 minutes ago Up 3 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp tenants Start the Web-Server When all the clusters are running and healthy, the multi-tenant client can be started using the command below. This will start the webserver and expose the endpoints on http://127.0.0.1:8080 . <markup lang=\"bash\" >docker run -d --name webserver --network coherence-net \\ -e COHERENCE_EXTEND_ADDRESS=tenants \\ -e COHERENCE_EXTEND_PORT=20000 \\ -p 8080:8080 \\ ghcr.io/coherence-community/multi-cluster-client:latest Using docker ps the status of the webserver container should eventually be (healthy) too. Create the Tenant Meta-Data Once the webserver container is healthy the /tenants endpoint can be used to create the metadata for the two tenants. The curl command below will add the meta-data for the Marvel tenant. This will connect to the Marvel cluster using Coherence Extend on port 20000. The default extend proxy port in the server container is 20000. <markup lang=\"bash\" >curl -i -w '' -X POST http://127.0.0.1:8080/tenants \\ -d '{\"tenant\":\"marvel\",\"type\":\"extend\",\"hostName\":\"marvel\",\"port\":20000,\"serializer\":\"java\"}' This should return a 200 response as show below: <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:15:26 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.TenantMetaData\", \"hostName\":\"marvel\", \"port\":20000, \"serializer\":\"java\", \"tenant\":\"marvel\", \"type\":\"extend\" } The curl command below will add the meta-data for the Star Wars tenant. This will connect to the Star Wars cluster using Coherence gRPC API on port 1408. The default gRPC port in the server container is 1408. <markup lang=\"bash\" >curl -i -w '' -X POST http://127.0.0.1:8080/tenants \\ -d '{\"tenant\":\"star-wars\",\"type\":\"grpc\",\"hostName\":\"star-wars\",\"port\":1408,\"serializer\":\"java\"}' This should return a 200 response as show below: <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:17:49 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.TenantMetaData\", \"hostName\":\"star-wars\", \"port\":1408, \"serializer\":\"java\", \"tenant\":\"star-wars\", \"type\":\"grpc\" } Access the Multi-Tenant Endpoints First, try a simple GET request without a tenant header value. <markup lang=\"bash\" >curl -i -w '' -X GET http://127.0.0.1:8080/users/foo This should return a 400 bad request response as shown below <markup lang=\"bash\" >HTTP/1.1 400 Bad Request Date: Thu, 07 Jul 2022 15:33:23 GMT Transfer-encoding: chunked {\"Error\":\"Missing tenant identifier\"} Now try the same get, with a valid tenant identifier in the header. <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X GET http://127.0.0.1:8080/users/foo This should return a 404, as no users have been created yet. <markup lang=\"bash\" >HTTP/1.1 404 Not Found Date: Thu, 07 Jul 2022 15:35:26 GMT Transfer-encoding: chunked {\"Error\":\"Unknown user foo\"} Create a User in the Marvel cluster with the command below, using the marvel tenant identifier in the header: <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X POST http://127.0.0.1:8080/users \\ -d '{\"firstName\":\"Iron\",\"lastName\":\"Man\",\"email\":\"iron.man@marvel.com\"}' The response should be a 200 response, with the json of the user created. This will include the ID of the new user, in this case the ID is Iron.Man . <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:37:04 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.User\", \"email\":\"iron.man@marvel.com\", \"firstName\":\"Iron\", \"id\":\"Iron.Man\", \"lastName\":\"Man\" } Now get the Iron.Man user from the Marvel cluster: <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X GET http://127.0.0.1:8080/users/Iron.Man This should respond with a 200 response code and the same json as above. Next, try to get the Iron.Man user from the Star Wars cluster by using the star-wars tenant ID in the header <markup lang=\"bash\" >curl -i -w '' -H 'tenant: star-wars' -X GET http://127.0.0.1:8080/users/Iron.Man The response should be a 404, not-found, as the Iron.Man user is not in the Star Wars tenant&#8217;s cluster. Clean-Up The demo is complete so everything can be cleaned up. <markup lang=\"bash\" >docker rm -f webserver tenants marvel star-wars docker network rm coherence-net ",
            "title": "Running the Example"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The data model used in this example is a simple User entity, with an id, a first name, a last name and an email address. A snippet of the source is shown below, the actual code includes serialization support for both Java and portable object format serialization. <markup lang=\"java\" title=\"User.java\" >/** * A simple user entity. */ public class User implements PortableObject, ExternalizableLite { /** * The user's identifier. */ private String id; /** * The user's first name. */ private String firstName; /** * The user's last name. */ private String lastName; /** * The user's email address. */ private String email; /** * A default constructor, required for Coherence serialization. */ public User() { } /** * Create a user. * * @param id the user's identifier * @param firstName the user's first name * @param lastName the user's last name * @param email the user's email address */ public User(String id, String firstName, String lastName, String email) { this.id = id; this.firstName = firstName; this.lastName = lastName; this.email = email; } /** * Returns the user's identifier. * * @return the user's identifier */ public String getId() { return id; } /** * Set the user's identifier. * * @param id the user's identifier */ public void setId(String id) { this.id = id; } /** * Returns the user's first name. * * @return the user's first name */ public String getFirstName() { return firstName; } /** * Set the user's first name. * * @param firstName the user's first name */ public void setFirstName(String firstName) { this.firstName = firstName; } /** * Returns the user's last name. * * @return the user's last name */ public String getLastName() { return lastName; } /** * Set the user's last name. * * @param lastName the user's last name */ public void setLastName(String lastName) { this.lastName = lastName; } /** * Returns the user's email address. * * @return the user's email address */ public String getEmail() { return email; } /** * Set the user's email address. * * @param email the user's email address */ public void setEmail(String email) { this.email = email; } } ",
            "title": "The Data Model"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The application does not have a main class with a main method. The Application class in the example code implements Coherence.LifecycleListener and will be discovered by Coherence using the Java ServiceLoader . The Application class then receives events when the Coherence bootstrap API starts and stops Coherence. Using these events, the Application class configures, starts and stops the web-server. The actual code is not discussed in detail here as it is not particularly relevant for the example. The application is started by running the com.tangosol.net.Coherence class; in this case Coherence is started as a client. ",
            "title": "The Main Class"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The TenantController class is one of two classes that expose REST endpoints in the web-server. The purpose of the TenantController is to perform CRUD operations on tenants, to allow runtime configuration of tenants, and which cluster a given tenant should connect to. Tenant meta-data is contained in a TenantMetaData class. This holds the tenant name, the host name and port of the Coherence cluster holding the tenants data and whether the client session should use Coherence Extend or Coherence gRPC to connect to the cluster. ",
            "title": "The TenantController"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The UserController class is the main class in the example. This class exposes some REST endpoints to perform CRUD operations on User entities. There are four methods supported by the controller, POST to create a user, 'PUT' to update a user, GET to get a user and DELETE to delete a user. Every request much contain a tenant header with the name of the tenant as the header value. Any request without a tenant header is rejected. ",
            "title": "The UserController"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The application is a very basic CRUD application to manage simple User entities in a Coherence cache. The application exposes a REST API with get, create (POST), update (PUT) and delete methods. The application is multi-tenanted, so each request has the relevant tenant identifier in a request header. Requests without a tenant identifier, or with an unknown tenant, are rejected. The web-server is a Coherence client application, and is hence storage disabled. The data for each tenant is held in a separate storage enabled Coherence cluster for each tenant. There is an admin cluster that holds the meta-data about tenants. This allows tenants and their cluster details to be added and maintained at runtime. The main reason for doing this in the example, is because it makes testing much simpler. A stateless web server that accesses data to service requests from different Coherence clusters has various pros and cons with its design, but the purpose of this example is to show connecting Coherence clients to different clusters, its purpose is not to produce the best, most efficient, web application. There is certainly no security built in to this example. The actual web-server implementation used in this example is unimportant and not really relevant to the example code. The code shown here could easily be ported to other web-application frameworks, such as Coherence CDI with Helidon , Coherence and Spring , or Coherence and Micronaut , etc. The Data Model The data model used in this example is a simple User entity, with an id, a first name, a last name and an email address. A snippet of the source is shown below, the actual code includes serialization support for both Java and portable object format serialization. <markup lang=\"java\" title=\"User.java\" >/** * A simple user entity. */ public class User implements PortableObject, ExternalizableLite { /** * The user's identifier. */ private String id; /** * The user's first name. */ private String firstName; /** * The user's last name. */ private String lastName; /** * The user's email address. */ private String email; /** * A default constructor, required for Coherence serialization. */ public User() { } /** * Create a user. * * @param id the user's identifier * @param firstName the user's first name * @param lastName the user's last name * @param email the user's email address */ public User(String id, String firstName, String lastName, String email) { this.id = id; this.firstName = firstName; this.lastName = lastName; this.email = email; } /** * Returns the user's identifier. * * @return the user's identifier */ public String getId() { return id; } /** * Set the user's identifier. * * @param id the user's identifier */ public void setId(String id) { this.id = id; } /** * Returns the user's first name. * * @return the user's first name */ public String getFirstName() { return firstName; } /** * Set the user's first name. * * @param firstName the user's first name */ public void setFirstName(String firstName) { this.firstName = firstName; } /** * Returns the user's last name. * * @return the user's last name */ public String getLastName() { return lastName; } /** * Set the user's last name. * * @param lastName the user's last name */ public void setLastName(String lastName) { this.lastName = lastName; } /** * Returns the user's email address. * * @return the user's email address */ public String getEmail() { return email; } /** * Set the user's email address. * * @param email the user's email address */ public void setEmail(String email) { this.email = email; } } The Main Class The application does not have a main class with a main method. The Application class in the example code implements Coherence.LifecycleListener and will be discovered by Coherence using the Java ServiceLoader . The Application class then receives events when the Coherence bootstrap API starts and stops Coherence. Using these events, the Application class configures, starts and stops the web-server. The actual code is not discussed in detail here as it is not particularly relevant for the example. The application is started by running the com.tangosol.net.Coherence class; in this case Coherence is started as a client. The TenantController The TenantController class is one of two classes that expose REST endpoints in the web-server. The purpose of the TenantController is to perform CRUD operations on tenants, to allow runtime configuration of tenants, and which cluster a given tenant should connect to. Tenant meta-data is contained in a TenantMetaData class. This holds the tenant name, the host name and port of the Coherence cluster holding the tenants data and whether the client session should use Coherence Extend or Coherence gRPC to connect to the cluster. The UserController The UserController class is the main class in the example. This class exposes some REST endpoints to perform CRUD operations on User entities. There are four methods supported by the controller, POST to create a user, 'PUT' to update a user, GET to get a user and DELETE to delete a user. Every request much contain a tenant header with the name of the tenant as the header value. Any request without a tenant header is rejected. ",
            "title": "The Example Application"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The work of obtaining or creating a Session for a tenant is in the UserController.ensureSession method. <markup lang=\"java\" title=\"UserController.java\" > private Session ensureSession(String tenant) { TenantMetaData metaData = tenants.get(tenant); if (metaData == null) { return null; } Coherence coherence = Coherence.getInstance(); return coherence.getSessionIfPresent(tenant) .orElseGet(()-&gt;createSession(coherence, metaData)); } The meta-data for the tenant is obtained from the tenants cache. If there is no meta-data in the cache, the method returns null . The default Coherence instance is obtained, as this will be the owner of all the client Session instances. The Coherence.getSessionIfPresent() method is called, which will return an existing Session for a given tenant name if one exists. The Coherence.getSessionIfPresent() returns an Optional&lt;Session&gt; and if this is empty, the supplier in the orElseGet() method is called, which calles the UserController.createSession() method to actually create a Session . ",
            "title": "The ensureSession Method"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " If a Session does not yet exist for a tenant, one must be created from the TenantMetaData for the tenant. The UserController.createSession() method is responsible for creating a Session for a tenant. <markup lang=\"java\" title=\"UserController.java\" > private Session createSession(Coherence coherence, TenantMetaData metaData) { String tenant = metaData.getTenant(); if (metaData.isExtend()) { coherence.addSessionIfAbsent(tenant, ()-&gt;createExtendConfiguration(metaData)); } else { coherence.addSessionIfAbsent(tenant, ()-&gt;createGrpcConfiguration(metaData)); } return coherence.getSession(tenant); } The createSession method is very simple, it just delegates to another method, depending on whether the required Session is for an Extend client or a gRPC client. A SessionConfiguration is created, either for an Extend client, or gRPC client, and is passed to the Coherence.addSessionIfAbsent() method. The add if absent method is used in case multiple threads attempt to create the same tenant&#8217;s session, it will only be added once. ",
            "title": "The createSession Method"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " A Session is simple to add to a running Coherence instance. It just requires creating a SessionConfiguration instance and adding it to the Coherence instance. An Extend client configuration can be created using the SessionConfiguration builder. <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createExtendConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.extend.address\", metaData.getHostName()) .withParameter(\"coherence.extend.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to remote-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address Extend client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.extend.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.extend.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. ",
            "title": "Creating an Extend Session"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " Creating a gRPC Session is as simple as creating an Extend Session . <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createGrpcConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.GrpcFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.grpc.address\", metaData.getHostName()) .withParameter(\"coherence.grpc.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to grpc-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address gRPC client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.grpc.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.grpc.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. ",
            "title": "Creating a gRPC Session"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The example code could be simplified if the application only ever used Extend or only ever used gRPC. There are also many alternative approaches to holding tenant metata data used to create the sessions. The important parts of the example are the methods in UserController to obtain a session from the Coherence instance, and create a new Session is one does not already exist. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " When a request comes to the UserController a Coherence Session must be obtained for the tenant. All the requests perform the same logic, the code below is from the get request handler. <markup lang=\"java\" title=\"UserController.java\" > public Response get(HttpRequest request) { String tenant = request.getHeaderString(TENANT_HEADER); if (tenant == null || tenant.isBlank()) { return Response.status(400).entity(Map.of(\"Error\", \"Missing tenant identifier\")).build(); } Session session = ensureSession(tenant); if (session == null) { return Response.status(400).entity(Map.of(\"Error\", \"Unknown tenant \" + tenant)).build(); } The \"tenant\" header value is obtained from the request If there is no tenant header a 400 response is returned A Session is obtained for the tenant (this is covered in detail below) If the Session is null , a 400 response is returned Once a valid Session has been obtained for the tenant, the rest of the request processing can continue. The ensureSession Method The work of obtaining or creating a Session for a tenant is in the UserController.ensureSession method. <markup lang=\"java\" title=\"UserController.java\" > private Session ensureSession(String tenant) { TenantMetaData metaData = tenants.get(tenant); if (metaData == null) { return null; } Coherence coherence = Coherence.getInstance(); return coherence.getSessionIfPresent(tenant) .orElseGet(()-&gt;createSession(coherence, metaData)); } The meta-data for the tenant is obtained from the tenants cache. If there is no meta-data in the cache, the method returns null . The default Coherence instance is obtained, as this will be the owner of all the client Session instances. The Coherence.getSessionIfPresent() method is called, which will return an existing Session for a given tenant name if one exists. The Coherence.getSessionIfPresent() returns an Optional&lt;Session&gt; and if this is empty, the supplier in the orElseGet() method is called, which calles the UserController.createSession() method to actually create a Session . The createSession Method If a Session does not yet exist for a tenant, one must be created from the TenantMetaData for the tenant. The UserController.createSession() method is responsible for creating a Session for a tenant. <markup lang=\"java\" title=\"UserController.java\" > private Session createSession(Coherence coherence, TenantMetaData metaData) { String tenant = metaData.getTenant(); if (metaData.isExtend()) { coherence.addSessionIfAbsent(tenant, ()-&gt;createExtendConfiguration(metaData)); } else { coherence.addSessionIfAbsent(tenant, ()-&gt;createGrpcConfiguration(metaData)); } return coherence.getSession(tenant); } The createSession method is very simple, it just delegates to another method, depending on whether the required Session is for an Extend client or a gRPC client. A SessionConfiguration is created, either for an Extend client, or gRPC client, and is passed to the Coherence.addSessionIfAbsent() method. The add if absent method is used in case multiple threads attempt to create the same tenant&#8217;s session, it will only be added once. Creating an Extend Session A Session is simple to add to a running Coherence instance. It just requires creating a SessionConfiguration instance and adding it to the Coherence instance. An Extend client configuration can be created using the SessionConfiguration builder. <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createExtendConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.extend.address\", metaData.getHostName()) .withParameter(\"coherence.extend.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to remote-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address Extend client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.extend.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.extend.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. Creating a gRPC Session Creating a gRPC Session is as simple as creating an Extend Session . <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createGrpcConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.GrpcFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.grpc.address\", metaData.getHostName()) .withParameter(\"coherence.grpc.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to grpc-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address gRPC client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.grpc.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.grpc.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. Summary The example code could be simplified if the application only ever used Extend or only ever used gRPC. There are also many alternative approaches to holding tenant metata data used to create the sessions. The important parts of the example are the methods in UserController to obtain a session from the Coherence instance, and create a new Session is one does not already exist. ",
            "title": "Creating a Tenant&#8217;s Session"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " In this example, the tenants are dynamic and Coherence client sessions are created on-demand using meta-data held in a tenants cache. An alternative would have been to create all the tenant client sessions when the application started up using the Coherence Bootstrap API to configure them. This would have been a much simpler example, but then testing and demonstrating it would have been harder. A more dynamic multi-tenant system is probably closer to a real-world scenario. When the web-server starts it connects using Coherence Extend to the tenant meta-data cluster and obtains a reference to a Coherence NamedMap to hold tenant meta-data. Creating a Tenant&#8217;s Session When a request comes to the UserController a Coherence Session must be obtained for the tenant. All the requests perform the same logic, the code below is from the get request handler. <markup lang=\"java\" title=\"UserController.java\" > public Response get(HttpRequest request) { String tenant = request.getHeaderString(TENANT_HEADER); if (tenant == null || tenant.isBlank()) { return Response.status(400).entity(Map.of(\"Error\", \"Missing tenant identifier\")).build(); } Session session = ensureSession(tenant); if (session == null) { return Response.status(400).entity(Map.of(\"Error\", \"Unknown tenant \" + tenant)).build(); } The \"tenant\" header value is obtained from the request If there is no tenant header a 400 response is returned A Session is obtained for the tenant (this is covered in detail below) If the Session is null , a 400 response is returned Once a valid Session has been obtained for the tenant, the rest of the request processing can continue. The ensureSession Method The work of obtaining or creating a Session for a tenant is in the UserController.ensureSession method. <markup lang=\"java\" title=\"UserController.java\" > private Session ensureSession(String tenant) { TenantMetaData metaData = tenants.get(tenant); if (metaData == null) { return null; } Coherence coherence = Coherence.getInstance(); return coherence.getSessionIfPresent(tenant) .orElseGet(()-&gt;createSession(coherence, metaData)); } The meta-data for the tenant is obtained from the tenants cache. If there is no meta-data in the cache, the method returns null . The default Coherence instance is obtained, as this will be the owner of all the client Session instances. The Coherence.getSessionIfPresent() method is called, which will return an existing Session for a given tenant name if one exists. The Coherence.getSessionIfPresent() returns an Optional&lt;Session&gt; and if this is empty, the supplier in the orElseGet() method is called, which calles the UserController.createSession() method to actually create a Session . The createSession Method If a Session does not yet exist for a tenant, one must be created from the TenantMetaData for the tenant. The UserController.createSession() method is responsible for creating a Session for a tenant. <markup lang=\"java\" title=\"UserController.java\" > private Session createSession(Coherence coherence, TenantMetaData metaData) { String tenant = metaData.getTenant(); if (metaData.isExtend()) { coherence.addSessionIfAbsent(tenant, ()-&gt;createExtendConfiguration(metaData)); } else { coherence.addSessionIfAbsent(tenant, ()-&gt;createGrpcConfiguration(metaData)); } return coherence.getSession(tenant); } The createSession method is very simple, it just delegates to another method, depending on whether the required Session is for an Extend client or a gRPC client. A SessionConfiguration is created, either for an Extend client, or gRPC client, and is passed to the Coherence.addSessionIfAbsent() method. The add if absent method is used in case multiple threads attempt to create the same tenant&#8217;s session, it will only be added once. Creating an Extend Session A Session is simple to add to a running Coherence instance. It just requires creating a SessionConfiguration instance and adding it to the Coherence instance. An Extend client configuration can be created using the SessionConfiguration builder. <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createExtendConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.extend.address\", metaData.getHostName()) .withParameter(\"coherence.extend.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to remote-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address Extend client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.extend.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.extend.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. Creating a gRPC Session Creating a gRPC Session is as simple as creating an Extend Session . <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createGrpcConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.GrpcFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.grpc.address\", metaData.getHostName()) .withParameter(\"coherence.grpc.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to grpc-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address gRPC client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.grpc.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.grpc.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. Summary The example code could be simplified if the application only ever used Extend or only ever used gRPC. There are also many alternative approaches to holding tenant metata data used to create the sessions. The important parts of the example are the methods in UserController to obtain a session from the Coherence instance, and create a new Session is one does not already exist. ",
            "title": "Implementing Multi-Tenancy"
        },
        {
            "location": "/docs/core/09_queues",
            "text": " This implementation of a queue does not scale the same way as a Coherence cache or a Coherence topic does. In the current queue implementation, all the data for the queue is stored in a single Coherence partition. This means that the total size of the elements in a single queue instance cannot be larger than 2GB. This is a limitation of Coherence where a partition for a cache cannot exceed 2GB, as that is the limit of the buffer size used to transfer data between cluster members. If Coherence queues allowed a partition to grow over 2GB, the queue would work fine up to the point where Coherence needed to transfer that partition to a new member on fail-over, at that point data would be lost. The Coherence server will refuse to accept offers to the queue if its size will exceed 2GB. The java.util.Queue contact allows for queues to reject offers, so this size limitation conforms to the queue contract. Application developers should check the response from offering data to the queue to determine whether the offer has succeeded or not. We use the term \"offer\" here to cover all queue and deque methods that add data to the queue. An alternative to checking the return boolean from an offer() call would be to use a NamedBlockingQueue where the put() method will block if the queue is full, To obtain an instance of the simple size limited queue from the Queues factory class there are two methods, dependeing on whether a queue or deque is required. For example to obtain a BlockingQueue named \"my-queue\": <markup lang=\"java\" >NamedBlockingQueue&lt;String&gt; queue = Queues.queue(\"my-queue\"); For example to obtain a BlockingDeque named \"my-deque\": <markup lang=\"java\" >NamedBlockingDeque&lt;String&gt; queue = Queues.deque(\"my-deque\"); ",
            "title": "Simple Size Limited Queue and Deque"
        },
        {
            "location": "/docs/core/09_queues",
            "text": " To obtain an instance of a distributed paged queue, from the Queues factory class use the pagedQueue method. <markup lang=\"java\" >NamedBlockingQueue&lt;String&gt; queue = Queues.pagedQueue(\"my-queue\"); ",
            "title": "Distributed Paged Queue"
        },
        {
            "location": "/docs/core/09_queues",
            "text": " The Coherence Concurrent module contains an implementation of java.util.concurrent.BlockingQueue called NamedBlockingQueue and an implementation of java.util.concurrent.BlockingDeque called NamedBlockingDeque . To use the Coherence blocking queue or deque in your application you need to add a dependency on the coherence-concurrent module. <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-concurrent&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; The blocking queue implementations work by using Coherence events. When application code calls a blocking method, the calling thread will be blocked, the blocking is not on the server. The application code will become unblocked when it receives an event from the server. For example, if application code calls the NamedBlockingQueue take() method, and the queue is empty, this method will block the calling thread. When an element is put into the queue by another thread (maybe on another JVM) the calling application will receive an event. This will retry the take() and if successful will return. If the retry of the take() is unsuccessful the calling thread remains blocked (for example another thread or another JVM was also blocked taking from the same queue and managed to get its retry in first. Another example would be an application calling NamedBlockingQueue put() method, which will block when the queue is full (i.e. reaches the 2GB size limit). In this case the calling thread will be blocked until a delete event is received to signal that there is now space in the queue. The put() will be retried and if successful control returned to the calling thread. If the retry is unsuccessful the thread will remain blocked (for example another thread or JVM was also blocked on a put() and its retry succeeded and refilled the queue). To obtain an instance of a blocking queue use the com.oracle.coherence.concurrent.Queues factory class. Simple Size Limited Queue and Deque This implementation of a queue does not scale the same way as a Coherence cache or a Coherence topic does. In the current queue implementation, all the data for the queue is stored in a single Coherence partition. This means that the total size of the elements in a single queue instance cannot be larger than 2GB. This is a limitation of Coherence where a partition for a cache cannot exceed 2GB, as that is the limit of the buffer size used to transfer data between cluster members. If Coherence queues allowed a partition to grow over 2GB, the queue would work fine up to the point where Coherence needed to transfer that partition to a new member on fail-over, at that point data would be lost. The Coherence server will refuse to accept offers to the queue if its size will exceed 2GB. The java.util.Queue contact allows for queues to reject offers, so this size limitation conforms to the queue contract. Application developers should check the response from offering data to the queue to determine whether the offer has succeeded or not. We use the term \"offer\" here to cover all queue and deque methods that add data to the queue. An alternative to checking the return boolean from an offer() call would be to use a NamedBlockingQueue where the put() method will block if the queue is full, To obtain an instance of the simple size limited queue from the Queues factory class there are two methods, dependeing on whether a queue or deque is required. For example to obtain a BlockingQueue named \"my-queue\": <markup lang=\"java\" >NamedBlockingQueue&lt;String&gt; queue = Queues.queue(\"my-queue\"); For example to obtain a BlockingDeque named \"my-deque\": <markup lang=\"java\" >NamedBlockingDeque&lt;String&gt; queue = Queues.deque(\"my-deque\"); Distributed Paged Queue To obtain an instance of a distributed paged queue, from the Queues factory class use the pagedQueue method. <markup lang=\"java\" >NamedBlockingQueue&lt;String&gt; queue = Queues.pagedQueue(\"my-queue\"); ",
            "title": "Blocking Queue"
        },
        {
            "location": "/docs/core/09_queues",
            "text": " Starting with Coherence CE 24.03, Coherence supports Queues as data structure. The Coherence NamedQueue is an implementation of java.util.Queue and NamedDeque is and implementation of java.util.Deque . Coherence has two implementations of BlockingQueue , one is a simple size limited queue, the second is a distributed paged queue that has a much larger capacity. The simple queue is available as both a BlockingQueue and a double ended BlockingDeque . The distributed paged queue is only available as a BlockingQueue implementation. Important Coherence queues are mapped to caches with the same name as the queue. If a cache is being used for a queue the same cache must not also be used as a normal data cache. Blocking Queue The Coherence Concurrent module contains an implementation of java.util.concurrent.BlockingQueue called NamedBlockingQueue and an implementation of java.util.concurrent.BlockingDeque called NamedBlockingDeque . To use the Coherence blocking queue or deque in your application you need to add a dependency on the coherence-concurrent module. <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-concurrent&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; The blocking queue implementations work by using Coherence events. When application code calls a blocking method, the calling thread will be blocked, the blocking is not on the server. The application code will become unblocked when it receives an event from the server. For example, if application code calls the NamedBlockingQueue take() method, and the queue is empty, this method will block the calling thread. When an element is put into the queue by another thread (maybe on another JVM) the calling application will receive an event. This will retry the take() and if successful will return. If the retry of the take() is unsuccessful the calling thread remains blocked (for example another thread or another JVM was also blocked taking from the same queue and managed to get its retry in first. Another example would be an application calling NamedBlockingQueue put() method, which will block when the queue is full (i.e. reaches the 2GB size limit). In this case the calling thread will be blocked until a delete event is received to signal that there is now space in the queue. The put() will be retried and if successful control returned to the calling thread. If the retry is unsuccessful the thread will remain blocked (for example another thread or JVM was also blocked on a put() and its retry succeeded and refilled the queue). To obtain an instance of a blocking queue use the com.oracle.coherence.concurrent.Queues factory class. Simple Size Limited Queue and Deque This implementation of a queue does not scale the same way as a Coherence cache or a Coherence topic does. In the current queue implementation, all the data for the queue is stored in a single Coherence partition. This means that the total size of the elements in a single queue instance cannot be larger than 2GB. This is a limitation of Coherence where a partition for a cache cannot exceed 2GB, as that is the limit of the buffer size used to transfer data between cluster members. If Coherence queues allowed a partition to grow over 2GB, the queue would work fine up to the point where Coherence needed to transfer that partition to a new member on fail-over, at that point data would be lost. The Coherence server will refuse to accept offers to the queue if its size will exceed 2GB. The java.util.Queue contact allows for queues to reject offers, so this size limitation conforms to the queue contract. Application developers should check the response from offering data to the queue to determine whether the offer has succeeded or not. We use the term \"offer\" here to cover all queue and deque methods that add data to the queue. An alternative to checking the return boolean from an offer() call would be to use a NamedBlockingQueue where the put() method will block if the queue is full, To obtain an instance of the simple size limited queue from the Queues factory class there are two methods, dependeing on whether a queue or deque is required. For example to obtain a BlockingQueue named \"my-queue\": <markup lang=\"java\" >NamedBlockingQueue&lt;String&gt; queue = Queues.queue(\"my-queue\"); For example to obtain a BlockingDeque named \"my-deque\": <markup lang=\"java\" >NamedBlockingDeque&lt;String&gt; queue = Queues.deque(\"my-deque\"); Distributed Paged Queue To obtain an instance of a distributed paged queue, from the Queues factory class use the pagedQueue method. <markup lang=\"java\" >NamedBlockingQueue&lt;String&gt; queue = Queues.pagedQueue(\"my-queue\"); ",
            "title": "Queues"
        },
        {
            "location": "/docs/core/09_queues",
            "text": " It is important to understand how the two Coherence queue implementations store data and how this limits the size of a queue. Simple Coherence Queue – the simple queue (and deque) implementation stores data in a single Coherence cache partition. This enforces a size limit of 2GB because a Coherence cache partition should not exceed 2GB in size, and in reality, a partition should be a lot smaller than this. Large partitions slow down recovery when a storage enabled member leaves the cluster. With a modern fast network 300Mb – 500MB should be a suitable maximum partition size, on a 10Gb network this could even go as high as 1GB. The distributed paged queue stores data in pages that are distributed around the Coherence cluster over multiple partitions, the same as normal cache data. This means that the paged queue can store far more than 2GB. It is still important to be aware of how partition sizes limit the total queue size. The absolute hard limit of 2GB per partition give the following size: 2GB x 257 = 514GB But this is far too big to be reliable in production use. If we take a size limit of 500MB and the default partition count of 257 we can see how this affects queue size. 500MB x 257 = 128GB So, by default a realistic limit for a paged queue is around 128GB. If the partition count is increased to 1087 the queue size becomes: 500MB x 1087 = 543GB Of course, all these examples assume that there are enough JVMs with big enough heap sizes in the cluster to store the queue data in memory. ",
            "title": "Sizing Queues"
        },
        {
            "location": "/docs/core/09_queues",
            "text": " The current queue implementation in Coherence has some limitations detailed below. In normal operation queues should not get huge, this would usually mean that the processes reading from the queue are not keeping up with the processes writing to the queue. Application developers should obviously load test their applications using queues to ensure that they are not going to have issues with capacity. Queue operations such as offering and polling will contend on certain data structures and this will limit the number of parallel requests and how fast requests can be processed. In order to maintain ordering, polling contends on either the head or tail entry, depending on which end of the queue is being polled. This means that poll methods can only be processed sequentially, so even though a poll is efficient and fast, a large number of concurrent poll requests will queue and be processed one at a time. Offer methods do not contend on the head or tail, but will contend on the atomic counters used to maintain the head and tail identifiers. So, whilst Coherence can process multiple offer requests on different worker threads, there will be minor contention on the AtomicLong updates. Queue operations that work on the head and tail such as offering and polling are efficient. Some of the other methods in java.util.Queue and java.util.Deque are less efficient. For example, iterator methods, contains() and so on. These are not frequently used by applications that require basic queue functionality. Some optional methods on the java.util.Queue API that mutate the queue will throw UnsupportedOperationException (this is allowed by the Java Queue contract), for example retainAll() , removeAll() , and removal using an iterator. ",
            "title": "Limitations"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Each of the features above is backed by one or more Coherence caches, possibly with preconfigured interceptors, but for the most part you shouldn&#8217;t care about that: all interaction with lower level Coherence primitives is hidden behind various factory classes that allow you to get the instances of the classes you need. For example, you will use factory methods within Atomics class to get instances of various atomic types, Locks to get lock instances, Latches and Semaphores to get, well, latches and semaphores. ",
            "title": "Factory Classes"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " In many cases the factory classes will allow you to get both the local and the remote instances of various constructs. For example, Locks.localLock will give you an instance of a standard java.util.concurrent.locks.ReentrantLock , while Locks.remoteLock will return an instance of a RemoteLock . In cases where JDK doesn&#8217;t provide a standard interface, which is the case with atomics, latches and semaphores, we&#8217;ve extracted the interface from the existing JDK class, and created a thin wrapper around the corresponding JDK implementation. For example, Coherence Concurrent provides a Semaphore interface, and LocalSemaphore class that wraps java.util.concurrent.Semaphore . The same is true for the CountDownLatch , and all atomic types. The main advantage of using factory classes to construct both the local and remote instances is that it allows you to name local locks the same way you name remote locks.: calling Locks.localLock(\"foo\") will always return the same Lock instance, as the Locks class internally caches both the local and the remote instances it created. Of course, in the case of remote locks, every locally cached remote lock instance is ultimately backed by a shared lock instance somewhere in the cluster, which is used to synchronize lock state across the processes. ",
            "title": "Local vs Remote"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent supports both Java serialization and POF out-of-the-box, with Java serialization being the default. If you want to use POF instead, you will need to specify that by setting coherence.concurrent.serializer system property to pof . You will also need to include coherence-concurrent-pof-config.xml into your own POF configuration file, in order to register built-in Coherence Concurrent types. ",
            "title": "Serialization"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent supports both active and on-demand persistence, but just like in the rest of Coherence it is set to on-demand by default. In order to use active persistence you should set coherence.concurrent.persistence.environment system property to default-active , or another persistence environment that has active persistence enabled. ",
            "title": "Persistence"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent module provides distributed implementations of the concurrency primitives from the java.util.concurrent package that you are already familiar with, such as executors, atomics, locks, semaphores and latches. This allows you to implement concurrent applications using the constructs you are already familiar with, but to expand the \"scope\" of concurrency from a single process to potentially hundreds of processes within a Coherence cluster. For example - you can use executors to submit tasks to be executed somewhere in the cluster; you can use locks, latches and semaphores to synchronize execution across many cluster members; you can use atomics to implement global counters across many processes, etc. Please keep in mind that while these features are extremely powerful and allow you to reuse the knowledge you already have, they may have detrimental effect on scalability and/or performance. Whenever you synchronize execution via locks, latches or semaphores, you are introducing a potential bottleneck into the architecture. Whenever you use a distributed atomic to implement a global counter, you are turning very simple operations that take mere nanoseconds locally, such as increment and decrement, into fairly expensive network calls that could take milliseconds (and potentially block even longer under heavy load). So, use these features sparingly. In many cases there is a better, faster and more scalable way to accomplish the same goal using Coherence primitives such as entry processors, aggregators and events, which were designed to perform and scale well in a distributed environment from the get-go. Factory Classes Each of the features above is backed by one or more Coherence caches, possibly with preconfigured interceptors, but for the most part you shouldn&#8217;t care about that: all interaction with lower level Coherence primitives is hidden behind various factory classes that allow you to get the instances of the classes you need. For example, you will use factory methods within Atomics class to get instances of various atomic types, Locks to get lock instances, Latches and Semaphores to get, well, latches and semaphores. Local vs Remote In many cases the factory classes will allow you to get both the local and the remote instances of various constructs. For example, Locks.localLock will give you an instance of a standard java.util.concurrent.locks.ReentrantLock , while Locks.remoteLock will return an instance of a RemoteLock . In cases where JDK doesn&#8217;t provide a standard interface, which is the case with atomics, latches and semaphores, we&#8217;ve extracted the interface from the existing JDK class, and created a thin wrapper around the corresponding JDK implementation. For example, Coherence Concurrent provides a Semaphore interface, and LocalSemaphore class that wraps java.util.concurrent.Semaphore . The same is true for the CountDownLatch , and all atomic types. The main advantage of using factory classes to construct both the local and remote instances is that it allows you to name local locks the same way you name remote locks.: calling Locks.localLock(\"foo\") will always return the same Lock instance, as the Locks class internally caches both the local and the remote instances it created. Of course, in the case of remote locks, every locally cached remote lock instance is ultimately backed by a shared lock instance somewhere in the cluster, which is used to synchronize lock state across the processes. Serialization Coherence Concurrent supports both Java serialization and POF out-of-the-box, with Java serialization being the default. If you want to use POF instead, you will need to specify that by setting coherence.concurrent.serializer system property to pof . You will also need to include coherence-concurrent-pof-config.xml into your own POF configuration file, in order to register built-in Coherence Concurrent types. Persistence Coherence Concurrent supports both active and on-demand persistence, but just like in the rest of Coherence it is set to on-demand by default. In order to use active persistence you should set coherence.concurrent.persistence.environment system property to default-active , or another persistence environment that has active persistence enabled. ",
            "title": "Distributed Concurrency"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": "",
            "title": "Executors"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent provides a facility to dispatch tasks, either a Runnable , Callable , or Task to a Coherence cluster for execution. Executors that will actually execute the submitted tasks are configured on each cluster member by defining one or more named executors within a cache configuration resource. ",
            "title": "Overview"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " By default, each Coherence cluster with the coherence-concurrent module on the classpath, will include a single-threaded executor that may be used to execute dispatched tasks. Given this, the simplest example would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.getDefault(); Future&lt;Void&gt; result = remoteExecutor.submit(() -&gt; System.out.println(\"Executed\")); result.get(); // block until completion If for example, an executor was configured named Fixed5 , the code would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.get(\"Fixed5\"); If no executor has been configured with the given name, the RemoteExecutor will throw RejectedExecutionException . Each RemoteExecutor instance may hold local resources that should be released when the RemoteExecutor is no longer needed. Like an ExecutorService , a RemoteExecutor has similar methods to shut the executor down. When calling these methods, it will have no impact on the executors registered within the cluster. ",
            "title": "Usage Examples"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " While the RemoteExecutor does provide functionality similar to the standard ExecutorService included in the JDK, this may not be enough in the context of Coherence. A task might need to run across multiple Coherence members, produce intermediate results, and remain durable in case a cluster member executing the task fails. In such cases, task orchestration can be used. Before diving into the details of orchestration, the following concepts should be understood: Interface Description Task Tasks are like Callable and Runnable classes in that they are designed to be potentially executed by one or more threads. Unlike Callable and Runnable classes, the execution may occur in different Java Virtual Machines, fail and/or recover between different Java Virtual Machine processes. Task.Context Provides contextual information for a Task as it is executed, including the ability to access and update intermediate results for the Executor executing the said Task . Task.Orchestration Defines information concerning the orchestration of a Task across a set of executors defined across multiple Coherence members for a given RemoteExecutor . Task.Coordinator A publisher of collected Task results that additionally permits * coordination of the submitted Task . Task.Subscriber A receiver of items produced by a Task.Coordinator . Task.Properties State sharing mechanism for tasks. Task.Collector A mutable reduction operation that accumulates results into a mutable result container, optionally transforming the accumulated result into a final representation after all results have been processed. ",
            "title": "Orchestration"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Task implementations define a single method called execute(Context) that performs the task, possibly yielding execution to some later point. Once the method has completed execution, by returning a result or throwing an exception (but not a Yield exception), the task is considered completed for the assigned Executor . A Task may yield execution for a given time by throwing a Yield exception. This exception type signals the execution of a Task by an Executor is to be suspended and resumed at some later point in time, typically by the same Executor . ",
            "title": "Tasks"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " When a Task is executed a Context instance will be passed as an execution argument. The Context provides access to task properties allowing shared state between tasks running in multiple Java Virtual Machines. The Context provides details on overall execution status: Execution State Method Description Complete Context.isDone() Allows a Task to determine if the task is complete. Completion may be due to normal termination, an exception or cancellation. In all of these cases, this method will return true . Cancelled Context.isCancelled() Allows a Task to determine if the task is effectively cancelled. Resuming Context.isResuming() Determines if a Task execution by an Executor resuming after being recovered (i.e. fail-over) or due to resumption after a task had previously thrown a Yield exception. ",
            "title": "Task Context"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Orchestrations begin by calling RemoteExecutor.orchestrate(Task) which will return a Task.Orchestration instance for the given Task . With the Task.Orchestration , it&#8217;s possible to configure the aspects of where the task will be run. Method Description concurrently() Tasks will be run, concurrently, across all Java Virtual Machines where the named executor is defined/configured. This is the default. sequentially() Tasks will be run, in sequence, across all Java Virtual Machines where the named executor is defined/configured. limit(int) Limit the task to n executors. Use this to limit the number of executors that will be considered for task execution. If not set, the default behavior is to run the task on all Java Virtual Machine where the named executor is defined/configured. filter(Predicate) Filtering provides an additional way to constrain where a task may be run. The predicates will be applied against metadata associated with each executor on each Java Virtual Machine. Some examples of metadata would be the member in which the executor is running, or the role of a member. Predicates may be chained to provide boolean logic in determining an appropriate executor. define(String, &lt;V&gt;) Define initial state that will be available to all tasks no matter which Java Virtual Machine that task is running on. retrain(Duration) When specified, the task will be retained allowing new subscribers to be notified of the final result of a task computation after it has completed. collect(Collector) This is the terminal of the orchestration builder returning a Task.Collectable which defines how results are to be collected and ultimately submits the task to the grid. ",
            "title": "Task Orchestration"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The Task.Collector passed to the orchestration will collect results from tasks and optionally transforms the collected results into a final format. Collectors are best illustrated by using examples of Collectors that are available in the TaskCollector class: Method Description count() The count of non-null results that have been collected from the executing task(s). firstOf() Collects and returns the first result provided by the executing task(s). lastOf() Collects and returns the last result returned by the executing task(s). setOf() Collects and returns all non-null results as a Set. listOf() Collects and returns all non-null results as a List. The Task.Collectable instance returned by calling collect on the orchestration allows, among other things, setting the condition under which no more results will be collected or published any registered subscribers. Calling submit() on the Task.Collectable will being the orchestration of the task. ",
            "title": "Task Collector and Collectable"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Upon calling submit() on the orchestration Collectable , a Task.Coordinator is returned. Like the Task.Collectable the Task.Coordinator allows for the registration of subscribers. Additionally, provides the ability to cancel or check the completion status of the orchestration. ",
            "title": "Task Coordinator"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The Task.Subscriber receives various events pertaining to the execution status of the orchestration: Method Description onComplete() Signals the completion of the orchestration. onError(Throwable) Called when an unrecoverable error (given as the argument) has occurred. onNext(&lt;T&gt;) Called when the Task.Coordinator has produced a result. onSubscribe(Task.Subscription) Called prior to any calls to onComplete() , onError(Throwable) , or onNext(&lt;T&gt;) are called. The Task.Subscription provided gives access to cancelling the subscription, or obtaining a reference to the Task.Coordinator . ",
            "title": "Task Subscriber"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " To begin, consider the following code common to the orchestration examples: <markup lang=\"java\" >// demonstrate orchestration using the default RemoteExecutor RemoteExecutor executor = RemoteExecutor.getDefault(); // WaitingSubscriber is an implementation of the // com.oracle.coherence.concurrent.executor.Task.Subscriber interface // that has a get() method that blocks until Subscriber.onComplete() is // called and will return the results received by onNext() WaitingSubscriber subscriber = new WaitingSubscriber(); // ValueTask is an implementation of the // com.oracle.coherence.concurrent.executor.Task interface // that returns the value provided at construction time ValueTask task = new ValueTask(\"Hello World\"); Given the above, the simplest example of an orchestration: <markup lang=\"java\" >// orchestrate the task, subscribe, and submit executor.orchestrate(task) .subscribe(subscriber) .submit(); // wait for the task to complete // if this was run on four cluster members running the default executor service, // the returned Collection will have four results Collection&lt;String&gt; results = subscriber.get(); Building on the above, assume a cluster with two storage and two proxy members. The cluster members are configured with the roles of storage and proxy , respectively. Let&#8217;s say the task needs to run on storage members only, then the orchestration could look like: <markup lang=\"java\" >// orchestrate the task, filtering by a role, subscribe, and submit executor.orchestrate(task) .filter(Predicates.role(\"storage\")) .subscribe(subscriber) .submit(); // wait for the task to complete // as there are only two storage members in this hypothetical, only two // results will be returned Collection&lt;String&gt; results = subscriber.get(); There are several predicates available for use in com.oracle.coherence.concurrent.executor.function.Predicates , however, in the case none apply to the target use case, simply implement the Remote.Predicate interface. Collection of results and how they are presented to the subscriber can be customized by using collect(Collector) and until(Predicate) : <markup lang=\"java\" >// orchestrate the task, collecting the first non-null result, // subscribe, and submit executor.orchestrate(new MayReturnNullTask()) .collect(TaskCollectors.firstOf()) .until(Predicates.nonNullValue()) .subscribe(subscriber) .submit(); // wait for the task to complete // the first non-result returned will be the one provided to the // subscriber Collection&lt;String&gt; results = subscriber.get(); Several collectors are provided in com.oracle.coherence.concurrent.executor.TaskCollectors , however, in the case none apply to the target use case, implement the Task.Collector interface. ",
            "title": "Advanced Orchestration Examples"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named 'Single' --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named `SingleTF` with a thread factor --&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;instance&gt; &lt;class-name&gt;my.custom.ThreadFactory&lt;/class-name&gt; &lt;/instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named 'Fixed5' with a thread-count of 5 --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Fixed5&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; <markup lang=\"xml\" >&lt;!-- creates a custom executor named 'custom' by calling com.acme.CustomExecutorFactory.createExecutor() --&gt; &lt;c:custom-executor&gt; &lt;c:name&gt;custom&lt;/c:name&gt; &lt;instance&gt; &lt;class-factory-name&gt;com.acme.CustomExecutorFactory&lt;/class-factory-name&gt; &lt;method-name&gt;createExecutor&lt;/method-name&gt; &lt;/instance&gt; &lt;/c:custom-executor&gt; ",
            "title": "Configuration Examples"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor custom-executor no java.util.concurrent.ExecutorService Defines a custom executor virtual-per-task no N/A Defines a VirtualThread-per-task executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes Depending on the context, it will yield either a java.util.concurrent.ExecutorService or a java.util.concurrent.ThreadFactory Defines how the ThreadFactory or the ExecutorService will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. Configuration Examples To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named 'Single' --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named `SingleTF` with a thread factor --&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;instance&gt; &lt;class-name&gt;my.custom.ThreadFactory&lt;/class-name&gt; &lt;/instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named 'Fixed5' with a thread-count of 5 --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Fixed5&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; <markup lang=\"xml\" >&lt;!-- creates a custom executor named 'custom' by calling com.acme.CustomExecutorFactory.createExecutor() --&gt; &lt;c:custom-executor&gt; &lt;c:name&gt;custom&lt;/c:name&gt; &lt;instance&gt; &lt;class-factory-name&gt;com.acme.CustomExecutorFactory&lt;/class-factory-name&gt; &lt;method-name&gt;createExecutor&lt;/method-name&gt; &lt;/instance&gt; &lt;/c:custom-executor&gt; ",
            "title": "Configuration Elements"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. ",
            "title": "ExecutorMBean Attributes"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. ",
            "title": "Operations"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The ExecutorMBean represents the operational state of a registered executor. The object name of the MBean is: <markup >type=Executor,name=&lt;executor name&gt;,nodeId=&lt;cluster node&gt; ExecutorMBean Attributes Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. Operations The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. ",
            "title": "Management"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Management over REST exposes endpoints to query and invoke actions against ExecutorMBean instances. Description Method Path Produces View all Executors GET /management/coherence/cluster/executors JSON View all Executors with matching name GET /management/coherence/cluster/executors/{name} JSON Reset Executor statistics by name POST /management/coherence/cluster/executors/{name}/resetStatistics JSON ",
            "title": "Management over REST"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " RemoteExecutors may be injected via CDI. For example: <markup lang=\"java\" >@Inject private RemoteExecutor single; @Inject @Name(\"Fixed5\") private RemoteExecutor fixedPoolRemoteExecutor; injects a RemoteExecutor named single . injects a RemoteExecutor named Fixed5 . ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Several executor types are available for configuration. ExecutorService Type Description Single thread Creates an ExecutorService with a single thread. Fixed thread Creates an ExecutorService with a fixed number of threads. Cached Create an ExecutorService that will create new threads as needed and reuse existing threads when possible. Work stealing Creates a work-stealing thread pool using the number of available processors as its target parallelism level. Custom Allows the creation of non-standard executors. VirtualThread Creates a VirtualThread-per-task ExecutorService. Requires JDK 21 or newer. Configuration Elements Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor custom-executor no java.util.concurrent.ExecutorService Defines a custom executor virtual-per-task no N/A Defines a VirtualThread-per-task executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes Depending on the context, it will yield either a java.util.concurrent.ExecutorService or a java.util.concurrent.ThreadFactory Defines how the ThreadFactory or the ExecutorService will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. Configuration Examples To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named 'Single' --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named `SingleTF` with a thread factor --&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;instance&gt; &lt;class-name&gt;my.custom.ThreadFactory&lt;/class-name&gt; &lt;/instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named 'Fixed5' with a thread-count of 5 --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Fixed5&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; <markup lang=\"xml\" >&lt;!-- creates a custom executor named 'custom' by calling com.acme.CustomExecutorFactory.createExecutor() --&gt; &lt;c:custom-executor&gt; &lt;c:name&gt;custom&lt;/c:name&gt; &lt;instance&gt; &lt;class-factory-name&gt;com.acme.CustomExecutorFactory&lt;/class-factory-name&gt; &lt;method-name&gt;createExecutor&lt;/method-name&gt; &lt;/instance&gt; &lt;/c:custom-executor&gt; Management The ExecutorMBean represents the operational state of a registered executor. The object name of the MBean is: <markup >type=Executor,name=&lt;executor name&gt;,nodeId=&lt;cluster node&gt; ExecutorMBean Attributes Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. Operations The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. Management over REST Coherence Management over REST exposes endpoints to query and invoke actions against ExecutorMBean instances. Description Method Path Produces View all Executors GET /management/coherence/cluster/executors JSON View all Executors with matching name GET /management/coherence/cluster/executors/{name} JSON Reset Executor statistics by name POST /management/coherence/cluster/executors/{name}/resetStatistics JSON CDI Support RemoteExecutors may be injected via CDI. For example: <markup lang=\"java\" >@Inject private RemoteExecutor single; @Inject @Name(\"Fixed5\") private RemoteExecutor fixedPoolRemoteExecutor; injects a RemoteExecutor named single . injects a RemoteExecutor named Fixed5 . ",
            "title": "Configuration"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The instances of numeric atomic types, such as AtomicInteger and AtomicLong , are frequently used to represent various counters in the application, where a client may need to increment the value, but doesn&#8217;t necessarily need to know what the new value is. When working with the local atomics, the same API shown above can be used, and the return value simply ignored. However, when using distributed atomics that would introduce unnecessary blocking on the client while waiting for the response from the server, which would then simply be discarded. Obviously, this would have negative impact on both performance and throughput of the atomics. To reduce the impact of remote calls in those situations, Coherence Concurrent also provides non-blocking, asynchronous implementations of all atomic types it supports. To obtain a non-blocking instance of any supported atomic type, simply call async method on the blocking instance of that type: <markup lang=\"java\" >AsyncAtomicInteger asyncFoo = Atomics.remoteAtomicInteger(\"foo\").async(); AsyncAtomicLong asyncBar = Atomics.remoteAtomicLong(\"bar\", 5L).async(); creates a remote, distributed instance of named, non-blocking AsyncAtomicInteger , with an implicit initial value of 0 creates a remote, distributed instance of named, non-blocking AsyncAtomicLong , with an initial value of 5 Once created, these instances can be used the same way you would use any of the corresponding blocking types. The only difference is that they will simply return a CompletableFuture for the result, and will not block: <markup lang=\"java\" >CompletableFuture&lt;Integer&gt; futureCounter1 = asyncFoo.incrementAndGet(); CompletableFuture&lt;Long&gt; futureCounter5 = asyncBar.addAndGet(5L); Both the blocking and the non-blocking instance of any distributed atomic type, with the same name, are backed by the same cluster-side atomic instance state, so they can be used interchangeably. ",
            "title": "Asynchronous Implementations"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Atomic types from Coherence Concurrent can also be injected using CDI, which eliminates the need for explicit factory method calls on the Atomics class. <markup lang=\"java\" >@Inject @Name(\"foo\") private AtomicInteger localFoo; @Inject @Remote @Name(\"foo\") private AtomicInteger remoteFoo; @Inject @Remote private AsyncAtomicLong asyncBar; injects a local, in-process instance of an AtomicInteger named foo , with an implicit initial value of 0 injects a remote, distributed instance of an AtomicInteger named foo , distinct from the local instance foo , with an implicit initial value of 0 injects a remote, distributed instance of non-blocking AsyncAtomicLong , with an implicit name of asyncBar Once an instance of an atomic type is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Atomics factory class. ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent provides distributed implementations of atomic types, such as AtomicInteger , AtomicLong and AtomicReference . It also provides local implementations of the same types. The local implementations are just thin wrappers around existing java.util.concurrent.atomic types, which implement the same interface as their distributed variants, in order to be interchangeable. To create instances of atomic types you need to call the appropriate factory method on the Atomics class: <markup lang=\"java\" >AtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); AtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); AtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); creates a local, in-process instance of named AtomicInteger with an implicit initial value of 0 creates a remote, distributed instance of named AtomicInteger , distinct from the local instance foo , with an implicit initial value of 0 creates a remote, distributed instance of named AtomicLong , with an initial value of 5 Note that the AtomicInteger and AtomicLong types used above are not types from the java.util.concurrent.atomic package that you are familiar with&#8201;&#8212;&#8201;they are actually interfaces defined within com.oracle.coherence.concurrent.atomic package, that both LocalAtomicXyz and RemoteAtomicXyz classes implement, which are the instances that are actually returned by the methods above. That means that the above code could be rewritten as: <markup lang=\"java\" >LocalAtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); RemoteAtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); RemoteAtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); However, we strongly suggest that you use interfaces instead of concrete types, as they make it easy to switch between local and distributed implementations when necessary. Once created, these instances can be used the same way you would use any of the corresponding java.util.concurrent.atomic types: <markup lang=\"java\" >int counter1 = remoteFoo.incrementAndGet(); long counter5 = remoteBar.addAndGet(5L); Asynchronous Implementations The instances of numeric atomic types, such as AtomicInteger and AtomicLong , are frequently used to represent various counters in the application, where a client may need to increment the value, but doesn&#8217;t necessarily need to know what the new value is. When working with the local atomics, the same API shown above can be used, and the return value simply ignored. However, when using distributed atomics that would introduce unnecessary blocking on the client while waiting for the response from the server, which would then simply be discarded. Obviously, this would have negative impact on both performance and throughput of the atomics. To reduce the impact of remote calls in those situations, Coherence Concurrent also provides non-blocking, asynchronous implementations of all atomic types it supports. To obtain a non-blocking instance of any supported atomic type, simply call async method on the blocking instance of that type: <markup lang=\"java\" >AsyncAtomicInteger asyncFoo = Atomics.remoteAtomicInteger(\"foo\").async(); AsyncAtomicLong asyncBar = Atomics.remoteAtomicLong(\"bar\", 5L).async(); creates a remote, distributed instance of named, non-blocking AsyncAtomicInteger , with an implicit initial value of 0 creates a remote, distributed instance of named, non-blocking AsyncAtomicLong , with an initial value of 5 Once created, these instances can be used the same way you would use any of the corresponding blocking types. The only difference is that they will simply return a CompletableFuture for the result, and will not block: <markup lang=\"java\" >CompletableFuture&lt;Integer&gt; futureCounter1 = asyncFoo.incrementAndGet(); CompletableFuture&lt;Long&gt; futureCounter5 = asyncBar.addAndGet(5L); Both the blocking and the non-blocking instance of any distributed atomic type, with the same name, are backed by the same cluster-side atomic instance state, so they can be used interchangeably. CDI Support Atomic types from Coherence Concurrent can also be injected using CDI, which eliminates the need for explicit factory method calls on the Atomics class. <markup lang=\"java\" >@Inject @Name(\"foo\") private AtomicInteger localFoo; @Inject @Remote @Name(\"foo\") private AtomicInteger remoteFoo; @Inject @Remote private AsyncAtomicLong asyncBar; injects a local, in-process instance of an AtomicInteger named foo , with an implicit initial value of 0 injects a remote, distributed instance of an AtomicInteger named foo , distinct from the local instance foo , with an implicit initial value of 0 injects a remote, distributed instance of non-blocking AsyncAtomicLong , with an implicit name of asyncBar Once an instance of an atomic type is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Atomics factory class. ",
            "title": "Atomics"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " A RemoteLock class provides an implementation of a Lock interface and allows you to ensure that only one thread on one member is running critical section guarded by the lock at any given time. To obtain an instance of a RemoteLock , call Locks.remoteLock factory method: <markup lang=\"java\" >Lock foo = Locks.remoteLock(\"foo\"); Just like with Atomics , you can also obtain a local Lock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantLock , by calling localLock factory method: <markup lang=\"java\" >Lock foo = Locks.localLock(\"foo\"); Once you have a Lock instance, you can use it as you normally would: <markup lang=\"java\" >foo.lock(); try { // critical section guarded by the exclusive lock `foo` } finally { foo.unlock(); } ",
            "title": "Exclusive Locks"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " A RemoteReadWriteLock class provides an implementation of a ReadWriteLock interface and allows you to ensure that only one thread on one member is running critical section guarded by the write lock at any given time, while allowing multiple concurrent readers. To obtain an instance of a RemoteReadWriteLock , call Locks.remoteReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.remoteReadWriteLock(\"bar\"); Just like with Atomics , you can also obtain a local ReadWriteLock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantReadWriteLock , by calling localReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.localReadWriteLock(\"bar\"); Once you have a ReadWriteLock instance, you can use it as you normally would: <markup lang=\"java\" >bar.writeLock().lock(); try { // critical section guarded by the exclusive write lock `bar` } finally { bar.writeLock().unlock(); } Or: <markup lang=\"java\" >bar.readLock().lock(); try { // critical section guarded by the shared read lock `bar` } finally { bar.readLock().unlock(); } ",
            "title": "Read/Write Locks"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " You can also use CDI to inject both the exclusive and read/write lock instances into objects that need them: <markup lang=\"java\" >@Inject @Remote @Name(\"foo\") private Lock lock; @Inject @Remote private ReadWriteLock bar; injects distributed exclusive lock named foo into lock field injects distributed read/write lock named bar into bar field Once an instance of lock is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Locks factory class. ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent provides distributed implementations of Lock and ReadWriteLock interfaces from the java.util.concurrent.locks package, allowing you to implement lock-based concurrency control across cluster members when necessary. Unlike local JDK implementations, the classes in this package use cluster member/process ID and thread ID to identify lock owner, and store shared lock state within a Coherence NamedMap . However, that also implies that the calls to acquire and release locks are remote, network calls, as they need to update shared state that is likely stored on a different cluster member, which will have an impact on performance of lock and unlock operations. Exclusive Locks A RemoteLock class provides an implementation of a Lock interface and allows you to ensure that only one thread on one member is running critical section guarded by the lock at any given time. To obtain an instance of a RemoteLock , call Locks.remoteLock factory method: <markup lang=\"java\" >Lock foo = Locks.remoteLock(\"foo\"); Just like with Atomics , you can also obtain a local Lock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantLock , by calling localLock factory method: <markup lang=\"java\" >Lock foo = Locks.localLock(\"foo\"); Once you have a Lock instance, you can use it as you normally would: <markup lang=\"java\" >foo.lock(); try { // critical section guarded by the exclusive lock `foo` } finally { foo.unlock(); } Read/Write Locks A RemoteReadWriteLock class provides an implementation of a ReadWriteLock interface and allows you to ensure that only one thread on one member is running critical section guarded by the write lock at any given time, while allowing multiple concurrent readers. To obtain an instance of a RemoteReadWriteLock , call Locks.remoteReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.remoteReadWriteLock(\"bar\"); Just like with Atomics , you can also obtain a local ReadWriteLock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantReadWriteLock , by calling localReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.localReadWriteLock(\"bar\"); Once you have a ReadWriteLock instance, you can use it as you normally would: <markup lang=\"java\" >bar.writeLock().lock(); try { // critical section guarded by the exclusive write lock `bar` } finally { bar.writeLock().unlock(); } Or: <markup lang=\"java\" >bar.readLock().lock(); try { // critical section guarded by the shared read lock `bar` } finally { bar.readLock().unlock(); } CDI Support You can also use CDI to inject both the exclusive and read/write lock instances into objects that need them: <markup lang=\"java\" >@Inject @Remote @Name(\"foo\") private Lock lock; @Inject @Remote private ReadWriteLock bar; injects distributed exclusive lock named foo into lock field injects distributed read/write lock named bar into bar field Once an instance of lock is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Locks factory class. ",
            "title": "Locks"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The RemoteCoundDownLatch class provides a distributed implementation of a CountDownLatch , and allows you to ensure that the execution of the code on any cluster member that is waiting for the latch proceeds only when the latch reaches zero. Any cluster member can both wait for a latch, and count down. To obtain an instance of a RemoteCountDownLatch , call Latches.remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.remoteCountDownLatch(\"foo\", 5); create an instance of a RemoteCountDownLatch with the initial count of 5 Just like with Atomics and Locks , you can also obtain a local CountDownLatch instance from the Latches class by calling remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.localCountDownLatch(\"foo\", 10); create an instance of a LocalCountDownLatch with the initial count of 10 Once you have a RemoteCountDownLatch instance, you can use it as you normally would, by calling countDown and await methods on it. ",
            "title": "Count Down Latch"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The RemoteSemaphore class provides a distributed implementation of a Semaphore , and allows any cluster member to acquire and release permits from the same semaphore instance. To obtain an instance of a RemoteSemaphore , call Semaphores.remoteSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.remoteSemaphore(\"foo\", 5); create an instance of a RemoteSemaphore with 5 permits Just like with Atomics and Locks , you can also obtain a local Semaphore instance from the Semaphores class by calling localSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.localSemaphore(\"foo\"); create an instance of a LocalSemaphore with 0 permits Once you have a Semaphore instance, you can use it as you normally would, by calling release and acquire methods on it. ",
            "title": "Semaphore"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " You can also use CDI to inject both the CountDownLatch and Semaphore instances into objects that need them: <markup lang=\"java\" >@Inject @Name(\"foo\") @Count(5) private CountDownLatch localLatchFoo; @Inject @Name(\"foo\") @Remote @Count(10) private CountDownLatch remoteLatchFoo; @Inject @Name(\"bar\") @Remote private Semaphore localSemaphoreBar; @Inject @Name(\"bar\") @Remote @Permits(1) private Semaphore remoteSemaphoreBar; inject an instance of a LocalCountDownLatch with the initial count of five inject an instance of a RemoteCountDownLatch with the initial count of ten inject an instance of a LocalSemaphore with zero permits available inject an instance of a RemoteSemaphore with one permit available Once a latch or a semaphore instance is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Latches or Semaphores factory classes. The @Name annotation is optional in both cases, as long as the member name (in the examples above, the field name) can be obtained from the injection point, but is required otherwise (such as when using constructor injection). The @Count annotation specifies the initial latch count, and if omitted will be defaulted to one. The @Permits annotation specifies the number of available permits for a semaphore, and if omitted will be defaulted to zero, which means that the first acquire call will block until another thread releases one or more permits. ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent also provides distributed implementations of a CountDownLatch and Semaphore classes from java.util.concurrent package, allowing you to implement synchronization of execution across multiple Coherence cluster members as easily as you can implement it within a single process using those two JDK classes. It also provides interfaces for those two concurrency primitives, that both remote and local implementations conform to. Just like with atomics, the local implementations are nothing more than thin wrappers around corresponding JDK classes. Count Down Latch The RemoteCoundDownLatch class provides a distributed implementation of a CountDownLatch , and allows you to ensure that the execution of the code on any cluster member that is waiting for the latch proceeds only when the latch reaches zero. Any cluster member can both wait for a latch, and count down. To obtain an instance of a RemoteCountDownLatch , call Latches.remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.remoteCountDownLatch(\"foo\", 5); create an instance of a RemoteCountDownLatch with the initial count of 5 Just like with Atomics and Locks , you can also obtain a local CountDownLatch instance from the Latches class by calling remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.localCountDownLatch(\"foo\", 10); create an instance of a LocalCountDownLatch with the initial count of 10 Once you have a RemoteCountDownLatch instance, you can use it as you normally would, by calling countDown and await methods on it. Semaphore The RemoteSemaphore class provides a distributed implementation of a Semaphore , and allows any cluster member to acquire and release permits from the same semaphore instance. To obtain an instance of a RemoteSemaphore , call Semaphores.remoteSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.remoteSemaphore(\"foo\", 5); create an instance of a RemoteSemaphore with 5 permits Just like with Atomics and Locks , you can also obtain a local Semaphore instance from the Semaphores class by calling localSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.localSemaphore(\"foo\"); create an instance of a LocalSemaphore with 0 permits Once you have a Semaphore instance, you can use it as you normally would, by calling release and acquire methods on it. CDI Support You can also use CDI to inject both the CountDownLatch and Semaphore instances into objects that need them: <markup lang=\"java\" >@Inject @Name(\"foo\") @Count(5) private CountDownLatch localLatchFoo; @Inject @Name(\"foo\") @Remote @Count(10) private CountDownLatch remoteLatchFoo; @Inject @Name(\"bar\") @Remote private Semaphore localSemaphoreBar; @Inject @Name(\"bar\") @Remote @Permits(1) private Semaphore remoteSemaphoreBar; inject an instance of a LocalCountDownLatch with the initial count of five inject an instance of a RemoteCountDownLatch with the initial count of ten inject an instance of a LocalSemaphore with zero permits available inject an instance of a RemoteSemaphore with one permit available Once a latch or a semaphore instance is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Latches or Semaphores factory classes. The @Name annotation is optional in both cases, as long as the member name (in the examples above, the field name) can be obtained from the injection point, but is required otherwise (such as when using constructor injection). The @Count annotation specifies the initial latch count, and if omitted will be defaulted to one. The @Permits annotation specifies the number of available permits for a semaphore, and if omitted will be defaulted to zero, which means that the first acquire call will block until another thread releases one or more permits. ",
            "title": "Latches and Semaphores"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " In order to use Coherence Concurrent features, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-concurrent&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; Once the necessary dependency is in place, you can start using the features it provides, as the following sections describe. Executors Executors Overview Executors Usage Advanced Orchestration Tasks Task Context Task Orchestration Task Collector and Collectable Task Coordinator Task Subscriber Advanced Orchestration Examples Executors Configuration Executors Configuration Examples Executors Management Executors Management over REST CDI Support for Executors Atomics Non-blocking Atomics CDI Support for Atomics Locks Exclusive Locks Read/Write Locks CDI Support for Locks Latches and Semaphores Count Down Latch Semaphore CDI Support for Latches and Semaphores Executors Overview Coherence Concurrent provides a facility to dispatch tasks, either a Runnable , Callable , or Task to a Coherence cluster for execution. Executors that will actually execute the submitted tasks are configured on each cluster member by defining one or more named executors within a cache configuration resource. Usage Examples By default, each Coherence cluster with the coherence-concurrent module on the classpath, will include a single-threaded executor that may be used to execute dispatched tasks. Given this, the simplest example would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.getDefault(); Future&lt;Void&gt; result = remoteExecutor.submit(() -&gt; System.out.println(\"Executed\")); result.get(); // block until completion If for example, an executor was configured named Fixed5 , the code would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.get(\"Fixed5\"); If no executor has been configured with the given name, the RemoteExecutor will throw RejectedExecutionException . Each RemoteExecutor instance may hold local resources that should be released when the RemoteExecutor is no longer needed. Like an ExecutorService , a RemoteExecutor has similar methods to shut the executor down. When calling these methods, it will have no impact on the executors registered within the cluster. Orchestration While the RemoteExecutor does provide functionality similar to the standard ExecutorService included in the JDK, this may not be enough in the context of Coherence. A task might need to run across multiple Coherence members, produce intermediate results, and remain durable in case a cluster member executing the task fails. In such cases, task orchestration can be used. Before diving into the details of orchestration, the following concepts should be understood: Interface Description Task Tasks are like Callable and Runnable classes in that they are designed to be potentially executed by one or more threads. Unlike Callable and Runnable classes, the execution may occur in different Java Virtual Machines, fail and/or recover between different Java Virtual Machine processes. Task.Context Provides contextual information for a Task as it is executed, including the ability to access and update intermediate results for the Executor executing the said Task . Task.Orchestration Defines information concerning the orchestration of a Task across a set of executors defined across multiple Coherence members for a given RemoteExecutor . Task.Coordinator A publisher of collected Task results that additionally permits * coordination of the submitted Task . Task.Subscriber A receiver of items produced by a Task.Coordinator . Task.Properties State sharing mechanism for tasks. Task.Collector A mutable reduction operation that accumulates results into a mutable result container, optionally transforming the accumulated result into a final representation after all results have been processed. Tasks Task implementations define a single method called execute(Context) that performs the task, possibly yielding execution to some later point. Once the method has completed execution, by returning a result or throwing an exception (but not a Yield exception), the task is considered completed for the assigned Executor . A Task may yield execution for a given time by throwing a Yield exception. This exception type signals the execution of a Task by an Executor is to be suspended and resumed at some later point in time, typically by the same Executor . Task Context When a Task is executed a Context instance will be passed as an execution argument. The Context provides access to task properties allowing shared state between tasks running in multiple Java Virtual Machines. The Context provides details on overall execution status: Execution State Method Description Complete Context.isDone() Allows a Task to determine if the task is complete. Completion may be due to normal termination, an exception or cancellation. In all of these cases, this method will return true . Cancelled Context.isCancelled() Allows a Task to determine if the task is effectively cancelled. Resuming Context.isResuming() Determines if a Task execution by an Executor resuming after being recovered (i.e. fail-over) or due to resumption after a task had previously thrown a Yield exception. Task Orchestration Orchestrations begin by calling RemoteExecutor.orchestrate(Task) which will return a Task.Orchestration instance for the given Task . With the Task.Orchestration , it&#8217;s possible to configure the aspects of where the task will be run. Method Description concurrently() Tasks will be run, concurrently, across all Java Virtual Machines where the named executor is defined/configured. This is the default. sequentially() Tasks will be run, in sequence, across all Java Virtual Machines where the named executor is defined/configured. limit(int) Limit the task to n executors. Use this to limit the number of executors that will be considered for task execution. If not set, the default behavior is to run the task on all Java Virtual Machine where the named executor is defined/configured. filter(Predicate) Filtering provides an additional way to constrain where a task may be run. The predicates will be applied against metadata associated with each executor on each Java Virtual Machine. Some examples of metadata would be the member in which the executor is running, or the role of a member. Predicates may be chained to provide boolean logic in determining an appropriate executor. define(String, &lt;V&gt;) Define initial state that will be available to all tasks no matter which Java Virtual Machine that task is running on. retrain(Duration) When specified, the task will be retained allowing new subscribers to be notified of the final result of a task computation after it has completed. collect(Collector) This is the terminal of the orchestration builder returning a Task.Collectable which defines how results are to be collected and ultimately submits the task to the grid. Task Collector and Collectable The Task.Collector passed to the orchestration will collect results from tasks and optionally transforms the collected results into a final format. Collectors are best illustrated by using examples of Collectors that are available in the TaskCollector class: Method Description count() The count of non-null results that have been collected from the executing task(s). firstOf() Collects and returns the first result provided by the executing task(s). lastOf() Collects and returns the last result returned by the executing task(s). setOf() Collects and returns all non-null results as a Set. listOf() Collects and returns all non-null results as a List. The Task.Collectable instance returned by calling collect on the orchestration allows, among other things, setting the condition under which no more results will be collected or published any registered subscribers. Calling submit() on the Task.Collectable will being the orchestration of the task. Task Coordinator Upon calling submit() on the orchestration Collectable , a Task.Coordinator is returned. Like the Task.Collectable the Task.Coordinator allows for the registration of subscribers. Additionally, provides the ability to cancel or check the completion status of the orchestration. Task Subscriber The Task.Subscriber receives various events pertaining to the execution status of the orchestration: Method Description onComplete() Signals the completion of the orchestration. onError(Throwable) Called when an unrecoverable error (given as the argument) has occurred. onNext(&lt;T&gt;) Called when the Task.Coordinator has produced a result. onSubscribe(Task.Subscription) Called prior to any calls to onComplete() , onError(Throwable) , or onNext(&lt;T&gt;) are called. The Task.Subscription provided gives access to cancelling the subscription, or obtaining a reference to the Task.Coordinator . Advanced Orchestration Examples To begin, consider the following code common to the orchestration examples: <markup lang=\"java\" >// demonstrate orchestration using the default RemoteExecutor RemoteExecutor executor = RemoteExecutor.getDefault(); // WaitingSubscriber is an implementation of the // com.oracle.coherence.concurrent.executor.Task.Subscriber interface // that has a get() method that blocks until Subscriber.onComplete() is // called and will return the results received by onNext() WaitingSubscriber subscriber = new WaitingSubscriber(); // ValueTask is an implementation of the // com.oracle.coherence.concurrent.executor.Task interface // that returns the value provided at construction time ValueTask task = new ValueTask(\"Hello World\"); Given the above, the simplest example of an orchestration: <markup lang=\"java\" >// orchestrate the task, subscribe, and submit executor.orchestrate(task) .subscribe(subscriber) .submit(); // wait for the task to complete // if this was run on four cluster members running the default executor service, // the returned Collection will have four results Collection&lt;String&gt; results = subscriber.get(); Building on the above, assume a cluster with two storage and two proxy members. The cluster members are configured with the roles of storage and proxy , respectively. Let&#8217;s say the task needs to run on storage members only, then the orchestration could look like: <markup lang=\"java\" >// orchestrate the task, filtering by a role, subscribe, and submit executor.orchestrate(task) .filter(Predicates.role(\"storage\")) .subscribe(subscriber) .submit(); // wait for the task to complete // as there are only two storage members in this hypothetical, only two // results will be returned Collection&lt;String&gt; results = subscriber.get(); There are several predicates available for use in com.oracle.coherence.concurrent.executor.function.Predicates , however, in the case none apply to the target use case, simply implement the Remote.Predicate interface. Collection of results and how they are presented to the subscriber can be customized by using collect(Collector) and until(Predicate) : <markup lang=\"java\" >// orchestrate the task, collecting the first non-null result, // subscribe, and submit executor.orchestrate(new MayReturnNullTask()) .collect(TaskCollectors.firstOf()) .until(Predicates.nonNullValue()) .subscribe(subscriber) .submit(); // wait for the task to complete // the first non-result returned will be the one provided to the // subscriber Collection&lt;String&gt; results = subscriber.get(); Several collectors are provided in com.oracle.coherence.concurrent.executor.TaskCollectors , however, in the case none apply to the target use case, implement the Task.Collector interface. Configuration Several executor types are available for configuration. ExecutorService Type Description Single thread Creates an ExecutorService with a single thread. Fixed thread Creates an ExecutorService with a fixed number of threads. Cached Create an ExecutorService that will create new threads as needed and reuse existing threads when possible. Work stealing Creates a work-stealing thread pool using the number of available processors as its target parallelism level. Custom Allows the creation of non-standard executors. VirtualThread Creates a VirtualThread-per-task ExecutorService. Requires JDK 21 or newer. Configuration Elements Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor custom-executor no java.util.concurrent.ExecutorService Defines a custom executor virtual-per-task no N/A Defines a VirtualThread-per-task executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes Depending on the context, it will yield either a java.util.concurrent.ExecutorService or a java.util.concurrent.ThreadFactory Defines how the ThreadFactory or the ExecutorService will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. Configuration Examples To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named 'Single' --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named `SingleTF` with a thread factor --&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;instance&gt; &lt;class-name&gt;my.custom.ThreadFactory&lt;/class-name&gt; &lt;/instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named 'Fixed5' with a thread-count of 5 --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Fixed5&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; <markup lang=\"xml\" >&lt;!-- creates a custom executor named 'custom' by calling com.acme.CustomExecutorFactory.createExecutor() --&gt; &lt;c:custom-executor&gt; &lt;c:name&gt;custom&lt;/c:name&gt; &lt;instance&gt; &lt;class-factory-name&gt;com.acme.CustomExecutorFactory&lt;/class-factory-name&gt; &lt;method-name&gt;createExecutor&lt;/method-name&gt; &lt;/instance&gt; &lt;/c:custom-executor&gt; Management The ExecutorMBean represents the operational state of a registered executor. The object name of the MBean is: <markup >type=Executor,name=&lt;executor name&gt;,nodeId=&lt;cluster node&gt; ExecutorMBean Attributes Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. Operations The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. Management over REST Coherence Management over REST exposes endpoints to query and invoke actions against ExecutorMBean instances. Description Method Path Produces View all Executors GET /management/coherence/cluster/executors JSON View all Executors with matching name GET /management/coherence/cluster/executors/{name} JSON Reset Executor statistics by name POST /management/coherence/cluster/executors/{name}/resetStatistics JSON CDI Support RemoteExecutors may be injected via CDI. For example: <markup lang=\"java\" >@Inject private RemoteExecutor single; @Inject @Name(\"Fixed5\") private RemoteExecutor fixedPoolRemoteExecutor; injects a RemoteExecutor named single . injects a RemoteExecutor named Fixed5 . Atomics Coherence Concurrent provides distributed implementations of atomic types, such as AtomicInteger , AtomicLong and AtomicReference . It also provides local implementations of the same types. The local implementations are just thin wrappers around existing java.util.concurrent.atomic types, which implement the same interface as their distributed variants, in order to be interchangeable. To create instances of atomic types you need to call the appropriate factory method on the Atomics class: <markup lang=\"java\" >AtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); AtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); AtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); creates a local, in-process instance of named AtomicInteger with an implicit initial value of 0 creates a remote, distributed instance of named AtomicInteger , distinct from the local instance foo , with an implicit initial value of 0 creates a remote, distributed instance of named AtomicLong , with an initial value of 5 Note that the AtomicInteger and AtomicLong types used above are not types from the java.util.concurrent.atomic package that you are familiar with&#8201;&#8212;&#8201;they are actually interfaces defined within com.oracle.coherence.concurrent.atomic package, that both LocalAtomicXyz and RemoteAtomicXyz classes implement, which are the instances that are actually returned by the methods above. That means that the above code could be rewritten as: <markup lang=\"java\" >LocalAtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); RemoteAtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); RemoteAtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); However, we strongly suggest that you use interfaces instead of concrete types, as they make it easy to switch between local and distributed implementations when necessary. Once created, these instances can be used the same way you would use any of the corresponding java.util.concurrent.atomic types: <markup lang=\"java\" >int counter1 = remoteFoo.incrementAndGet(); long counter5 = remoteBar.addAndGet(5L); Asynchronous Implementations The instances of numeric atomic types, such as AtomicInteger and AtomicLong , are frequently used to represent various counters in the application, where a client may need to increment the value, but doesn&#8217;t necessarily need to know what the new value is. When working with the local atomics, the same API shown above can be used, and the return value simply ignored. However, when using distributed atomics that would introduce unnecessary blocking on the client while waiting for the response from the server, which would then simply be discarded. Obviously, this would have negative impact on both performance and throughput of the atomics. To reduce the impact of remote calls in those situations, Coherence Concurrent also provides non-blocking, asynchronous implementations of all atomic types it supports. To obtain a non-blocking instance of any supported atomic type, simply call async method on the blocking instance of that type: <markup lang=\"java\" >AsyncAtomicInteger asyncFoo = Atomics.remoteAtomicInteger(\"foo\").async(); AsyncAtomicLong asyncBar = Atomics.remoteAtomicLong(\"bar\", 5L).async(); creates a remote, distributed instance of named, non-blocking AsyncAtomicInteger , with an implicit initial value of 0 creates a remote, distributed instance of named, non-blocking AsyncAtomicLong , with an initial value of 5 Once created, these instances can be used the same way you would use any of the corresponding blocking types. The only difference is that they will simply return a CompletableFuture for the result, and will not block: <markup lang=\"java\" >CompletableFuture&lt;Integer&gt; futureCounter1 = asyncFoo.incrementAndGet(); CompletableFuture&lt;Long&gt; futureCounter5 = asyncBar.addAndGet(5L); Both the blocking and the non-blocking instance of any distributed atomic type, with the same name, are backed by the same cluster-side atomic instance state, so they can be used interchangeably. CDI Support Atomic types from Coherence Concurrent can also be injected using CDI, which eliminates the need for explicit factory method calls on the Atomics class. <markup lang=\"java\" >@Inject @Name(\"foo\") private AtomicInteger localFoo; @Inject @Remote @Name(\"foo\") private AtomicInteger remoteFoo; @Inject @Remote private AsyncAtomicLong asyncBar; injects a local, in-process instance of an AtomicInteger named foo , with an implicit initial value of 0 injects a remote, distributed instance of an AtomicInteger named foo , distinct from the local instance foo , with an implicit initial value of 0 injects a remote, distributed instance of non-blocking AsyncAtomicLong , with an implicit name of asyncBar Once an instance of an atomic type is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Atomics factory class. Locks Coherence Concurrent provides distributed implementations of Lock and ReadWriteLock interfaces from the java.util.concurrent.locks package, allowing you to implement lock-based concurrency control across cluster members when necessary. Unlike local JDK implementations, the classes in this package use cluster member/process ID and thread ID to identify lock owner, and store shared lock state within a Coherence NamedMap . However, that also implies that the calls to acquire and release locks are remote, network calls, as they need to update shared state that is likely stored on a different cluster member, which will have an impact on performance of lock and unlock operations. Exclusive Locks A RemoteLock class provides an implementation of a Lock interface and allows you to ensure that only one thread on one member is running critical section guarded by the lock at any given time. To obtain an instance of a RemoteLock , call Locks.remoteLock factory method: <markup lang=\"java\" >Lock foo = Locks.remoteLock(\"foo\"); Just like with Atomics , you can also obtain a local Lock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantLock , by calling localLock factory method: <markup lang=\"java\" >Lock foo = Locks.localLock(\"foo\"); Once you have a Lock instance, you can use it as you normally would: <markup lang=\"java\" >foo.lock(); try { // critical section guarded by the exclusive lock `foo` } finally { foo.unlock(); } Read/Write Locks A RemoteReadWriteLock class provides an implementation of a ReadWriteLock interface and allows you to ensure that only one thread on one member is running critical section guarded by the write lock at any given time, while allowing multiple concurrent readers. To obtain an instance of a RemoteReadWriteLock , call Locks.remoteReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.remoteReadWriteLock(\"bar\"); Just like with Atomics , you can also obtain a local ReadWriteLock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantReadWriteLock , by calling localReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.localReadWriteLock(\"bar\"); Once you have a ReadWriteLock instance, you can use it as you normally would: <markup lang=\"java\" >bar.writeLock().lock(); try { // critical section guarded by the exclusive write lock `bar` } finally { bar.writeLock().unlock(); } Or: <markup lang=\"java\" >bar.readLock().lock(); try { // critical section guarded by the shared read lock `bar` } finally { bar.readLock().unlock(); } CDI Support You can also use CDI to inject both the exclusive and read/write lock instances into objects that need them: <markup lang=\"java\" >@Inject @Remote @Name(\"foo\") private Lock lock; @Inject @Remote private ReadWriteLock bar; injects distributed exclusive lock named foo into lock field injects distributed read/write lock named bar into bar field Once an instance of lock is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Locks factory class. Latches and Semaphores Coherence Concurrent also provides distributed implementations of a CountDownLatch and Semaphore classes from java.util.concurrent package, allowing you to implement synchronization of execution across multiple Coherence cluster members as easily as you can implement it within a single process using those two JDK classes. It also provides interfaces for those two concurrency primitives, that both remote and local implementations conform to. Just like with atomics, the local implementations are nothing more than thin wrappers around corresponding JDK classes. Count Down Latch The RemoteCoundDownLatch class provides a distributed implementation of a CountDownLatch , and allows you to ensure that the execution of the code on any cluster member that is waiting for the latch proceeds only when the latch reaches zero. Any cluster member can both wait for a latch, and count down. To obtain an instance of a RemoteCountDownLatch , call Latches.remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.remoteCountDownLatch(\"foo\", 5); create an instance of a RemoteCountDownLatch with the initial count of 5 Just like with Atomics and Locks , you can also obtain a local CountDownLatch instance from the Latches class by calling remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.localCountDownLatch(\"foo\", 10); create an instance of a LocalCountDownLatch with the initial count of 10 Once you have a RemoteCountDownLatch instance, you can use it as you normally would, by calling countDown and await methods on it. Semaphore The RemoteSemaphore class provides a distributed implementation of a Semaphore , and allows any cluster member to acquire and release permits from the same semaphore instance. To obtain an instance of a RemoteSemaphore , call Semaphores.remoteSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.remoteSemaphore(\"foo\", 5); create an instance of a RemoteSemaphore with 5 permits Just like with Atomics and Locks , you can also obtain a local Semaphore instance from the Semaphores class by calling localSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.localSemaphore(\"foo\"); create an instance of a LocalSemaphore with 0 permits Once you have a Semaphore instance, you can use it as you normally would, by calling release and acquire methods on it. CDI Support You can also use CDI to inject both the CountDownLatch and Semaphore instances into objects that need them: <markup lang=\"java\" >@Inject @Name(\"foo\") @Count(5) private CountDownLatch localLatchFoo; @Inject @Name(\"foo\") @Remote @Count(10) private CountDownLatch remoteLatchFoo; @Inject @Name(\"bar\") @Remote private Semaphore localSemaphoreBar; @Inject @Name(\"bar\") @Remote @Permits(1) private Semaphore remoteSemaphoreBar; inject an instance of a LocalCountDownLatch with the initial count of five inject an instance of a RemoteCountDownLatch with the initial count of ten inject an instance of a LocalSemaphore with zero permits available inject an instance of a RemoteSemaphore with one permit available Once a latch or a semaphore instance is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Latches or Semaphores factory classes. The @Name annotation is optional in both cases, as long as the member name (in the examples above, the field name) can be obtained from the injection point, but is required otherwise (such as when using constructor injection). The @Count annotation specifies the initial latch count, and if omitted will be defaulted to one. The @Permits annotation specifies the number of available permits for a semaphore, and if omitted will be defaulted to zero, which means that the first acquire call will block until another thread releases one or more permits. ",
            "title": "Usage"
        },
        {
            "location": "/docs/core/11_otel",
            "text": " This version of Coherence adds support for OpenTelemetry in addition to OpenTracing as an option for distributed tracing within a Coherence cluster. Coherence does not include any tracing implementation libraries. Therefore, the developer will need to provide the desired tracing runtime. As OpenTracing is no longer maintained, it is recommended that OpenTelemetry be used instead. A minimum of OpenTelemetry for Java version 1.29 or later is recommended. OpenTracing, while now deprecated in Coherence, is still a supported option using the latest OpenTracing 0.33.0. ",
            "title": "OpenTelemetry Support"
        },
        {
            "location": "/docs/core/11_otel",
            "text": " At a minimum, the following OpenTelemetry dependencies (version 1.29 or later) are required in order to enable support in Coherence: opentelemetry-api opentelemetry-context opentelemetry-sdk ",
            "title": "Dependencies"
        },
        {
            "location": "/docs/core/11_otel",
            "text": " It is possible to use a Tracer that has already been created with Coherence by simply ensuring that the Tracer is available via the GlobalOpenTelemtry API included with the OpenTelemetry. When this is the case, Coherence will use the available Tracer, but will not attempt to configure or close the tracer when the cluster member is terminated. ",
            "title": "Externally Managed Tracer"
        },
        {
            "location": "/docs/core/11_otel",
            "text": " If it&#8217;s desirable for Coherence to manage the initialization and lifecycle of the tracing runtime, the following dependency is also required: opentelemetry-sdk-extension-autoconfigure Refer to the documentation for this library for details on how to configure the tracing runtime. This will also require setting the following system property -Dotel.java.global-autoconfigure.enabled=true when starting Coherence (in addition to any other telemetry related properties that may be used) In order for Coherence to generate tracing spans, edit the operational override tangosol-coherence-override.xml file and add a &lt;tracing-config&gt; element with a child &lt;sampling-ratio&gt; element. For example: <markup lang=\"xml\" >&lt;tracing-config&gt; &lt;sampling-ratio&gt;0&lt;/sampling-ratio&gt; &lt;!-- user-initiated tracing --&gt; &lt;/tracing-config&gt; The coherence.tracing.ratio system property is used to specify the tracing sampling ratio instead of using the operational override file. For example: <markup lang=\"bash\" >-Dcoherence.tracing.ratio=0 Tracing operates in three modes: -1 - This value disables tracing. 0 - This value enables user-initiated tracing. This means that Coherence will not initiate tracing on its own and the application should start an outer tracing span, from which Coherence will collect the inner tracing spans. If the outer tracing span is not started, the tracing activity will not be performed. 0.01-1.0 - This range indicates the tracing span being collected. For example, a value of 1.0 will result in all spans being collected, while a value of 0.1 will result in roughly 1 out of every 10 spans being collected. Externally Managed Tracer It is possible to use a Tracer that has already been created with Coherence by simply ensuring that the Tracer is available via the GlobalOpenTelemtry API included with the OpenTelemetry. When this is the case, Coherence will use the available Tracer, but will not attempt to configure or close the tracer when the cluster member is terminated. ",
            "title": "Configuration"
        },
        {
            "location": "/docs/core/11_otel",
            "text": " The following Coherence traced operations may be captured: All operations exposed by the NamedCache API when using partitioned caches. Events processed by event listeners (such as EventInterceptor or MapListener). Persistence operations. CacheStore operations. ExecutorService operations. ",
            "title": "Traced Operations"
        },
        {
            "location": "/docs/core/11_otel",
            "text": " When the sampling ratio is set to 0 , the application will be required to start a tracing span prior to invoking a Coherence operation. <markup lang=\"java\" >Tracer tracer = GlobalOpenTelemetry.getTracer(\"your-tracer\"); Span span = tracer.spanBuilder(\"test\").startSpan(); NamedCache cache = CacheFactory.get(\"some-cache\"); try (Scope scope = span.makeCurrent()) { cache.put(\"a\", \"b\"); cache.get(\"a\"); } finally { span.end(); } ",
            "title": "User Initiated Tracing"
        },
        {
            "location": "/docs/about/04_important",
            "text": " JPMS JDK command line options such as --add-opens , --add-exports and --add-reads of standard JDK modules to com.oracle.coherence module documented in Section Using Java Modules to Build a Coherence Application of Coherence commercial release 14.1.1.2206 are no longer required. A new JPMS requirement is an application module containing Coherence remote lambda(s) must open itself to module com.oracle.coherence so the remote lambda(s) can be resolved to the application&#8217;s lambda(s) during deserialization. ",
            "title": "Concerning Java Platform Module (JPMS) Options"
        },
        {
            "location": "/docs/about/04_important",
            "text": " Coherence CE 24.09.1 requires a minimum of version 17 of the Java Development Kit (JDK). Concerning Java Platform Module (JPMS) Options JPMS JDK command line options such as --add-opens , --add-exports and --add-reads of standard JDK modules to com.oracle.coherence module documented in Section Using Java Modules to Build a Coherence Application of Coherence commercial release 14.1.1.2206 are no longer required. A new JPMS requirement is an application module containing Coherence remote lambda(s) must open itself to module com.oracle.coherence so the remote lambda(s) can be resolved to the application&#8217;s lambda(s) during deserialization. ",
            "title": "Java Development Kit Requirements"
        },
        {
            "location": "/docs/about/04_important",
            "text": " Coherence CE 24.09.1 has migrated to Jakarta EE 9.1 from Java EE 8, importing types in jakarta packages instead of javax packages. The following table describes the mapping of javax packages to jakarta packages and Maven artifacts in Coherence CE 24.09.1. 'javax' Package 'jakarta' Package Maven Group ID Maven Artifact ID Version javax.activation jakarta.activation jakarta.activation jakarta.activation-api 2.0.1 javax.annotation jakarta.annotation jakarta.annotation jakarta.annotation-api 2.0.0 javax.enterprise jakarta.enterprise jakarta.enterprise jakarta.enterprise.cdi-api 3.0.0 javax.inject jakarta.inject jakarta.inject jakarta.inject-api 2.0.1 javax.interceptor jakarta.interceptor jakarta.interceptor jakarta.interceptor-api 2.0.0 javax.json jakarta.json jakarta.json jakarta.json-api 2.0.2 javax.json.bind jakarta.json.bind jakarta.json.bind jakarta.json.bind-api 2.0.0 javax.resource jakarta.resource jakarta.resource jakarta.resource-api 2.0.0 javax.ws.rs jakarta.ws.rs jakarta.ws.rs jakarta.ws.rs-api 3.0.0 javax.xml.bind jakarta.xml.bind jakarta.xml.bind jakarta.xml.bind-api 3.0.1 We&#8217;ve updated our Coherence CE examples to use the jakarta packages where relevant. These examples still hold for older versions of Coherence CE; in these cases developers will need to change from jakarta to javax . In addition to these standard APIs being migrated, we&#8217;ve also updated some of our major dependent libraries that have undertaken this migration as well. Most notably: Library Version Helidon 3.0.0 Jersey 3.0.5 Jackson 2.13.3 Jackson DataBind 2.13.3 Weld 4.0.3.Final JAXB Core 3.0.2 JAXB Implementation 3.0.2 Eclipse MP Config 3.0.1 Eclipse MP Metrics 4.0 Note If using the older jackson-rs-base and jackson-jaxrs-json-provider libraries, it will be necessary to migrate to the 'jakarta' versions. The Maven groupId for the 'jakarta' versions is com.fasterxml.jackson.jakarta.rs with the artifactIds being jackson-jakarta-rs-base and jackson-jakarta-rs-json-provider , respectively. Note If using the older jackson-module-jaxb-annotations library, it will be necessary to migrate to the jakarta versions. The maven groupId for the 'jakarta' version remains the same ( com.fasterxml.jackson.module ), however the artifactId should now be jackson-module-jakarta-xmlbind-annotations ",
            "title": "Jakarta EE 9.1 Compatibility"
        },
        {
            "location": "/docs/about/04_important",
            "text": " The following deprecated packages have been removed from this release: com.oracle.datagrid.persistence com.tangosol.persistence com.oracle.common.base (NOTE: these classes are now in com.oracle.coherence.common.base) ",
            "title": "Deprecated Code Removal"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " What You Will Build What You Need CacheLoader and CacheStore Interface Simple Cache Store Example Simple CacheLoader Simple CacheStore Enable Write Behind File Cache Store Example HSQLDb Cache Store Example Refresh Ahead Expiring HSQLDb Cache Store Example Write Behind HSQLDb Cache Store Example H2 R2DBC Non Blocking Entry Store Example Pluggable Cache Stores Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " This code is written as a number of separate classes representing the different types of cache stores and can be run as a series of Junit tests to show the functionality. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Before we go into some examples, we should review two interfaces that are key. CacheLoader - CacheLoader - defines an interface for loading individual entries via a key or a collection keys from a backend database . CacheStore - CacheStore - defines and interface for storing ior erasing individual entries via a key or collection of keys into a backend database . This interface also extends CacheLoader . In the rest of this document we will refer to CacheLoaders and CacheStores as just \"Cache Stores\" for simplicity. Coherence caches have an in-memory backing map on each storage-enabled member to store cache data. When cache stores are defined against a cache, operations are carried out on the cache stores in addition to the backing map. We will explain this in more detail below. ",
            "title": "CacheLoader and CacheStore Interfaces"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). ",
            "title": "Simple CacheLoader"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll ",
            "title": "Simple CacheStore"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. ",
            "title": "Enable Write Behind"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Before we jump straight into using a \"Database\", we will demonstrate how CacheLoaders and CacheStores work by implementing a mock cache loader that outputs messages to help us understand how this works behind the scenes. Simple CacheLoader The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). Simple CacheStore The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll Enable Write Behind Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. ",
            "title": "Simple Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we will create a file-based cache store which stores values in files with the name of the key under a specific directory. This is to show how a back-end cache store, and the cache interact. This is an example only to see how cache stores work under the covers and will not work with multiple cache servers running and is not recommended for production use. Review the FileCacheStore <markup lang=\"java\" >public class FileCacheStore implements CacheStore&lt;Integer, String&gt; { /** * Base directory off which to store data. */ private final File directory; public FileCacheStore(String directoryName) { if (directoryName == null || directoryName.equals(\"\")) { throw new IllegalArgumentException(\"A directory must be specified\"); } directory = new File(directoryName); if (!directory.isDirectory() || !directory.canWrite()) { throw new IllegalArgumentException(\"Unable to open directory \" + directory); } Logger.info(\"FileCacheStore constructed with directory \" + directory); } @Override public void store(Integer key, String value) { try { BufferedWriter writer = new BufferedWriter(new FileWriter(getFile(directory, key), false)); writer.write(value); writer.close(); } catch (IOException e) { throw new RuntimeException(\"Unable to delete key \" + key, e); } } @Override public void erase(Integer key) { // we ignore result of delete as the key may not exist getFile(directory, key).delete(); } @Override public String load(Integer key) { File file = getFile(directory, key); try { // use Java 1.8 method return Files.readAllLines(file.toPath()).get(0); } catch (IOException e) { return null; // does not exist in cache store } } protected static File getFile(File directory, Integer key) { return new File(directory, key + \".txt\"); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the directory to use Implement the store method by writing the String value to a file in the base directory with the key + \".txt\" as the name Implement the erase method by removing the file with the key + \".txt\" as the name Implement the load method by loading the contents of the file with the key + \".txt\" as the name Review the Cache Configuration file-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.FileCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"test.base.dir\"&gt;/tmp/&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Specify the class that implements the CacheStore interface Pass the directory to the constructor and optionally using a system property to override Uncomment the commented line below to a directory of your choice which must already exist. Comment out the line containg the FileHelper call. <markup lang=\"java\" >baseDirectory = FileHelper.createTempDir(); // baseDirectory = new File(\"/tmp/tim\"); Also comment out the deleteDirectory below so you can look at the contents of the directory. <markup lang=\"java\" >FileHelper.deleteDir(baseDirectory); Inspect the contents of your directory: <markup lang=\"bash\" >$ ls -l /tmp/tim total 64 -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 2.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 3.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 4.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 5.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 6.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 7.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 8.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 9.txt You will see there are 8 files for the 8 entries that were written to the cache store. entry 1.txt was removed so does not exist in the cache store. Create a file 1.txt in the directory and put the text One . Re-run the test. You will notice that the test fails as when the test issues the following assertion as the value was not in the cache, but it was in the cache store and loaded into memory: <markup lang=\"java\" >assertNull(namedMap.get(1)); <markup lang=\"bash\" >org.opentest4j.AssertionFailedError: Expected :null Actual :One ",
            "title": "File Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we will manually create a database backed cache store using a HSQLDb database in embedded mode. This will show how a cache store could interact with a back-end database. In this example we are using an embedded HSQLDb database just as an example and normally the back-end database would be on a physically separate machine and not in-memory. In this example we are storing a simple Customer class in our cache and cache-store. Continue below to review the HSQLDbCacheStore class. Review the HSQLDbCacheStore Specify the class that implements the CacheStore interface <markup lang=\"java\" >public class HSQLDbCacheStore extends Base implements CacheStore&lt;Integer, Customer&gt; { Construct the CacheStore passing the cache name to the constructor <markup lang=\"java\" >/** * Construct a cache store. * * @param cacheName cache name * * @throws SQLException if any SQL errors */ public HSQLDbCacheStore(String cacheName) throws SQLException { this.tableName = cacheName; dbConn = DriverManager.getConnection(DB_URL); Logger.info(\"HSQLDbCacheStore constructed with cache Name \" + cacheName); } Implement the load method by selecting the customer from the database based upon the primary key of id <markup lang=\"java\" >@Override public Customer load(Integer key) { String query = \"SELECT id, name, address, creditLimit FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; ResultSet resultSet = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); resultSet = statement.executeQuery(); return resultSet.next() ? createFromResultSet(resultSet) : null; } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(resultSet); close(statement); } } Implement the store method by calling storeInternal and then issuing a commit. <markup lang=\"java\" >@Override public void store(Integer key, Customer customer) { try { storeInternal(key, customer); dbConn.commit(); } catch (Exception e) { throw ensureRuntimeException(e); } } Internal implementation of store to be re-used by store and storeAll to insert or update the record in the database <markup lang=\"java\" >/** * Store a {@link Customer} object using the id. This method does not issue a * commit so that either the store or storeAll method can reuse this. * * @param key customer id * @param customer {@link Customer} object */ private void storeInternal(Integer key, Customer customer) { // the following is very inefficient; it is recommended to use DB // specific functionality that is, REPLACE for MySQL or MERGE for Oracle String query = load(key) != null ? \"UPDATE \" + tableName + \" SET name = ?, address = ?, creditLimit = ? where id = ?\" : \"INSERT INTO \" + tableName + \" (name, address, creditLimit, id) VALUES(?, ?, ?, ?)\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setString(1, customer.getName()); statement.setString(2, customer.getAddress()); statement.setInt(3, customer.getCreditLimit()); statement.setInt(4, customer.getId()); statement.execute(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Implement the storeAll method <markup lang=\"java\" >@Override public void storeAll(Map&lt;? extends Integer, ? extends Customer&gt; mapEntries) { try { for (Customer customer : mapEntries.values()) { storeInternal(customer.getId(), customer); } dbConn.commit(); Logger.info(\"Ran storeAll on \" + mapEntries.size() + \" entries\"); } catch (Exception e) { try { dbConn.rollback(); } catch (SQLException ignore) { } throw ensureRuntimeException(e); } } The storeAll method will use a single transaction to insert/update all values. This method will be used internally for write-behind only. Implement the erase method by removing the entry from the database. <markup lang=\"java\" >@Override public void erase(Integer key) { String query = \"DELETE FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); statement.execute(); dbConn.commit(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Review the Cache Configuration Review the Cache Configuration hsqldb-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Customer&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt; com.oracle.coherence.guides.cachestores.HSQLDbCacheStore &lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;!-- Normally the assumption is the cache name will be the same as the table name but in this example we are hard coding the table name --&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;Customer&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;{write-delay 0s}&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for Customer cache to the hsqldb-cache-loader scheme Cache mapping for CustomerExpiring cache to the hsqldb-cache-loader scheme (see next section) Set the expiry to 20 seconds for the expiring cache Override the refresh-ahead factor for the expiring cache Specify the class that implements the CacheStore interface Specify the cache name Run the Unit Test Next we will run the HSqlDbCacheStoreTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"Customer\"); reloadCustomersDB(); } @Test public void testHSqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue an initial get on the NamedMap and validate the object is read from the cache store. <markup lang=\"java\" >long start = System.nanoTime(); // issue a get and it will load the existing customer Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, namedMap.size()); assertNotNull(customer); assertEquals(1, customer.getId()); assertEquals(\"Customer 1\", customer.getName()); You will see a message similar to the following indicating the time to retrieve a NamedMap entry that is not in the cache. (thread=main, member=1): Time for read-through 17.023 ms Issue a second get, the entry will be retrieved directly from memory and not the cache store. <markup lang=\"java\" >// issue a get again and it should be quicker start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"no read-through\")); You will see a message similar to the following indicating the time to retrieve a NamedMap entry is significantly quicker. (thread=main, member=1): Time for no read-through 0.889 ms Remove and entry from the NamedMap and the value should be removed from the underlying store. <markup lang=\"java\" >// remove a customer number 1 namedMap.remove(1); // we should have one less customer in the database assertEquals(MAX_CUSTOMERS - 1, getCustomerDBCount()); assertNull(namedMap.get(1)); // customer should not exist in DB assertNull(getCustomerFromDB(1)); Issue a get for another customer and then update the customer details. <markup lang=\"java\" >// Load customer 2 Customer customer2 = namedMap.get(2); assertNotNull(customer2); // update customer 2 with \"New Address\" namedMap.compute(2, (k, v)-&gt;{ v.setAddress(\"New Address\"); return v; }); // customer should have new address in cache and DB assertEquals(\"New Address\", namedMap.get(2).getAddress()); assertEquals(\"New Address\", getCustomerFromDB(2).getAddress()); Add a new customer and ensure it is created in the database. Then remove the same customer. <markup lang=\"java\" >// add a new customer 1010 namedMap.put(101, new Customer(101, \"Customer Name 101\", \"Customer address 101\", 20000)); assertTrue(namedMap.containsKey(101)); assertEquals(\"Customer address 101\", getCustomerFromDB(101).getAddress()); namedMap.remove(101); assertFalse(namedMap.containsKey(101)); assertNull(getCustomerFromDB(101)); Clear the NamedMap and show how to preload the data from the cache store. <markup lang=\"java\" >// clean the cache and reset the database namedMap.clear(); reloadCustomersDB(); assertEquals(0, namedMap.size()); // demonstrate loading the cache from the current contents of the DB // this can be done many ways but for this exercise you could fetch all the // customer id' from the DB but as we know there are 1..100 we can pretend we have. Set&lt;Integer&gt; keySet = IntStream.rangeClosed(1, 100).boxed().collect(Collectors.toSet()); namedMap.invokeAll(keySet, new PreloadRequest&lt;&gt;()); // cache should be fully primed assertEquals(MAX_CUSTOMERS, namedMap.size()); ",
            "title": "HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we use the CustomerExpiring cache which will expire data after 20 seconds and also has a refresh-ahead-factor of 0.5 meaning that if the cache is accessed after 10 seconds then an asynchronous refresh-ahead will be performed to speed up the next access to the data. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerExpiring cache passing in parameters to the caching-scheme to override expiry and refresh ahead values. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; The local-scheme uses the back-expiry parameter passed in: <markup lang=\"xml\" >&lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; The read-write-backing-map-scheme uses the refresh-ahead-factor parameter passed in: <markup lang=\"xml\" >&lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreExpiringTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerExpiring\"); reloadCustomersDB(); } @Test public void testHSQLDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue a get for customer 1 and log the time to load <markup lang=\"java\" >// expiry delay is setup to 20s for the cache and refresh ahead is 0.5 which // means that after 10s if the entry is read the old value is returned but after which a // refresh is done which means that subsequents reads will be fast as the new value is already present long start = System.nanoTime(); Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, customer.getId()); Notice the initial read through time similar to the following in the log: (thread=main, member=1): Time for read-through 19.129 ms Update the credit limit to 10000 in the database for customer 1 and ensure that after 11 seconds the value is still 5000 in the NamedMap. <markup lang=\"java\" >// update the database updateCustomerCreditLimitInDB(1, 10000); // sleep for 11 seconds get the cache entry, we should still get the original value Base.sleep(11000L); assertEquals(5000, namedMap.get(1).getCreditLimit()); The get within the 10 seconds (20s * 0.5), will cause an asynchronous refresh-ahead. Wait for 10 seconds and then retrieve the customer object which has been updated. <markup lang=\"java\" >// wait for another 10 seconds and the refresh-ahead should have completed Base.sleep(10000L); start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"after refresh-ahead\")); Notice the time to retrieve the entry is significantly reduced: (thread=main, member=1): Time for after refresh-ahead 1.116 ms ",
            "title": "Refresh Ahead HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this HSQLDb cache store example, we use the CustomerWriteBehind cache which has a write delay of 10 seconds. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerWriteBehind cache passing in parameters to the caching-scheme to override write-delay value. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreWriteBehindTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerWriteBehind\"); } @Test public void testHsqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain no customers assertEquals(0, getCustomerDBCount()); Insert 10 customers using an efficient putAll operation and confirm the data is not yet in the cache. <markup lang=\"java\" >// add 10 customers Map&lt;Integer, Customer&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 100; i++) { map.put(i, new Customer(i, \"Name \" + i, \"Address \" + i, i * 1000)); } namedMap.putAll(map); // initial check of the database should return 0 as we have write-delay set assertEquals(0, getCustomerDBCount()); Wait till after the write-delay has passed and confirm that the customers are in the database. <markup lang=\"java\" >// sleep for 15 seconds and the database should be populated as write-delay has elapsed Base.sleep(15000L); // Issuing Eventually assertThat in case of heavily loaded machine Eventually.assertThat(invoking(this).getCustomerDBCount(), is(100)); You will notice that you should see messages indicating 100 entries have been written. You may also see multiple writes as the data will be added in different partitions. load. &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 3 entries &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 97 entries OR &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 10 entries ",
            "title": "Write Behind HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this H2 R2DBC cache store example, we use the H2Person cache which implements the NonBlockingEntryStore for non-blocking APIs and access to entries in their serialized ( BinaryEntry ) form. Review the Cache Configuration The h2r2dbc-entry-store-cache-config.xml below shows the H2Person cache specifying the class name of the NonBlockingEntryStore implementation. <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;H2Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.H2R2DBCEntryStore&lt;/class-name&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Run the Unit Test Next we will run the H2R2DBCEntryStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { createTable(); startupCoherence(\"h2r2dbc-entry-store-cache-config.xml\"); } /** * Performs some cache manipulations. */ @Test public void testNonBlockingEntryStore() { NamedMap&lt;Long, Person&gt; namedMap = getSession() .getMap(\"H2Person\", TypeAssertion.withTypes(Long.class, Person.class)); Person person1 = namedMap.get(Long.valueOf(101)); assertEquals(\"Robert\", person1.getFirstname()); Insert 1 person using a put operation and confirm the data is in the cache. <markup lang=\"java\" >Person person2 = new Person(Long.valueOf(102), 40, \"Tony\", \"Soprano\"); namedMap.put(Long.valueOf(102), person2); Person person3 = namedMap.get(Long.valueOf(102)); assertEquals(\"Tony\", person3.getFirstname()); Delete a couple records and verify the state of the cache. <markup lang=\"java\" >namedMap.remove(Long.valueOf(101)); namedMap.remove(Long.valueOf(102)); assertEquals(null, namedMap.get(Long.valueOf(101))); assertEquals(null, namedMap.get(Long.valueOf(102))); Insert 10 persons using a putAll operation and confirm the data is in the cache. The actual database operations take place in parallel.s <markup lang=\"java\" >Map&lt;Long, Person&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 10; i++) { map.put(Long.valueOf(i), new Person(Long.valueOf(i), 20 + i, \"firstname\" + i, \"lastname\" + i)); } namedMap.putAll(map); Person person5 = namedMap.get(Long.valueOf(5)); assertEquals(\"firstname5\", person5.getFirstname()); assertEquals(10, namedMap.size()); You should see messages indicating activity on the store side: 2021-06-29 15:01:36.365/5.583 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.495/5.713 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore store 2021-06-29 15:01:36.501/5.720 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.504/5.722 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.507/5.726 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.508/5.727 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.509/5.728 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.512/5.730 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Could not find row for key: 101 2021-06-29 15:01:36.515/5.734 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore storeAll ",
            "title": "H2 R2DBC Non Blocking Entry Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " A cache store is an application-specific adapter used to connect a cache to an underlying data source. The cache store implementation accesses the data source by using a data access mechanism (for example, Hibernate, Toplink, JPA, application-specific JDBC calls, etc). The cache store understands how to build a Java object using data retrieved from the data source, map and write an object to the data source, and erase an object from the data source. In this example we are going to use a Hibernate cache store from the Coherence Hibernate OpenSource Project . Review the Configuration Review the Cache Configuration hibernate-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.hibernate.cachestore.HibernateCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;com.oracle.coherence.guides.cachestores.{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the distributed-hibernate scheme Specify the HibernateCacheStore scheme Pass the cache name using the in-built macro to the constructor In this case we do not have to write any code for our cache store as the Hibernate cache store understands the entity mapping and will deal with this. Review the Hibernate Configuration <markup lang=\"xml\" >&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;!-- Database connection settings --&gt; &lt;property name=\"connection.driver_class\"&gt;org.hsqldb.jdbcDriver&lt;/property&gt; &lt;property name=\"connection.url\"&gt;jdbc:hsqldb:mem:test&lt;/property&gt; &lt;property name=\"connection.username\"&gt;sa&lt;/property&gt; &lt;property name=\"connection.password\"&gt;&lt;/property&gt; &lt;!-- JDBC connection pool (use the built-in) --&gt; &lt;property name=\"connection.pool_size\"&gt;1&lt;/property&gt; &lt;!-- SQL dialect --&gt; &lt;property name=\"dialect\"&gt;org.hibernate.dialect.HSQLDialect&lt;/property&gt; &lt;!-- Enable Hibernate's automatic session context management --&gt; &lt;property name=\"current_session_context_class\"&gt;thread&lt;/property&gt; &lt;!-- Echo all executed SQL to stdout --&gt; &lt;property name=\"show_sql\"&gt;true&lt;/property&gt; &lt;!-- Drop and re-create the database schema on startup --&gt; &lt;property name=\"hbm2ddl.auto\"&gt;update&lt;/property&gt; &lt;mapping resource=\"Person.hbm.xml\"/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; - Specifies the Person mapping Review the Hibernate Mapping <markup lang=\"xml\" >&lt;hibernate-mapping package=\"com.oracle.coherence.guides.cachestores\"&gt; &lt;class name=\"Person\" table=\"PERSON\"&gt; &lt;id name=\"id\" column=\"id\"&gt; &lt;generator class=\"native\"/&gt; &lt;/id&gt; &lt;property name=\"age\"/&gt; &lt;property name=\"firstname\"/&gt; &lt;property name=\"lastname\"/&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt; Specifies the Person mapping Run the Unit Test Next we will run the HibernateCacheStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { startupCoherence(\"hibernate-cache-store-cache-config.xml\"); connection = DriverManager.getConnection(\"jdbc:hsqldb:mem:test\"); } Create a new Person and put it into the NamedMap. <markup lang=\"java\" >Person person1 = new Person(1L, 50, \"Tom\", \"Jones\"); namedMap.put(person1.getId(), person1); assertEquals(1, namedMap.size()); Retrieve the Person from the database and validate that the person from the database and cache are equal. <markup lang=\"java\" >Person person2 = getPersonFromDB(1L); person1 = namedMap.get(1L); assertNotNull(person2); assertEquals(person2, person1); Update the persons age in the NamedMap and confirm it is saved in the database <markup lang=\"java\" >person2.setAge(100); namedMap.put(person2.getId(), person2); Person person3 = getPersonFromDB(1L); assertNotNull(person2); assertEquals(person3.getAge(), 100); Remove person 1 and ensure they are also removed from the database. <markup lang=\"java\" >namedMap.remove(1L); Person person4 = getPersonFromDB(1L); assertNull(person4); ",
            "title": "Pluggable Cache Stores"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " You have seen how to use and configure Cache Stores within Coherence. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Caching Data Stores Coherence Hibernate OpenSource Project ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " This guide walks you through how to use and configure Cache Stores within Coherence. Coherence supports transparent read/write caching of any data source, including databases, web services, packaged applications and file systems; however, databases are the most common use case. As shorthand, \"database\" is used to describe any back-end data source. Effective caches must support both intensive read-only and read/write operations, and for read/write operations, the cache and database must be kept fully synchronized. To accomplish caching of data sources, Coherence supports Read-Through, Write-Through, Refresh-Ahead and Write-Behind caching. Coherence also supports BinaryEntryStore which provides access to the serialized form of entries for data sources capable of manipulating those. A variant of BinaryEntryStore is the NonBlockingEntryStore which, besides providing access to entries in their BinaryEntry form, integrates with data sources with non-blocking APIs such as R2DBC or Kafka. See the Coherence Documentation for detailed information on Cache Stores. Table of Contents What You Will Build What You Need CacheLoader and CacheStore Interface Simple Cache Store Example Simple CacheLoader Simple CacheStore Enable Write Behind File Cache Store Example HSQLDb Cache Store Example Refresh Ahead Expiring HSQLDb Cache Store Example Write Behind HSQLDb Cache Store Example H2 R2DBC Non Blocking Entry Store Example Pluggable Cache Stores Summary See Also What You Will Build This code is written as a number of separate classes representing the different types of cache stores and can be run as a series of Junit tests to show the functionality. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. CacheLoader and CacheStore Interfaces Before we go into some examples, we should review two interfaces that are key. CacheLoader - CacheLoader - defines an interface for loading individual entries via a key or a collection keys from a backend database . CacheStore - CacheStore - defines and interface for storing ior erasing individual entries via a key or collection of keys into a backend database . This interface also extends CacheLoader . In the rest of this document we will refer to CacheLoaders and CacheStores as just \"Cache Stores\" for simplicity. Coherence caches have an in-memory backing map on each storage-enabled member to store cache data. When cache stores are defined against a cache, operations are carried out on the cache stores in addition to the backing map. We will explain this in more detail below. Simple Cache Store Example Before we jump straight into using a \"Database\", we will demonstrate how CacheLoaders and CacheStores work by implementing a mock cache loader that outputs messages to help us understand how this works behind the scenes. Simple CacheLoader The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). Simple CacheStore The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll Enable Write Behind Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. File Cache Store Example In this next example, we will create a file-based cache store which stores values in files with the name of the key under a specific directory. This is to show how a back-end cache store, and the cache interact. This is an example only to see how cache stores work under the covers and will not work with multiple cache servers running and is not recommended for production use. Review the FileCacheStore <markup lang=\"java\" >public class FileCacheStore implements CacheStore&lt;Integer, String&gt; { /** * Base directory off which to store data. */ private final File directory; public FileCacheStore(String directoryName) { if (directoryName == null || directoryName.equals(\"\")) { throw new IllegalArgumentException(\"A directory must be specified\"); } directory = new File(directoryName); if (!directory.isDirectory() || !directory.canWrite()) { throw new IllegalArgumentException(\"Unable to open directory \" + directory); } Logger.info(\"FileCacheStore constructed with directory \" + directory); } @Override public void store(Integer key, String value) { try { BufferedWriter writer = new BufferedWriter(new FileWriter(getFile(directory, key), false)); writer.write(value); writer.close(); } catch (IOException e) { throw new RuntimeException(\"Unable to delete key \" + key, e); } } @Override public void erase(Integer key) { // we ignore result of delete as the key may not exist getFile(directory, key).delete(); } @Override public String load(Integer key) { File file = getFile(directory, key); try { // use Java 1.8 method return Files.readAllLines(file.toPath()).get(0); } catch (IOException e) { return null; // does not exist in cache store } } protected static File getFile(File directory, Integer key) { return new File(directory, key + \".txt\"); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the directory to use Implement the store method by writing the String value to a file in the base directory with the key + \".txt\" as the name Implement the erase method by removing the file with the key + \".txt\" as the name Implement the load method by loading the contents of the file with the key + \".txt\" as the name Review the Cache Configuration file-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.FileCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"test.base.dir\"&gt;/tmp/&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Specify the class that implements the CacheStore interface Pass the directory to the constructor and optionally using a system property to override Uncomment the commented line below to a directory of your choice which must already exist. Comment out the line containg the FileHelper call. <markup lang=\"java\" >baseDirectory = FileHelper.createTempDir(); // baseDirectory = new File(\"/tmp/tim\"); Also comment out the deleteDirectory below so you can look at the contents of the directory. <markup lang=\"java\" >FileHelper.deleteDir(baseDirectory); Inspect the contents of your directory: <markup lang=\"bash\" >$ ls -l /tmp/tim total 64 -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 2.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 3.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 4.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 5.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 6.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 7.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 8.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 9.txt You will see there are 8 files for the 8 entries that were written to the cache store. entry 1.txt was removed so does not exist in the cache store. Create a file 1.txt in the directory and put the text One . Re-run the test. You will notice that the test fails as when the test issues the following assertion as the value was not in the cache, but it was in the cache store and loaded into memory: <markup lang=\"java\" >assertNull(namedMap.get(1)); <markup lang=\"bash\" >org.opentest4j.AssertionFailedError: Expected :null Actual :One HSQLDb Cache Store Example In this next example, we will manually create a database backed cache store using a HSQLDb database in embedded mode. This will show how a cache store could interact with a back-end database. In this example we are using an embedded HSQLDb database just as an example and normally the back-end database would be on a physically separate machine and not in-memory. In this example we are storing a simple Customer class in our cache and cache-store. Continue below to review the HSQLDbCacheStore class. Review the HSQLDbCacheStore Specify the class that implements the CacheStore interface <markup lang=\"java\" >public class HSQLDbCacheStore extends Base implements CacheStore&lt;Integer, Customer&gt; { Construct the CacheStore passing the cache name to the constructor <markup lang=\"java\" >/** * Construct a cache store. * * @param cacheName cache name * * @throws SQLException if any SQL errors */ public HSQLDbCacheStore(String cacheName) throws SQLException { this.tableName = cacheName; dbConn = DriverManager.getConnection(DB_URL); Logger.info(\"HSQLDbCacheStore constructed with cache Name \" + cacheName); } Implement the load method by selecting the customer from the database based upon the primary key of id <markup lang=\"java\" >@Override public Customer load(Integer key) { String query = \"SELECT id, name, address, creditLimit FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; ResultSet resultSet = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); resultSet = statement.executeQuery(); return resultSet.next() ? createFromResultSet(resultSet) : null; } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(resultSet); close(statement); } } Implement the store method by calling storeInternal and then issuing a commit. <markup lang=\"java\" >@Override public void store(Integer key, Customer customer) { try { storeInternal(key, customer); dbConn.commit(); } catch (Exception e) { throw ensureRuntimeException(e); } } Internal implementation of store to be re-used by store and storeAll to insert or update the record in the database <markup lang=\"java\" >/** * Store a {@link Customer} object using the id. This method does not issue a * commit so that either the store or storeAll method can reuse this. * * @param key customer id * @param customer {@link Customer} object */ private void storeInternal(Integer key, Customer customer) { // the following is very inefficient; it is recommended to use DB // specific functionality that is, REPLACE for MySQL or MERGE for Oracle String query = load(key) != null ? \"UPDATE \" + tableName + \" SET name = ?, address = ?, creditLimit = ? where id = ?\" : \"INSERT INTO \" + tableName + \" (name, address, creditLimit, id) VALUES(?, ?, ?, ?)\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setString(1, customer.getName()); statement.setString(2, customer.getAddress()); statement.setInt(3, customer.getCreditLimit()); statement.setInt(4, customer.getId()); statement.execute(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Implement the storeAll method <markup lang=\"java\" >@Override public void storeAll(Map&lt;? extends Integer, ? extends Customer&gt; mapEntries) { try { for (Customer customer : mapEntries.values()) { storeInternal(customer.getId(), customer); } dbConn.commit(); Logger.info(\"Ran storeAll on \" + mapEntries.size() + \" entries\"); } catch (Exception e) { try { dbConn.rollback(); } catch (SQLException ignore) { } throw ensureRuntimeException(e); } } The storeAll method will use a single transaction to insert/update all values. This method will be used internally for write-behind only. Implement the erase method by removing the entry from the database. <markup lang=\"java\" >@Override public void erase(Integer key) { String query = \"DELETE FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); statement.execute(); dbConn.commit(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Review the Cache Configuration Review the Cache Configuration hsqldb-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Customer&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt; com.oracle.coherence.guides.cachestores.HSQLDbCacheStore &lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;!-- Normally the assumption is the cache name will be the same as the table name but in this example we are hard coding the table name --&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;Customer&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;{write-delay 0s}&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for Customer cache to the hsqldb-cache-loader scheme Cache mapping for CustomerExpiring cache to the hsqldb-cache-loader scheme (see next section) Set the expiry to 20 seconds for the expiring cache Override the refresh-ahead factor for the expiring cache Specify the class that implements the CacheStore interface Specify the cache name Run the Unit Test Next we will run the HSqlDbCacheStoreTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"Customer\"); reloadCustomersDB(); } @Test public void testHSqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue an initial get on the NamedMap and validate the object is read from the cache store. <markup lang=\"java\" >long start = System.nanoTime(); // issue a get and it will load the existing customer Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, namedMap.size()); assertNotNull(customer); assertEquals(1, customer.getId()); assertEquals(\"Customer 1\", customer.getName()); You will see a message similar to the following indicating the time to retrieve a NamedMap entry that is not in the cache. (thread=main, member=1): Time for read-through 17.023 ms Issue a second get, the entry will be retrieved directly from memory and not the cache store. <markup lang=\"java\" >// issue a get again and it should be quicker start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"no read-through\")); You will see a message similar to the following indicating the time to retrieve a NamedMap entry is significantly quicker. (thread=main, member=1): Time for no read-through 0.889 ms Remove and entry from the NamedMap and the value should be removed from the underlying store. <markup lang=\"java\" >// remove a customer number 1 namedMap.remove(1); // we should have one less customer in the database assertEquals(MAX_CUSTOMERS - 1, getCustomerDBCount()); assertNull(namedMap.get(1)); // customer should not exist in DB assertNull(getCustomerFromDB(1)); Issue a get for another customer and then update the customer details. <markup lang=\"java\" >// Load customer 2 Customer customer2 = namedMap.get(2); assertNotNull(customer2); // update customer 2 with \"New Address\" namedMap.compute(2, (k, v)-&gt;{ v.setAddress(\"New Address\"); return v; }); // customer should have new address in cache and DB assertEquals(\"New Address\", namedMap.get(2).getAddress()); assertEquals(\"New Address\", getCustomerFromDB(2).getAddress()); Add a new customer and ensure it is created in the database. Then remove the same customer. <markup lang=\"java\" >// add a new customer 1010 namedMap.put(101, new Customer(101, \"Customer Name 101\", \"Customer address 101\", 20000)); assertTrue(namedMap.containsKey(101)); assertEquals(\"Customer address 101\", getCustomerFromDB(101).getAddress()); namedMap.remove(101); assertFalse(namedMap.containsKey(101)); assertNull(getCustomerFromDB(101)); Clear the NamedMap and show how to preload the data from the cache store. <markup lang=\"java\" >// clean the cache and reset the database namedMap.clear(); reloadCustomersDB(); assertEquals(0, namedMap.size()); // demonstrate loading the cache from the current contents of the DB // this can be done many ways but for this exercise you could fetch all the // customer id' from the DB but as we know there are 1..100 we can pretend we have. Set&lt;Integer&gt; keySet = IntStream.rangeClosed(1, 100).boxed().collect(Collectors.toSet()); namedMap.invokeAll(keySet, new PreloadRequest&lt;&gt;()); // cache should be fully primed assertEquals(MAX_CUSTOMERS, namedMap.size()); Refresh Ahead HSQLDb Cache Store Example In this next example, we use the CustomerExpiring cache which will expire data after 20 seconds and also has a refresh-ahead-factor of 0.5 meaning that if the cache is accessed after 10 seconds then an asynchronous refresh-ahead will be performed to speed up the next access to the data. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerExpiring cache passing in parameters to the caching-scheme to override expiry and refresh ahead values. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; The local-scheme uses the back-expiry parameter passed in: <markup lang=\"xml\" >&lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; The read-write-backing-map-scheme uses the refresh-ahead-factor parameter passed in: <markup lang=\"xml\" >&lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreExpiringTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerExpiring\"); reloadCustomersDB(); } @Test public void testHSQLDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue a get for customer 1 and log the time to load <markup lang=\"java\" >// expiry delay is setup to 20s for the cache and refresh ahead is 0.5 which // means that after 10s if the entry is read the old value is returned but after which a // refresh is done which means that subsequents reads will be fast as the new value is already present long start = System.nanoTime(); Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, customer.getId()); Notice the initial read through time similar to the following in the log: (thread=main, member=1): Time for read-through 19.129 ms Update the credit limit to 10000 in the database for customer 1 and ensure that after 11 seconds the value is still 5000 in the NamedMap. <markup lang=\"java\" >// update the database updateCustomerCreditLimitInDB(1, 10000); // sleep for 11 seconds get the cache entry, we should still get the original value Base.sleep(11000L); assertEquals(5000, namedMap.get(1).getCreditLimit()); The get within the 10 seconds (20s * 0.5), will cause an asynchronous refresh-ahead. Wait for 10 seconds and then retrieve the customer object which has been updated. <markup lang=\"java\" >// wait for another 10 seconds and the refresh-ahead should have completed Base.sleep(10000L); start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"after refresh-ahead\")); Notice the time to retrieve the entry is significantly reduced: (thread=main, member=1): Time for after refresh-ahead 1.116 ms Write Behind HSQLDb Cache Store Example In this HSQLDb cache store example, we use the CustomerWriteBehind cache which has a write delay of 10 seconds. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerWriteBehind cache passing in parameters to the caching-scheme to override write-delay value. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreWriteBehindTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerWriteBehind\"); } @Test public void testHsqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain no customers assertEquals(0, getCustomerDBCount()); Insert 10 customers using an efficient putAll operation and confirm the data is not yet in the cache. <markup lang=\"java\" >// add 10 customers Map&lt;Integer, Customer&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 100; i++) { map.put(i, new Customer(i, \"Name \" + i, \"Address \" + i, i * 1000)); } namedMap.putAll(map); // initial check of the database should return 0 as we have write-delay set assertEquals(0, getCustomerDBCount()); Wait till after the write-delay has passed and confirm that the customers are in the database. <markup lang=\"java\" >// sleep for 15 seconds and the database should be populated as write-delay has elapsed Base.sleep(15000L); // Issuing Eventually assertThat in case of heavily loaded machine Eventually.assertThat(invoking(this).getCustomerDBCount(), is(100)); You will notice that you should see messages indicating 100 entries have been written. You may also see multiple writes as the data will be added in different partitions. load. &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 3 entries &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 97 entries OR &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 10 entries H2 R2DBC Non Blocking Entry Store Example In this H2 R2DBC cache store example, we use the H2Person cache which implements the NonBlockingEntryStore for non-blocking APIs and access to entries in their serialized ( BinaryEntry ) form. Review the Cache Configuration The h2r2dbc-entry-store-cache-config.xml below shows the H2Person cache specifying the class name of the NonBlockingEntryStore implementation. <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;H2Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.H2R2DBCEntryStore&lt;/class-name&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Run the Unit Test Next we will run the H2R2DBCEntryStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { createTable(); startupCoherence(\"h2r2dbc-entry-store-cache-config.xml\"); } /** * Performs some cache manipulations. */ @Test public void testNonBlockingEntryStore() { NamedMap&lt;Long, Person&gt; namedMap = getSession() .getMap(\"H2Person\", TypeAssertion.withTypes(Long.class, Person.class)); Person person1 = namedMap.get(Long.valueOf(101)); assertEquals(\"Robert\", person1.getFirstname()); Insert 1 person using a put operation and confirm the data is in the cache. <markup lang=\"java\" >Person person2 = new Person(Long.valueOf(102), 40, \"Tony\", \"Soprano\"); namedMap.put(Long.valueOf(102), person2); Person person3 = namedMap.get(Long.valueOf(102)); assertEquals(\"Tony\", person3.getFirstname()); Delete a couple records and verify the state of the cache. <markup lang=\"java\" >namedMap.remove(Long.valueOf(101)); namedMap.remove(Long.valueOf(102)); assertEquals(null, namedMap.get(Long.valueOf(101))); assertEquals(null, namedMap.get(Long.valueOf(102))); Insert 10 persons using a putAll operation and confirm the data is in the cache. The actual database operations take place in parallel.s <markup lang=\"java\" >Map&lt;Long, Person&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 10; i++) { map.put(Long.valueOf(i), new Person(Long.valueOf(i), 20 + i, \"firstname\" + i, \"lastname\" + i)); } namedMap.putAll(map); Person person5 = namedMap.get(Long.valueOf(5)); assertEquals(\"firstname5\", person5.getFirstname()); assertEquals(10, namedMap.size()); You should see messages indicating activity on the store side: 2021-06-29 15:01:36.365/5.583 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.495/5.713 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore store 2021-06-29 15:01:36.501/5.720 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.504/5.722 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.507/5.726 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.508/5.727 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.509/5.728 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.512/5.730 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Could not find row for key: 101 2021-06-29 15:01:36.515/5.734 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore storeAll Pluggable Cache Stores A cache store is an application-specific adapter used to connect a cache to an underlying data source. The cache store implementation accesses the data source by using a data access mechanism (for example, Hibernate, Toplink, JPA, application-specific JDBC calls, etc). The cache store understands how to build a Java object using data retrieved from the data source, map and write an object to the data source, and erase an object from the data source. In this example we are going to use a Hibernate cache store from the Coherence Hibernate OpenSource Project . Review the Configuration Review the Cache Configuration hibernate-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.hibernate.cachestore.HibernateCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;com.oracle.coherence.guides.cachestores.{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the distributed-hibernate scheme Specify the HibernateCacheStore scheme Pass the cache name using the in-built macro to the constructor In this case we do not have to write any code for our cache store as the Hibernate cache store understands the entity mapping and will deal with this. Review the Hibernate Configuration <markup lang=\"xml\" >&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;!-- Database connection settings --&gt; &lt;property name=\"connection.driver_class\"&gt;org.hsqldb.jdbcDriver&lt;/property&gt; &lt;property name=\"connection.url\"&gt;jdbc:hsqldb:mem:test&lt;/property&gt; &lt;property name=\"connection.username\"&gt;sa&lt;/property&gt; &lt;property name=\"connection.password\"&gt;&lt;/property&gt; &lt;!-- JDBC connection pool (use the built-in) --&gt; &lt;property name=\"connection.pool_size\"&gt;1&lt;/property&gt; &lt;!-- SQL dialect --&gt; &lt;property name=\"dialect\"&gt;org.hibernate.dialect.HSQLDialect&lt;/property&gt; &lt;!-- Enable Hibernate's automatic session context management --&gt; &lt;property name=\"current_session_context_class\"&gt;thread&lt;/property&gt; &lt;!-- Echo all executed SQL to stdout --&gt; &lt;property name=\"show_sql\"&gt;true&lt;/property&gt; &lt;!-- Drop and re-create the database schema on startup --&gt; &lt;property name=\"hbm2ddl.auto\"&gt;update&lt;/property&gt; &lt;mapping resource=\"Person.hbm.xml\"/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; - Specifies the Person mapping Review the Hibernate Mapping <markup lang=\"xml\" >&lt;hibernate-mapping package=\"com.oracle.coherence.guides.cachestores\"&gt; &lt;class name=\"Person\" table=\"PERSON\"&gt; &lt;id name=\"id\" column=\"id\"&gt; &lt;generator class=\"native\"/&gt; &lt;/id&gt; &lt;property name=\"age\"/&gt; &lt;property name=\"firstname\"/&gt; &lt;property name=\"lastname\"/&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt; Specifies the Person mapping Run the Unit Test Next we will run the HibernateCacheStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { startupCoherence(\"hibernate-cache-store-cache-config.xml\"); connection = DriverManager.getConnection(\"jdbc:hsqldb:mem:test\"); } Create a new Person and put it into the NamedMap. <markup lang=\"java\" >Person person1 = new Person(1L, 50, \"Tom\", \"Jones\"); namedMap.put(person1.getId(), person1); assertEquals(1, namedMap.size()); Retrieve the Person from the database and validate that the person from the database and cache are equal. <markup lang=\"java\" >Person person2 = getPersonFromDB(1L); person1 = namedMap.get(1L); assertNotNull(person2); assertEquals(person2, person1); Update the persons age in the NamedMap and confirm it is saved in the database <markup lang=\"java\" >person2.setAge(100); namedMap.put(person2.getId(), person2); Person person3 = getPersonFromDB(1L); assertNotNull(person2); assertEquals(person3.getAge(), 100); Remove person 1 and ensure they are also removed from the database. <markup lang=\"java\" >namedMap.remove(1L); Person person4 = getPersonFromDB(1L); assertNull(person4); Summary You have seen how to use and configure Cache Stores within Coherence. See Also Caching Data Stores Coherence Hibernate OpenSource Project ",
            "title": "Cache Stores"
        },
        {
            "location": "/examples/guides/000-overview",
            "text": " These simple guides are designed to be a quick hands-on introduction to a specific feature of Coherence. In most cases they require nothing more than a Coherence jar and an IDE (or a text editor if you&#8217;re really old-school). Guides are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. Bootstrap Coherence This guide walks you through various methods to configure and bootstrap a Coherence instance. Coherence*Extend Provides a guide for clients to connect to a Coherence Cluster via Coherence*Extend. Put Get and Remove This guide walks you through basic CRUD put , get , and remove operations on a NamedMap . Querying Caches This guide walks you through the basic concepts of querying Coherence caches. Built-in Aggregators This guide walks you through how to use built-in aggregators within Coherence. Custom Aggregators This guide walks you through how to create custom aggregators within Coherence. Views Learn about the basic concepts of working with views using the ContinuousQueryCache . Streams This guide walks you through how to use the Streams API with Coherence. Entry Processors This guide walks you through how to use Entry Processors with Coherence. Federation This guide walks you through how to use Federation within Coherence. Topics This guide walks you through how to use Topics within Coherence. Near Caching This guide walks you through how to use near caching within Coherence. Client Events This guide walks you through how to use client events within Coherence. Server-Side Events This guide walks you through how to use server-side events within Coherence. Durable Events This guide walks you through how to use durable events within Coherence. Cache Stores This guide walks you through how to use and configure Cache Stores. Bulk Loading Caches This guide shows approaches to bulk load data into caches, typically this would be loading data into caches from a DB when applications start. Securing with SSL This guide walks you through how to secure Coherence using SSL/TLS. Performance over Consistency & Availability This guide walks you through how to tweak Coherence to provide more performance at the expense of data consistency and availability. Executor Service This guide explains how to use the Coherence Executor Service. CDI Response Caching This guide walks you through how to configure CDI Response Caching to cache the results of method invocations. Key Association This guide walks you through a use case for key association in Coherence. Partition Level Transactions This guide explains how to atomically access and update multiple related entries using an EntryProcessor in a partition level transaction. Multi-Cluster Client An example of how to connect an Extend or gRPC client to multiple Coherence clusters. ",
            "title": "Guides"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " Key association in Coherence is a way of associating related data together in a single partition. This data could be entries in a single cache, or it could be entries in multiple caches managed by the same cache service. If related data is known to exist in a single partition, then this allows those related entries to be accessed as part of a single atomic partition level transaction. For example a single entry processor call could atomically update multiple related entries, possibly across multiple caches. Queries could also make use of this, for example a custom aggregator could aggregate results from multiple entries possibly from multiple caches, in a single partition. This can be a way to simulate certain types of join query for related data. Key association can be used to implement similar behaviour to a multi-map, where a single key maps to a list or set of related data. Using key association and related caches instead of a single multi-map offers a lot more flexibility for supporting various use-cases. ",
            "title": "Key Association"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " This example is going to demonstrate a simple use case of handling notifications sent to customers. A customer can have zero or more notifications. A customer may span regions, so notifications are region specific. A notification also has an expiry time, so it will be automatically evicted when the expiry time is reached. Using key association, notifications for a customer will be co-located in the same partition. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " In this example there will be two Java mode classes, Customer and Notification . The Customer has a String id field and String first name and last name fields. <markup lang=\"java\" title=\"Customer.java\" >@PortableType(id = 1001, version = 1) public class Customer { /** * The customer's identifier. */ private String id; /** * The customer's first name. */ private String firstName; /** * The customer's last name. */ private String lastName; /** * Create a customer. * * @param id the customer's identifier * @param firstName the customer's first name * @param lastName the customer's last name */ public Customer(String id, String firstName, String lastName) { this.id = id; this.firstName = firstName; this.lastName = lastName; } /** * Returns the customer's identifier. * * @return the customer's identifier */ public String getId() { return id; } /** * Returns the customer's first name. * * @return the customer's first name */ public String getFirstName() { return firstName; } /** * Set the customer's first name. * * @param firstName the customer's first name */ public void setFirstName(String firstName) { this.firstName = firstName; } /** * Returns the customer's last name. * * @return the customer's last name */ public String getLastName() { return lastName; } /** * Set the customer's last name. * * @param lastName the customer's last name */ public void setLastName(String lastName) { this.lastName = lastName; } } The Notification has a String body and a Java time LocalDateTime ttl field, to represent its expiry time. <markup lang=\"java\" title=\"Notification.java\" >@PortableType(id = 1010, version = 1) public class Notification { /** * The notification text. */ private String body; /** * The time the notification expires. */ private LocalDateTime ttl; /** * Create a {@link Notification}. * * @param body the notification text * @param ttl the time the notification expires */ public Notification(String body, LocalDateTime ttl) { this.body = body; this.ttl = ttl; } /** * Returns the notification text. * * @return the notification text */ public String getBody() { return body; } /** * Returns the time the notification expires. * * @return the time the notification expires */ public LocalDateTime getTTL() { return ttl; } } Both of the model classes are annotated with the @PortableType . This annotation is used by the Coherence POF Maven plugin to generate Portable Object code for the classes. Using the Coherence POF generator in this way avoids having to manually write serialization code and ensures that the serialization code generated is supports evolvability between versions. ",
            "title": "Model Classes"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The customers cache will be used to store customer data. The cache key will be the String customer id . The notifications cache will be used to store notification data. A NotificationId class will be used for the key of the cache. The NotificationId will hold the notification&#8217;s corresponding customer id, region and a unique UUID identifier for the notifications. The caches in this example do not require and special functionality, so the default cache configuration file will support everything required. ",
            "title": "Caches"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The data model used in this example is very simple and is made up of two entities, a Customer and a Notification . A customer can have zero or more notifications. A notification is specific to a region and has an expiry time. For example, in json the customer notification data may look like this: <markup lang=\"json\" title=\"customers.json\" >[ { \"id\": \"User01\", \"notificationsByRegion\": [ { \"region\": \"US\", \"notifications\": [ { \"body\": \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\", \"ttl\": \"24:00:00\" }, { \"body\": \"Eu turpis egestas pretium aenean.\", \"ttl\": \"12:00:00\" } ] }, { \"region\": \"EU\", \"notifications\": [ { \"body\": \"Tincidunt id aliquet risus feugiat.\", \"ttl\": \"06:00:00\" }, { \"body\": \"Quis risus sed vulputate odio ut enim blandit volutpat.\", \"ttl\": \"48:00:00\" }, { \"body\": \"Sem et tortor consequat id porta nibh.\", \"ttl\": \"01:00:00\" } ] } ] }, { \"id\": \"User02\", \"notificationsByRegion\": [ { \"region\": \"US\", \"notifications\": [ { \"body\": \"Et malesuada fames ac turpis egestas sed tempus urna.\", \"ttl\": \"01:23:45\" } ] } ] } ] This structure could be contained in Java, and hence in Coherence, in a Map&lt;String, Map&lt;String, List&lt;Notification&gt;&gt;&gt; or some other multi-map type of data structure. The disadvantages of this are that a customers' notifications are then treated as a single blob of data which could make certain operations less efficient. Any mutation or addition of notifications would require everything to be deserialized. There is also a requirement in this example to automatically expire notifications from the cache based on their TTL is reached. If all the notifications for a customer are in a single map structure, this would require some complex server side logic whereas holding each notification as a separate cache entry can leverage Coherence&#8217;s built in expiry functionality. The json data above is really just notification data and this example could use just a single cache, but using two entities and two caches, for Customer and Notification, will make the example a bit more interesting. Model Classes In this example there will be two Java mode classes, Customer and Notification . The Customer has a String id field and String first name and last name fields. <markup lang=\"java\" title=\"Customer.java\" >@PortableType(id = 1001, version = 1) public class Customer { /** * The customer's identifier. */ private String id; /** * The customer's first name. */ private String firstName; /** * The customer's last name. */ private String lastName; /** * Create a customer. * * @param id the customer's identifier * @param firstName the customer's first name * @param lastName the customer's last name */ public Customer(String id, String firstName, String lastName) { this.id = id; this.firstName = firstName; this.lastName = lastName; } /** * Returns the customer's identifier. * * @return the customer's identifier */ public String getId() { return id; } /** * Returns the customer's first name. * * @return the customer's first name */ public String getFirstName() { return firstName; } /** * Set the customer's first name. * * @param firstName the customer's first name */ public void setFirstName(String firstName) { this.firstName = firstName; } /** * Returns the customer's last name. * * @return the customer's last name */ public String getLastName() { return lastName; } /** * Set the customer's last name. * * @param lastName the customer's last name */ public void setLastName(String lastName) { this.lastName = lastName; } } The Notification has a String body and a Java time LocalDateTime ttl field, to represent its expiry time. <markup lang=\"java\" title=\"Notification.java\" >@PortableType(id = 1010, version = 1) public class Notification { /** * The notification text. */ private String body; /** * The time the notification expires. */ private LocalDateTime ttl; /** * Create a {@link Notification}. * * @param body the notification text * @param ttl the time the notification expires */ public Notification(String body, LocalDateTime ttl) { this.body = body; this.ttl = ttl; } /** * Returns the notification text. * * @return the notification text */ public String getBody() { return body; } /** * Returns the time the notification expires. * * @return the time the notification expires */ public LocalDateTime getTTL() { return ttl; } } Both of the model classes are annotated with the @PortableType . This annotation is used by the Coherence POF Maven plugin to generate Portable Object code for the classes. Using the Coherence POF generator in this way avoids having to manually write serialization code and ensures that the serialization code generated is supports evolvability between versions. Caches The customers cache will be used to store customer data. The cache key will be the String customer id . The notifications cache will be used to store notification data. A NotificationId class will be used for the key of the cache. The NotificationId will hold the notification&#8217;s corresponding customer id, region and a unique UUID identifier for the notifications. The caches in this example do not require and special functionality, so the default cache configuration file will support everything required. ",
            "title": "The Example Data Model"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " In this use case key association will be used to co-locate a Customer and all the Notification entries for that customer in the same Coherence partition. This will allow notifications to be added and queried for as specific customer as an atomic operation. To use key association, the key classes fo the caches to be associated must either be the same or implement the Coherence com.tangosol.net.cache.KeyAssociation interface. If notifications for a customer were going to be held in a map or list in a single cache entry, we could just use the same String customer identifier as the key and the customer and the notification map would automatically be assigned to the same partition, as they would have the same key value. In this case though, there will be many notification entries for a single customer so the notifications cache requires a custom key class that implements KeyAssociation . The NotificationId class is shown below: <markup lang=\"java\" title=\"NotificationId.java\" >@PortableType(id = 1011, version = 1) public class NotificationId implements KeyAssociation&lt;String&gt;, Comparable&lt;NotificationId&gt; { /** * The customer the notification is for. */ private String customerId; /** * The region the notification applies to. */ private String region; /** * The notification unique identifier. */ private UUID id; /** * Create a notification identifier. * * @param customerId the customer the notification is for * @param region the region the notification applies to * @param id the notification identifier */ public NotificationId(String customerId, String region, UUID id) { this.customerId = customerId; this.region = region; this.id = id; } /** * Returns the identifier of the customer the notification is for. * * @return the identifier of the customer the notification is for */ public String getCustomerId() { return customerId; } /** * Returns the region the notification applies to. * * @return the region the notification applies to */ public String getRegion() { return region; } /** * Returns the notification identifier. * * @return the notification identifier */ public UUID getId() { return id; } @Override public String getAssociatedKey() { return customerId; } @Override public int compareTo(NotificationId o) { int n = SafeComparator.compareSafe(Comparator.naturalOrder(), customerId, o.customerId); if (n == 0) { n = Long.compare(id.getTimestamp(), o.id.getTimestamp()); if (n == 0) { n = Long.compare(id.getCount(), o.id.getCount()); } } return n; } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } NotificationId that = (NotificationId) o; return Objects.equals(customerId, that.customerId) &amp;&amp; Objects.equals(region, that.region) &amp;&amp; Objects.equals(id, that.id); } @Override public int hashCode() { return Objects.hash(customerId, region, id); } } Like the Customer and Notification classes, the NotificationId class is annotated with @PortableType to automatically generate the PortableObject serialization code. All classes that will be used as cache keys in Coherence must properly implement the equals and hashCode methods and include all fields in those methods. The important method for this example is the getAssociatedKey() method from the KeyAssociation interface. This method should return the value that this key is to be associated with. In this case notifications are associated to customers, so the customer identifier is returned. This will then guarantee that a customer and its notifications are all located in the same partition in Coherence. <markup lang=\"java\" title=\"NotificationId.java\" > @Override public String getAssociatedKey() { return customerId; } ",
            "title": "Coherence Key Association"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " This example is going to use the \"repository\" functionality in Coherence. A repository is a simple class that provides CRUD operations for an entity. In this case the repository will be for the Customer entity, because that is the root entry point for all operations, including those on notifications. Making all updates and queries access caches via the customer in this way, ensures that updates to notifications are treated as a single atomic operation. The example does not require the use of a repository class, but it is a nice way to group all the customer related operations together in a single class. The minimum amount of code to implement a repository is shown below. The CustomerRepository class extends the com.oracle.coherence.repository.AbstractRepository base class and implements the required abstract methods. <markup lang=\"java\" title=\"CustomerRepository.java\" >public class CustomerRepository extends AbstractRepository&lt;String, Customer&gt; { /** * The customer's cache. */ private final NamedMap&lt;String, Customer&gt; customers; public CustomerRepository(NamedCache&lt;String, Customer&gt; customers) { this.customers = customers; } @Override protected String getId(Customer entity) { return entity.getId(); } @Override protected Class&lt;? extends Customer&gt; getEntityType() { return Customer.class; } @Override protected NamedMap&lt;String, Customer&gt; getMap() { return customers; } } In the rest of the example the CustomerRepository will be enhanced to add additional functionality for notifications. ",
            "title": "The Customer Repository"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " Although the CustomerRepository.addNotifications method could be called and would execute, the AddNotifications.process method is empty, so no notifications will actually be added. The next step is to implement the process method to add the notifications to the notifications cache. At this point it is worth going over what the process method must do for each entry in the notification map. Check the ttl of the entry, if it has already passed then ignore the notification as there i sno point adding it to be immediately expired Create a NotificationId for the key of the new notification cache entry. Use the key to obtain the cache entry to insert Set the notification as the value for the cache entry Set the expiry value for the new entry based on the ttl value of the notification. Iterate Over the Notifications The process method can simply iterate over the map of notifications like this: <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { // process notification... }); }); } Work out the Expiry Delay A Coherence cache entry expects the expiry for an entry to be the number of milliseconds after the entry is inserted or updated before it expires. The ttl value in the Notification class is a Java LocalDateTime so the expiry is the difference between now and the ttl in milliseconds. In Java that can be written as shown below: <markup lang=\"java\" >long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); If the ttlInMillis is greater than zero the notification can be added. If it is less than or equal to zero, then there is no point adding the notification as the ttl is already in the past. <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); if (ttlInMillis &gt; 0) { // add the notification... } }); }); } Create a NotificationId Creating the NotificationId is simple. The customer identifier can be taken from the key of the entry passed to the process method String customerId = entry.getKey(); , the region comes from the notifications map and the UUID is just a new UUID created at runtime. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); Obtain the Notification Cache Entry When using Coherence partition level transactions to atomically update other cache entries in an entry processor, those additional entries must be properly obtained from the relevant cache&#8217;s BackingMapContext . Coherence will then ensure that all mutations are properly handled, backup messages sent, events fired, etc. Each additional entry enlisted in this sort of lite partition transactions, will be locked until the entry processor completes processing. This can cause issues if two entry processors run that try to enlist the same set of entries but in different orders. Each processor may be holding locks on a sub-set of the entries, and then each is unable to obtain locks on the remaining entries it requires. The safest way around this is to sort the keys that will be enlisted so both processors always enlist entries in the same order. In this example, notifications are only ever inserted, so there is no chance of two processors enlisting the same entries. The entry processor is executing on an entry from the customers cache, so to obtain the BackingMapContext for the notifications cache can be obtained via the customer entry. <markup lang=\"java\" >BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); To obtain the entry to insert from the BackingMapContext the BackingMapContext.getBackingMapEntry() method is used. This method takes the key of the entry to obtain, but this key must be in serialized Binary format, not a plain NotificationId . The BackingMapManagerContext conveniently has a converter that can do the serialization. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); The notification is then set as the entry value using the setValue() method and the expiry set using the expire() method. <markup lang=\"java\" >binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); This can all be put together in the final process method: <markup lang=\"java\" title=\"AddNotifications.java\" > @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { BackingMapManagerContext context = entry.asBinaryEntry().getContext(); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); BackingMapContext ctxNotifications = context.getBackingMapContext( CustomerRepository.NOTIFICATIONS_MAP_NAME); String customerId = entry.getKey(); LocalDateTime now = LocalDateTime.now(); notifications.forEach((region, notificationsForRegion)-&gt; { notificationsForRegion.forEach(notification-&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(now, notification.getTTL()); if (ttlInMillis &gt; 0) { NotificationId id = new NotificationId(customerId, region, new UUID()); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); } }); }); return null; } ",
            "title": "Implement the Process Method"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " To perform the add operation, a custom Coherence entry processor can be written. This entry processor will take the map of notifications and apply it to the customer. As key association is being used, the entry processor will be executed against the customer identifier in the customer cache and apply all the notifications in a single atomic partition level transaction. For the duration of the operation on the server the customer will effectively be locked, guaranteeing that only a single concurrent mutation operation can happen to a customer. The boilerplate code for the AddNotifications entry processor is shown below. As with other classes, the entry processor is annotated with @PortableType to generate PortableObject code. The result returned from this entry processor&#8217;s process method is Void as there is no information that the caller requires as a result. <markup lang=\"java\" title=\"AddNotifications.java\" >@PortableType(id = 1100, version = 1) public class AddNotifications implements InvocableMap.EntryProcessor&lt;String, Customer, Void&gt; { /** * The notifications to add to the customer. */ private Map&lt;String, List&lt;Notification&gt;&gt; notifications; /** * Create a {@link AddNotifications} processor. * * @param notifications the notifications to add to the customer */ public AddNotifications(Map&lt;String, List&lt;Notification&gt;&gt; notifications) { this.notifications = notifications; } @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { return null; } A new addNotifications method can be added to the repository, which will invoke the AddNotifications entry processor against a specific customer identifier. The addNotifications first ensures the repository is initialized and then invokes the entry processor. using the map of notifications. The method will throw a NullPointerException if the customer identifier is null . <markup lang=\"java\" title=\"CustomerRepository.java\" > public void addNotifications(String customerId, Map&lt;String, List&lt;Notification&gt;&gt; notifications) { ensureInitialized(); customers.invoke(Objects.requireNonNull(customerId), new AddNotifications(Objects.requireNonNull(notifications))); } Implement the Process Method Although the CustomerRepository.addNotifications method could be called and would execute, the AddNotifications.process method is empty, so no notifications will actually be added. The next step is to implement the process method to add the notifications to the notifications cache. At this point it is worth going over what the process method must do for each entry in the notification map. Check the ttl of the entry, if it has already passed then ignore the notification as there i sno point adding it to be immediately expired Create a NotificationId for the key of the new notification cache entry. Use the key to obtain the cache entry to insert Set the notification as the value for the cache entry Set the expiry value for the new entry based on the ttl value of the notification. Iterate Over the Notifications The process method can simply iterate over the map of notifications like this: <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { // process notification... }); }); } Work out the Expiry Delay A Coherence cache entry expects the expiry for an entry to be the number of milliseconds after the entry is inserted or updated before it expires. The ttl value in the Notification class is a Java LocalDateTime so the expiry is the difference between now and the ttl in milliseconds. In Java that can be written as shown below: <markup lang=\"java\" >long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); If the ttlInMillis is greater than zero the notification can be added. If it is less than or equal to zero, then there is no point adding the notification as the ttl is already in the past. <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); if (ttlInMillis &gt; 0) { // add the notification... } }); }); } Create a NotificationId Creating the NotificationId is simple. The customer identifier can be taken from the key of the entry passed to the process method String customerId = entry.getKey(); , the region comes from the notifications map and the UUID is just a new UUID created at runtime. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); Obtain the Notification Cache Entry When using Coherence partition level transactions to atomically update other cache entries in an entry processor, those additional entries must be properly obtained from the relevant cache&#8217;s BackingMapContext . Coherence will then ensure that all mutations are properly handled, backup messages sent, events fired, etc. Each additional entry enlisted in this sort of lite partition transactions, will be locked until the entry processor completes processing. This can cause issues if two entry processors run that try to enlist the same set of entries but in different orders. Each processor may be holding locks on a sub-set of the entries, and then each is unable to obtain locks on the remaining entries it requires. The safest way around this is to sort the keys that will be enlisted so both processors always enlist entries in the same order. In this example, notifications are only ever inserted, so there is no chance of two processors enlisting the same entries. The entry processor is executing on an entry from the customers cache, so to obtain the BackingMapContext for the notifications cache can be obtained via the customer entry. <markup lang=\"java\" >BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); To obtain the entry to insert from the BackingMapContext the BackingMapContext.getBackingMapEntry() method is used. This method takes the key of the entry to obtain, but this key must be in serialized Binary format, not a plain NotificationId . The BackingMapManagerContext conveniently has a converter that can do the serialization. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); The notification is then set as the entry value using the setValue() method and the expiry set using the expire() method. <markup lang=\"java\" >binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); This can all be put together in the final process method: <markup lang=\"java\" title=\"AddNotifications.java\" > @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { BackingMapManagerContext context = entry.asBinaryEntry().getContext(); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); BackingMapContext ctxNotifications = context.getBackingMapContext( CustomerRepository.NOTIFICATIONS_MAP_NAME); String customerId = entry.getKey(); LocalDateTime now = LocalDateTime.now(); notifications.forEach((region, notificationsForRegion)-&gt; { notificationsForRegion.forEach(notification-&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(now, notification.getTTL()); if (ttlInMillis &gt; 0) { NotificationId id = new NotificationId(customerId, region, new UUID()); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); } }); }); return null; } ",
            "title": "The AddNotifications Entry Processor"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The CustomerRepository can then be used to add customers and notifications, this can be seen in the functional tests that are part of this example. <markup lang=\"java\" > CustomerRepository repository = new CustomerRepository(); Customer customer = new Customer(\"QA22\", \"Julian\", \"Alaphilippe\"); repository.save(customer); Notification notification = new Notification(\"Ride TdF\", LocalDateTime.now().plusDays(1)); repository.addNotifications(customer, \"FRA\", notification); ",
            "title": "Adding Notifications via the CustomerRepository"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The obvious starting point would be to enhance the repository to be able to add notifications for a customer. Read operations will come later, as they&#8217;d be a bit pointless without first having add operations. The use case here is to allow multiple notifications to be added to a customer is a single atomic operations. Notifications are specific to a region, so the obvious structure to hold the notifications to be added would be a map of the form Map&lt;String, List&lt;Notification&gt;&gt; where the key is the region and the value is a list of notifications for that region. The AddNotifications Entry Processor To perform the add operation, a custom Coherence entry processor can be written. This entry processor will take the map of notifications and apply it to the customer. As key association is being used, the entry processor will be executed against the customer identifier in the customer cache and apply all the notifications in a single atomic partition level transaction. For the duration of the operation on the server the customer will effectively be locked, guaranteeing that only a single concurrent mutation operation can happen to a customer. The boilerplate code for the AddNotifications entry processor is shown below. As with other classes, the entry processor is annotated with @PortableType to generate PortableObject code. The result returned from this entry processor&#8217;s process method is Void as there is no information that the caller requires as a result. <markup lang=\"java\" title=\"AddNotifications.java\" >@PortableType(id = 1100, version = 1) public class AddNotifications implements InvocableMap.EntryProcessor&lt;String, Customer, Void&gt; { /** * The notifications to add to the customer. */ private Map&lt;String, List&lt;Notification&gt;&gt; notifications; /** * Create a {@link AddNotifications} processor. * * @param notifications the notifications to add to the customer */ public AddNotifications(Map&lt;String, List&lt;Notification&gt;&gt; notifications) { this.notifications = notifications; } @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { return null; } A new addNotifications method can be added to the repository, which will invoke the AddNotifications entry processor against a specific customer identifier. The addNotifications first ensures the repository is initialized and then invokes the entry processor. using the map of notifications. The method will throw a NullPointerException if the customer identifier is null . <markup lang=\"java\" title=\"CustomerRepository.java\" > public void addNotifications(String customerId, Map&lt;String, List&lt;Notification&gt;&gt; notifications) { ensureInitialized(); customers.invoke(Objects.requireNonNull(customerId), new AddNotifications(Objects.requireNonNull(notifications))); } Implement the Process Method Although the CustomerRepository.addNotifications method could be called and would execute, the AddNotifications.process method is empty, so no notifications will actually be added. The next step is to implement the process method to add the notifications to the notifications cache. At this point it is worth going over what the process method must do for each entry in the notification map. Check the ttl of the entry, if it has already passed then ignore the notification as there i sno point adding it to be immediately expired Create a NotificationId for the key of the new notification cache entry. Use the key to obtain the cache entry to insert Set the notification as the value for the cache entry Set the expiry value for the new entry based on the ttl value of the notification. Iterate Over the Notifications The process method can simply iterate over the map of notifications like this: <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { // process notification... }); }); } Work out the Expiry Delay A Coherence cache entry expects the expiry for an entry to be the number of milliseconds after the entry is inserted or updated before it expires. The ttl value in the Notification class is a Java LocalDateTime so the expiry is the difference between now and the ttl in milliseconds. In Java that can be written as shown below: <markup lang=\"java\" >long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); If the ttlInMillis is greater than zero the notification can be added. If it is less than or equal to zero, then there is no point adding the notification as the ttl is already in the past. <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); if (ttlInMillis &gt; 0) { // add the notification... } }); }); } Create a NotificationId Creating the NotificationId is simple. The customer identifier can be taken from the key of the entry passed to the process method String customerId = entry.getKey(); , the region comes from the notifications map and the UUID is just a new UUID created at runtime. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); Obtain the Notification Cache Entry When using Coherence partition level transactions to atomically update other cache entries in an entry processor, those additional entries must be properly obtained from the relevant cache&#8217;s BackingMapContext . Coherence will then ensure that all mutations are properly handled, backup messages sent, events fired, etc. Each additional entry enlisted in this sort of lite partition transactions, will be locked until the entry processor completes processing. This can cause issues if two entry processors run that try to enlist the same set of entries but in different orders. Each processor may be holding locks on a sub-set of the entries, and then each is unable to obtain locks on the remaining entries it requires. The safest way around this is to sort the keys that will be enlisted so both processors always enlist entries in the same order. In this example, notifications are only ever inserted, so there is no chance of two processors enlisting the same entries. The entry processor is executing on an entry from the customers cache, so to obtain the BackingMapContext for the notifications cache can be obtained via the customer entry. <markup lang=\"java\" >BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); To obtain the entry to insert from the BackingMapContext the BackingMapContext.getBackingMapEntry() method is used. This method takes the key of the entry to obtain, but this key must be in serialized Binary format, not a plain NotificationId . The BackingMapManagerContext conveniently has a converter that can do the serialization. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); The notification is then set as the entry value using the setValue() method and the expiry set using the expire() method. <markup lang=\"java\" >binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); This can all be put together in the final process method: <markup lang=\"java\" title=\"AddNotifications.java\" > @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { BackingMapManagerContext context = entry.asBinaryEntry().getContext(); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); BackingMapContext ctxNotifications = context.getBackingMapContext( CustomerRepository.NOTIFICATIONS_MAP_NAME); String customerId = entry.getKey(); LocalDateTime now = LocalDateTime.now(); notifications.forEach((region, notificationsForRegion)-&gt; { notificationsForRegion.forEach(notification-&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(now, notification.getTTL()); if (ttlInMillis &gt; 0) { NotificationId id = new NotificationId(customerId, region, new UUID()); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); } }); }); return null; } Adding Notifications via the CustomerRepository The CustomerRepository can then be used to add customers and notifications, this can be seen in the functional tests that are part of this example. <markup lang=\"java\" > CustomerRepository repository = new CustomerRepository(); Customer customer = new Customer(\"QA22\", \"Julian\", \"Alaphilippe\"); repository.save(customer); Notification notification = new Notification(\"Ride TdF\", LocalDateTime.now().plusDays(1)); repository.addNotifications(customer, \"FRA\", notification); ",
            "title": "Adding Notifications"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " When the extractor&#8217;s extractFromEntry method executes, in this case the entry passed in by the aggregator will be an instance of BinaryEntry , so just like in the entry processor above, the BackingMapContext for the notifications cache can be obtained and from there access to the notification entries. Coherence does not currently have an API on a BackingMapContext that allows the data to be queried. For example, in this case some sort of filter query over all the entries in the partition with a specific customer id would get the notification required. This can be worked around by using cache indexes. The indexes on a cache are accessible via the BackingMapContext and from the index contents the required cache entries can be obtained. Take the first requirement, all notifications for a customer. By creating an index of customer id on the notifications cache, the keys of the entries for a given customer can be obtained from the index and the corresponding notifications returned from the extractor. Customer Id Index Indexes are created on a cache using a ValueExtractor to extract the values to be indexed. In the case of the customer id for a notification, this is a field in the NotificationId , which is the key to the notifications cache. An extractor to extract customer id can be created as shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); This extractor can be used as an index by calling the addIndex method on NamedCache or NamedMap . <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); notifications.addIndex(extractor); The Region Index The second index required is to be able to find notifications for a customer and region. In theory this index is not required, the index to find all notifications for a customer could be used, then those notifications filtered to only return those for the required region. If there will only be a small number of notifications per customer, that may be a suitable approach. This is one of the typical pros and cons that needs to be weighed up when using indexes. Does the cost in memory usage of the index and time to maintain the index on every mutation outweigh the benefits in speed gained by queries. This example is going to add an index on region, because it is an example there are no concerns over performance, and it will show how to perform an indexed query. The extractor to extract region from the NotificationId cache entry key is shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); This can be used to create an index: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); notifications.addIndex(extractor); Creating the Indexes The repository class already has a method that is called to create any required indexes when it is initialized. This method can be overridden and used to ensure the notifications indexes are added. <markup lang=\"java\" title=\"CustomerRepository.java\" > @Override @SuppressWarnings( {\"unchecked\", \"resource\"}) protected void createIndices() { super.createIndices(); CacheService service = customers.getService(); NamedCache&lt;NotificationId, Notification&gt; notifications = service.ensureCache(NOTIFICATIONS_MAP_NAME, service.getContextClassLoader()); notifications.addIndex(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); notifications.addIndex(ValueExtractor.of(NotificationId::getRegion).fromKey()); } Note, that the super class createIndicies() method must be called to ensure any other indicies required by the customer repository are created. ",
            "title": "Find the Customer&#8217;s Notifications"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " Now that the required indexes will be present the NotificationExtractor.extractFromEntry() method can be written. The techniques used below rely on the indexes being present and would not work if there were no indexes. Without indexes other less efficient methods would be required. The steps the extract method must perform are shown below: Obtain the map of indexes for the notifications cache From the index map, obtain the customer id index From the customer id index obtain the set of notification keys matching the customer id If the region is specified, reduce the set of keys to only those matching the required region For each remaining key, obtain the read-only cache entry containing the notification and add it to the results list return the list of notifications found Each step is covered in detail below: Obtain the map of indexes for the notifications cache The entry passed to the NotificationExtractor.extractFromEntry method when used in an aggregator will be an instance of a BinaryEntry so the entry can safely be cast to BinaryEntry . From a BinaryEntry it is possible to obtain the BackingMapManagerContext and from there the BackingMapContext of other caches. Remember, in this example the aggregator is executed on an entry in the customers cache, so the extractor needs to obtain the BackingMapContext of the notifications cache. From the notifications cache BackingMapContext the map of indexes can be obtained. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); From the index map, obtain the customer id index The index map is a map of MapIndex instances keyed by the ValueExtractor used to create the index. To obtain the customer id index just call the get() method using the same customer id extractor used to create the index above. This is one of the main reasons that all ValueExtractor implementations must properly implement equals() and hashCode() so that they can be used in indexes. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); From the customer id index obtain the set of notification keys matching the customer id A Coherence MapIndex typically holds two internal indexes. The keys in the index are in serialized binary format, that is, they can be used directly to obtain corresponding entries. A map of cache key to the extracted index value for that key A map of extracted index value to the set of keys that match that value In the case of the customer id index that means the index holds a map of binary key to corresponding customer id and a map of customer id to keys of entries for that customer id. The second map is the one required for this use case, which can be obtained from the MapIndex.getIndexContents() method. The set of keys for the customer can then be obtained with a simple get(customerId) on the index contents map (the customer id is just the key of the entry passed to the extractFromEntry method. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); At this point the keys set is the key of all the notification entries for the customer. Further Filter by Region If the region has been specified, the set of keys needs to be further filtered to just those for the required region. This could be achieved a number of ways, but this example is going to show how Coherence filters and indexes can be used to reduce a set of keys. Almost all filters in Coherence implement IndexAwareFilter which means they have an applyIndex method: <markup lang=\"java\" >public &lt;RK&gt; Filter&lt;V&gt; applyIndex( Map&lt;? extends ValueExtractor&lt;? extends V, Object&gt;, ? extends MapIndex&lt;? extends RK, ? extends V, Object&gt;&gt; mapIndexes, Set&lt;? extends RK&gt; setKeys); When the applyIndex method is called, the Set of keys passed in will be reduced to only those keys matching the filter. This means that an EqualsFilter using the region extractor can be used to reduce the set of all keys for the customer down to just those keys matching the region too. Again, the extractor used in the EqualsFilter must be the same extractor used to create the region index. <markup lang=\"java\" >if (region != null &amp;&amp; !region.isBlank()) { ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Now the keys set has been reduced to only key matching both customer id and region. Obtain the Notifications The set of keys can be used to obtain notification from the notifications cache. The safest way to do this is to use the BackingMapContext.getReadOnlyEntry() method. The final list of notifications will be ordered by creation data. This is possible because the NotificationId class used in this example implements Comparable and makes use of the fact that the Coherence UUID used as a unique id in the notification contains a timestamp. The example used Java streams to process the keys into a list of notifications, the code is shown below: <markup lang=\"java\" > Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2)-&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); The key is mapped to a read-only InvocableMap.Entry Only process entries that are present for the key (in case it has just been removed) Sort the entries using the comparator to sort by key (i.e. NotificationId ) Map the entry to just the value (i.e. the Notification ) Cast the value to a Notification (this is because Java does not know the InvocableMap.Entry generic types) Collect the final Notification instances into a list The Final Method All the code above can be combined into the final extractFromEntry() method. <markup lang=\"java\" > @Override @SuppressWarnings( {\"rawtypes\", \"unchecked\"}) public List&lt;Notification&gt; extractFromEntry(Map.Entry entry) { BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext() .getBackingMapContext(CustomerRepository.NOTIFICATIONS_MAP_NAME); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(binaryEntry.getKeyPartition()); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); if (keys == null || keys.isEmpty()) { return Collections.emptyList(); } if (region != null &amp;&amp; !region.isBlank()) { // copy the keys, so we don't modify the underlying index keys = new HashSet&lt;&gt;(keys); ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2)-&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); } Note Looking at the source code, or JavaDoc, for BackingMapContext will show the getBackingMap() method, which returns the actual map of Binary keys and values in the cache; it should also be obvious that this method is deprecated. It may seem like this is a good way to access the data in the cache for the use case above, but directly accessing the data this way can break the guarantees and locks provided by Coherence. Ideally this method would have been removed, but backwards compatibility constraints mean it is still there, but it should not be used. ",
            "title": "Write the NotificationExtractor extractFromEntry method"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The purpose of the custom ValueExtractor will be to obtain the notifications for a customer. The notifications are all co-located in a single partition, so when the extractor is run against an entry in the customer cache, all the notifications are also stored locally. This particular ValueExtract is going to need access to the entry the aggregator is executing on, so it needs to extend the Coherence com.tangosol.util.extractor.AbstractExtractor class. The AbstractExtractor is treated as a special case by Coherence when it is extracting data from a cache entry, where Coherence will call its extractFromEntry method. The boilerplate code for a custom extractor is shown below. All ValueExtractor implementations should have a correct equals() and hashCode() methods. The extractFromEntry method returns null , and will be completed in the next section. <markup lang=\"java\" title=\"NotificationExtractor.java\" >@PortableType(id = 1200, version = 1) public class NotificationExtractor extends AbstractExtractor&lt;Customer, List&lt;Notification&gt;&gt; { /** * An optional region identifier to use to retrieve * only notifications for a specific region. */ private String region; /** * Create a {@link NotificationExtractor} that will specifically * target the key when used to extract from a cache entry. * * @param region an optional region identifier */ public NotificationExtractor(String region) { this.region = region; } @Override public List&lt;Notification&gt; extractFromEntry(Map.Entry entry) { return null; } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } if (!super.equals(o)) { return false; } NotificationExtractor that = (NotificationExtractor) o; return Objects.equals(region, that.region); } @Override public int hashCode() { return Objects.hash(super.hashCode(), region); } Find the Customer&#8217;s Notifications When the extractor&#8217;s extractFromEntry method executes, in this case the entry passed in by the aggregator will be an instance of BinaryEntry , so just like in the entry processor above, the BackingMapContext for the notifications cache can be obtained and from there access to the notification entries. Coherence does not currently have an API on a BackingMapContext that allows the data to be queried. For example, in this case some sort of filter query over all the entries in the partition with a specific customer id would get the notification required. This can be worked around by using cache indexes. The indexes on a cache are accessible via the BackingMapContext and from the index contents the required cache entries can be obtained. Take the first requirement, all notifications for a customer. By creating an index of customer id on the notifications cache, the keys of the entries for a given customer can be obtained from the index and the corresponding notifications returned from the extractor. Customer Id Index Indexes are created on a cache using a ValueExtractor to extract the values to be indexed. In the case of the customer id for a notification, this is a field in the NotificationId , which is the key to the notifications cache. An extractor to extract customer id can be created as shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); This extractor can be used as an index by calling the addIndex method on NamedCache or NamedMap . <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); notifications.addIndex(extractor); The Region Index The second index required is to be able to find notifications for a customer and region. In theory this index is not required, the index to find all notifications for a customer could be used, then those notifications filtered to only return those for the required region. If there will only be a small number of notifications per customer, that may be a suitable approach. This is one of the typical pros and cons that needs to be weighed up when using indexes. Does the cost in memory usage of the index and time to maintain the index on every mutation outweigh the benefits in speed gained by queries. This example is going to add an index on region, because it is an example there are no concerns over performance, and it will show how to perform an indexed query. The extractor to extract region from the NotificationId cache entry key is shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); This can be used to create an index: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); notifications.addIndex(extractor); Creating the Indexes The repository class already has a method that is called to create any required indexes when it is initialized. This method can be overridden and used to ensure the notifications indexes are added. <markup lang=\"java\" title=\"CustomerRepository.java\" > @Override @SuppressWarnings( {\"unchecked\", \"resource\"}) protected void createIndices() { super.createIndices(); CacheService service = customers.getService(); NamedCache&lt;NotificationId, Notification&gt; notifications = service.ensureCache(NOTIFICATIONS_MAP_NAME, service.getContextClassLoader()); notifications.addIndex(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); notifications.addIndex(ValueExtractor.of(NotificationId::getRegion).fromKey()); } Note, that the super class createIndicies() method must be called to ensure any other indicies required by the customer repository are created. Write the NotificationExtractor extractFromEntry method Now that the required indexes will be present the NotificationExtractor.extractFromEntry() method can be written. The techniques used below rely on the indexes being present and would not work if there were no indexes. Without indexes other less efficient methods would be required. The steps the extract method must perform are shown below: Obtain the map of indexes for the notifications cache From the index map, obtain the customer id index From the customer id index obtain the set of notification keys matching the customer id If the region is specified, reduce the set of keys to only those matching the required region For each remaining key, obtain the read-only cache entry containing the notification and add it to the results list return the list of notifications found Each step is covered in detail below: Obtain the map of indexes for the notifications cache The entry passed to the NotificationExtractor.extractFromEntry method when used in an aggregator will be an instance of a BinaryEntry so the entry can safely be cast to BinaryEntry . From a BinaryEntry it is possible to obtain the BackingMapManagerContext and from there the BackingMapContext of other caches. Remember, in this example the aggregator is executed on an entry in the customers cache, so the extractor needs to obtain the BackingMapContext of the notifications cache. From the notifications cache BackingMapContext the map of indexes can be obtained. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); From the index map, obtain the customer id index The index map is a map of MapIndex instances keyed by the ValueExtractor used to create the index. To obtain the customer id index just call the get() method using the same customer id extractor used to create the index above. This is one of the main reasons that all ValueExtractor implementations must properly implement equals() and hashCode() so that they can be used in indexes. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); From the customer id index obtain the set of notification keys matching the customer id A Coherence MapIndex typically holds two internal indexes. The keys in the index are in serialized binary format, that is, they can be used directly to obtain corresponding entries. A map of cache key to the extracted index value for that key A map of extracted index value to the set of keys that match that value In the case of the customer id index that means the index holds a map of binary key to corresponding customer id and a map of customer id to keys of entries for that customer id. The second map is the one required for this use case, which can be obtained from the MapIndex.getIndexContents() method. The set of keys for the customer can then be obtained with a simple get(customerId) on the index contents map (the customer id is just the key of the entry passed to the extractFromEntry method. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); At this point the keys set is the key of all the notification entries for the customer. Further Filter by Region If the region has been specified, the set of keys needs to be further filtered to just those for the required region. This could be achieved a number of ways, but this example is going to show how Coherence filters and indexes can be used to reduce a set of keys. Almost all filters in Coherence implement IndexAwareFilter which means they have an applyIndex method: <markup lang=\"java\" >public &lt;RK&gt; Filter&lt;V&gt; applyIndex( Map&lt;? extends ValueExtractor&lt;? extends V, Object&gt;, ? extends MapIndex&lt;? extends RK, ? extends V, Object&gt;&gt; mapIndexes, Set&lt;? extends RK&gt; setKeys); When the applyIndex method is called, the Set of keys passed in will be reduced to only those keys matching the filter. This means that an EqualsFilter using the region extractor can be used to reduce the set of all keys for the customer down to just those keys matching the region too. Again, the extractor used in the EqualsFilter must be the same extractor used to create the region index. <markup lang=\"java\" >if (region != null &amp;&amp; !region.isBlank()) { ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Now the keys set has been reduced to only key matching both customer id and region. Obtain the Notifications The set of keys can be used to obtain notification from the notifications cache. The safest way to do this is to use the BackingMapContext.getReadOnlyEntry() method. The final list of notifications will be ordered by creation data. This is possible because the NotificationId class used in this example implements Comparable and makes use of the fact that the Coherence UUID used as a unique id in the notification contains a timestamp. The example used Java streams to process the keys into a list of notifications, the code is shown below: <markup lang=\"java\" > Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2)-&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); The key is mapped to a read-only InvocableMap.Entry Only process entries that are present for the key (in case it has just been removed) Sort the entries using the comparator to sort by key (i.e. NotificationId ) Map the entry to just the value (i.e. the Notification ) Cast the value to a Notification (this is because Java does not know the InvocableMap.Entry generic types) Collect the final Notification instances into a list The Final Method All the code above can be combined into the final extractFromEntry() method. <markup lang=\"java\" > @Override @SuppressWarnings( {\"rawtypes\", \"unchecked\"}) public List&lt;Notification&gt; extractFromEntry(Map.Entry entry) { BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext() .getBackingMapContext(CustomerRepository.NOTIFICATIONS_MAP_NAME); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(binaryEntry.getKeyPartition()); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); if (keys == null || keys.isEmpty()) { return Collections.emptyList(); } if (region != null &amp;&amp; !region.isBlank()) { // copy the keys, so we don't modify the underlying index keys = new HashSet&lt;&gt;(keys); ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2)-&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); } Note Looking at the source code, or JavaDoc, for BackingMapContext will show the getBackingMap() method, which returns the actual map of Binary keys and values in the cache; it should also be obvious that this method is deprecated. It may seem like this is a good way to access the data in the cache for the use case above, but directly accessing the data this way can break the guarantees and locks provided by Coherence. Ideally this method would have been removed, but backwards compatibility constraints mean it is still there, but it should not be used. ",
            "title": "The NotificationExtractor"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " Now the NotificationExtractor is complete, methods can now be added to the CustomerRepository to get notifications for a customer and optionally a region. <markup lang=\"java\" title=\"CustomerRepository.java\" > /** * Returns the notifications for a customer. * * @param customerId the identifier of the customer to obtain the notifications for * * @return the notifications for the customer */ public List&lt;Notification&gt; getNotifications(String customerId) { return getNotifications(customerId, null); } /** * Returns the notifications for a customer, and optionally a region. * * @param customerId the identifier of the customer to obtain the notifications for * @param region an optional region to get notifications for * * @return the notifications for the customer, optionally restricted to a region */ public List&lt;Notification&gt; getNotifications(String customerId, String region) { Map&lt;String, List&lt;Notification&gt;&gt; map = getAll(List.of(customerId), new NotificationExtractor(region)); return map.getOrDefault(customerId, Collections.emptyList()); } The getNotifications() method calls the getAll() method on the AbstractRepository super class, which takes a collection of keys and a ValueExtractor . Under the covers, the AbstractRepository.getAll() method just runs a ReducerAggregator with the provided ValueExtractor after ensuring the repository is properly initialized. The map of results returned by getAll() will only ever contain a single entry, as it is only ever called here with a singleton list of keys. The result map will be a map of customer id to a list of notifications. ",
            "title": "Add Get Notification Methods to the CustomerRepository"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " Now that notifications can be added for a customer, the read functions can be added to get notifications for a customer. There are two use cases to implement, first get all notifications for a customer, second get notification for a customer and specific region. As notifications are in their own cache, the notifications for a customer and customer/region could be obtained by simply running a filter query on the notifications cache. This example is all about uses of key association though, so the method used here will be slightly more complex, but it will show how key association can be used for reading entries as well as updating entries. Reading notifications could be implemented using an entry processor, which is invoked against the customer cache, that then returns the required notifications, either all for the customer or for a specific region. An entry processor is typically used for mutations and will cause an entry (or entries) to be locked for the duration of its execution. For read operations an aggregator is more efficient as it will not involve locking entries. To recap the use case, the aggregator needs to return either all the notifications for a customer, or just the notifications for a region. At this point a custom aggregator could be written, but sometimes writing aggregators can be complex and Coherence already has an aggregator that does most of what is required. The ReducerAggregator Coherence contains a built-in aggregator named com.tangosol.util.aggregator.ReducerAggregator . This aggregator takes a ValueExtractor and executes it against each entry and returns the results. The results returned will be a map of with the keys of the entries the aggregator ran over and the extracted values. By using the ReducerAggregator aggregator in this use case all that is required is a custom ValueExtractor . In this example the aggregator will only be run against a single entry (the customer) and the custom ValueExtractor will \"extract\" the required notifications. The NotificationExtractor The purpose of the custom ValueExtractor will be to obtain the notifications for a customer. The notifications are all co-located in a single partition, so when the extractor is run against an entry in the customer cache, all the notifications are also stored locally. This particular ValueExtract is going to need access to the entry the aggregator is executing on, so it needs to extend the Coherence com.tangosol.util.extractor.AbstractExtractor class. The AbstractExtractor is treated as a special case by Coherence when it is extracting data from a cache entry, where Coherence will call its extractFromEntry method. The boilerplate code for a custom extractor is shown below. All ValueExtractor implementations should have a correct equals() and hashCode() methods. The extractFromEntry method returns null , and will be completed in the next section. <markup lang=\"java\" title=\"NotificationExtractor.java\" >@PortableType(id = 1200, version = 1) public class NotificationExtractor extends AbstractExtractor&lt;Customer, List&lt;Notification&gt;&gt; { /** * An optional region identifier to use to retrieve * only notifications for a specific region. */ private String region; /** * Create a {@link NotificationExtractor} that will specifically * target the key when used to extract from a cache entry. * * @param region an optional region identifier */ public NotificationExtractor(String region) { this.region = region; } @Override public List&lt;Notification&gt; extractFromEntry(Map.Entry entry) { return null; } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } if (!super.equals(o)) { return false; } NotificationExtractor that = (NotificationExtractor) o; return Objects.equals(region, that.region); } @Override public int hashCode() { return Objects.hash(super.hashCode(), region); } Find the Customer&#8217;s Notifications When the extractor&#8217;s extractFromEntry method executes, in this case the entry passed in by the aggregator will be an instance of BinaryEntry , so just like in the entry processor above, the BackingMapContext for the notifications cache can be obtained and from there access to the notification entries. Coherence does not currently have an API on a BackingMapContext that allows the data to be queried. For example, in this case some sort of filter query over all the entries in the partition with a specific customer id would get the notification required. This can be worked around by using cache indexes. The indexes on a cache are accessible via the BackingMapContext and from the index contents the required cache entries can be obtained. Take the first requirement, all notifications for a customer. By creating an index of customer id on the notifications cache, the keys of the entries for a given customer can be obtained from the index and the corresponding notifications returned from the extractor. Customer Id Index Indexes are created on a cache using a ValueExtractor to extract the values to be indexed. In the case of the customer id for a notification, this is a field in the NotificationId , which is the key to the notifications cache. An extractor to extract customer id can be created as shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); This extractor can be used as an index by calling the addIndex method on NamedCache or NamedMap . <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); notifications.addIndex(extractor); The Region Index The second index required is to be able to find notifications for a customer and region. In theory this index is not required, the index to find all notifications for a customer could be used, then those notifications filtered to only return those for the required region. If there will only be a small number of notifications per customer, that may be a suitable approach. This is one of the typical pros and cons that needs to be weighed up when using indexes. Does the cost in memory usage of the index and time to maintain the index on every mutation outweigh the benefits in speed gained by queries. This example is going to add an index on region, because it is an example there are no concerns over performance, and it will show how to perform an indexed query. The extractor to extract region from the NotificationId cache entry key is shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); This can be used to create an index: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); notifications.addIndex(extractor); Creating the Indexes The repository class already has a method that is called to create any required indexes when it is initialized. This method can be overridden and used to ensure the notifications indexes are added. <markup lang=\"java\" title=\"CustomerRepository.java\" > @Override @SuppressWarnings( {\"unchecked\", \"resource\"}) protected void createIndices() { super.createIndices(); CacheService service = customers.getService(); NamedCache&lt;NotificationId, Notification&gt; notifications = service.ensureCache(NOTIFICATIONS_MAP_NAME, service.getContextClassLoader()); notifications.addIndex(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); notifications.addIndex(ValueExtractor.of(NotificationId::getRegion).fromKey()); } Note, that the super class createIndicies() method must be called to ensure any other indicies required by the customer repository are created. Write the NotificationExtractor extractFromEntry method Now that the required indexes will be present the NotificationExtractor.extractFromEntry() method can be written. The techniques used below rely on the indexes being present and would not work if there were no indexes. Without indexes other less efficient methods would be required. The steps the extract method must perform are shown below: Obtain the map of indexes for the notifications cache From the index map, obtain the customer id index From the customer id index obtain the set of notification keys matching the customer id If the region is specified, reduce the set of keys to only those matching the required region For each remaining key, obtain the read-only cache entry containing the notification and add it to the results list return the list of notifications found Each step is covered in detail below: Obtain the map of indexes for the notifications cache The entry passed to the NotificationExtractor.extractFromEntry method when used in an aggregator will be an instance of a BinaryEntry so the entry can safely be cast to BinaryEntry . From a BinaryEntry it is possible to obtain the BackingMapManagerContext and from there the BackingMapContext of other caches. Remember, in this example the aggregator is executed on an entry in the customers cache, so the extractor needs to obtain the BackingMapContext of the notifications cache. From the notifications cache BackingMapContext the map of indexes can be obtained. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); From the index map, obtain the customer id index The index map is a map of MapIndex instances keyed by the ValueExtractor used to create the index. To obtain the customer id index just call the get() method using the same customer id extractor used to create the index above. This is one of the main reasons that all ValueExtractor implementations must properly implement equals() and hashCode() so that they can be used in indexes. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); From the customer id index obtain the set of notification keys matching the customer id A Coherence MapIndex typically holds two internal indexes. The keys in the index are in serialized binary format, that is, they can be used directly to obtain corresponding entries. A map of cache key to the extracted index value for that key A map of extracted index value to the set of keys that match that value In the case of the customer id index that means the index holds a map of binary key to corresponding customer id and a map of customer id to keys of entries for that customer id. The second map is the one required for this use case, which can be obtained from the MapIndex.getIndexContents() method. The set of keys for the customer can then be obtained with a simple get(customerId) on the index contents map (the customer id is just the key of the entry passed to the extractFromEntry method. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); At this point the keys set is the key of all the notification entries for the customer. Further Filter by Region If the region has been specified, the set of keys needs to be further filtered to just those for the required region. This could be achieved a number of ways, but this example is going to show how Coherence filters and indexes can be used to reduce a set of keys. Almost all filters in Coherence implement IndexAwareFilter which means they have an applyIndex method: <markup lang=\"java\" >public &lt;RK&gt; Filter&lt;V&gt; applyIndex( Map&lt;? extends ValueExtractor&lt;? extends V, Object&gt;, ? extends MapIndex&lt;? extends RK, ? extends V, Object&gt;&gt; mapIndexes, Set&lt;? extends RK&gt; setKeys); When the applyIndex method is called, the Set of keys passed in will be reduced to only those keys matching the filter. This means that an EqualsFilter using the region extractor can be used to reduce the set of all keys for the customer down to just those keys matching the region too. Again, the extractor used in the EqualsFilter must be the same extractor used to create the region index. <markup lang=\"java\" >if (region != null &amp;&amp; !region.isBlank()) { ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Now the keys set has been reduced to only key matching both customer id and region. Obtain the Notifications The set of keys can be used to obtain notification from the notifications cache. The safest way to do this is to use the BackingMapContext.getReadOnlyEntry() method. The final list of notifications will be ordered by creation data. This is possible because the NotificationId class used in this example implements Comparable and makes use of the fact that the Coherence UUID used as a unique id in the notification contains a timestamp. The example used Java streams to process the keys into a list of notifications, the code is shown below: <markup lang=\"java\" > Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2)-&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); The key is mapped to a read-only InvocableMap.Entry Only process entries that are present for the key (in case it has just been removed) Sort the entries using the comparator to sort by key (i.e. NotificationId ) Map the entry to just the value (i.e. the Notification ) Cast the value to a Notification (this is because Java does not know the InvocableMap.Entry generic types) Collect the final Notification instances into a list The Final Method All the code above can be combined into the final extractFromEntry() method. <markup lang=\"java\" > @Override @SuppressWarnings( {\"rawtypes\", \"unchecked\"}) public List&lt;Notification&gt; extractFromEntry(Map.Entry entry) { BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext() .getBackingMapContext(CustomerRepository.NOTIFICATIONS_MAP_NAME); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(binaryEntry.getKeyPartition()); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); if (keys == null || keys.isEmpty()) { return Collections.emptyList(); } if (region != null &amp;&amp; !region.isBlank()) { // copy the keys, so we don't modify the underlying index keys = new HashSet&lt;&gt;(keys); ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2)-&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); } Note Looking at the source code, or JavaDoc, for BackingMapContext will show the getBackingMap() method, which returns the actual map of Binary keys and values in the cache; it should also be obvious that this method is deprecated. It may seem like this is a good way to access the data in the cache for the use case above, but directly accessing the data this way can break the guarantees and locks provided by Coherence. Ideally this method would have been removed, but backwards compatibility constraints mean it is still there, but it should not be used. Add Get Notification Methods to the CustomerRepository Now the NotificationExtractor is complete, methods can now be added to the CustomerRepository to get notifications for a customer and optionally a region. <markup lang=\"java\" title=\"CustomerRepository.java\" > /** * Returns the notifications for a customer. * * @param customerId the identifier of the customer to obtain the notifications for * * @return the notifications for the customer */ public List&lt;Notification&gt; getNotifications(String customerId) { return getNotifications(customerId, null); } /** * Returns the notifications for a customer, and optionally a region. * * @param customerId the identifier of the customer to obtain the notifications for * @param region an optional region to get notifications for * * @return the notifications for the customer, optionally restricted to a region */ public List&lt;Notification&gt; getNotifications(String customerId, String region) { Map&lt;String, List&lt;Notification&gt;&gt; map = getAll(List.of(customerId), new NotificationExtractor(region)); return map.getOrDefault(customerId, Collections.emptyList()); } The getNotifications() method calls the getAll() method on the AbstractRepository super class, which takes a collection of keys and a ValueExtractor . Under the covers, the AbstractRepository.getAll() method just runs a ReducerAggregator with the provided ValueExtractor after ensuring the repository is properly initialized. The map of results returned by getAll() will only ever contain a single entry, as it is only ever called here with a singleton list of keys. The result map will be a map of customer id to a list of notifications. ",
            "title": "Getting Notifications"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " A question often asked about Coherence is whether it can support joins in queries like a database, the answer is that it does not. Efficiently performing distributed joins in queries is extremely difficult to do, and typically data ends up being pulled back to a single member where it is joined. Using key association to guarantee associated data is in a single partition can be used to implement join type aggregations across those related entities. These techniques have been used by customers to implement quite complex join and data enrichment queries in large Coherence applications. ",
            "title": "A Poor Man&#8217;s Join"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The examples above show just some uses of key association in Coherence. It can be quite a powerful concept if used wisely. There are some downsides, mainly in cases where the amount of associated data is not very even. For example, in the use case above, if some customers has a very large number of notifications, all those would be stored in single partition. This can lead to some partitions and hence some cluster members using a larger amount of memory than others. Generally in a Coherence cache, keys are reasonably evenly distributed over partitions and cache entry sizes are relatively consistent so uneven memory usage is not an issue, but when using key association it is something to be aware of. ",
            "title": "Summary"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " The default behavior of the Coherence Gradle Plugin, can be customized using several optional properties. Simply provide a coherencePof closure to your build.gradle script containing any additional configuration properties, e.g.: <markup lang=\"groovy\" title=\"Build.gradle\" >coherencePof { debug=true } This will instruct Coherence to provide more logging output in regard to the instrumented classes ",
            "title": "Custom Configuration"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " Set the boolean debug property to true in order to instruct the underlying PortableTypeGenerator to generate debug code in regards the instrumented classes. If not specified, this property defaults to false . ",
            "title": "Enable Debugging"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " Set the boolean instrumentTestClasses property to true in order to instrument test classes. If not specified, this property defaults to false . ",
            "title": "Instrumentation of Test Classes"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " Provide a path to a custom test classes directory using property testClassesDirectory . If not set, it will default to the default test output directory. ",
            "title": "Set a Custom TestClassesDirectory"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " Provide a path to a custom classes directory using property mainClassesDirectory . If not set, it will default to the default output directory. ",
            "title": "Set a Custom MainClassesDirectory"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " Enable Debugging Set the boolean debug property to true in order to instruct the underlying PortableTypeGenerator to generate debug code in regards the instrumented classes. If not specified, this property defaults to false . Instrumentation of Test Classes Set the boolean instrumentTestClasses property to true in order to instrument test classes. If not specified, this property defaults to false . Set a Custom TestClassesDirectory Provide a path to a custom test classes directory using property testClassesDirectory . If not set, it will default to the default test output directory. Set a Custom MainClassesDirectory Provide a path to a custom classes directory using property mainClassesDirectory . If not set, it will default to the default output directory. ",
            "title": "Available Configuration Properties"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " In some cases, it may be necessary to expand the type system with the types that are not annotated with the @PortableType annotation, and are not discovered automatically. This is typically the case when some of your portable types have enum values, or existing classes that implement the PortableObject interface explicitly as attributes. You can add those types to the schema by creating a META-INF/schema.xml file and specifying them explicitly. For example, if you assume that the Color class from the earlier code examples: <markup lang=\"xml\" title=\"META-INF/schema.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;schema xmlns=\"http://xmlns.oracle.com/coherence/schema\" xmlns:java=\"http://xmlns.oracle.com/coherence/schema/java\" external=\"true\"&gt; &lt;type name=\"Color\"&gt; &lt;java:type name=\"petstore.Color\"/&gt; &lt;/type&gt; &lt;/schema&gt; ",
            "title": "What about classes without the @PortableType annotation?"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " In order to use the POF Gradle Plugin, you need to declare it as a plugin dependency in your build.gradle file. Furthermore, you should declare your Coherence dependency: <markup lang=\"groovy\" >plugins { id 'java' id 'com.oracle.coherence.ce' version '24.03' } &#8230;&#8203; dependencies { &#8230;&#8203; implementation 'com.oracle.coherence.ce:coherence:24.09.1' } Without any further configuration, the plugin will add a task named coherencePof to your project and you will see the task listed under the task group Coherence when you execute: <markup lang=\"bash\" >gradle tasks The coherencePof task will use the output of the compileJava task as input. As such the coherencePof task will depend on the compileJava task. Executing: <markup lang=\"bash\" >gradle compileJava will NOT execute the coherencePof task but on the other hand executing: <markup lang=\"bash\" >gradle coherencePof will execute the compileJava task first. By default , the coherencePof task will take the build output as input for classes to be instrumented excluding any test classes. The POF Gradle Plugin supports incremental builds . This means that only if Java classes have changed, the coherencePof task will execute (and only for the changed classes). The coherencePof task will also become a dependency to all tasks that depend on the compileJava . Thefore, executing the build or jar task will invoke the coherencePof task in case of class changes. By just adding the plugin using the configuration above, the Coherence Gradle Plugin will discover and instrument all project classes annotated with the @PortableType annotation, excluding test classes. If you do need to instrument test classes, you can add the coherencePof closure and provide additional configuration properties. Custom Configuration The default behavior of the Coherence Gradle Plugin, can be customized using several optional properties. Simply provide a coherencePof closure to your build.gradle script containing any additional configuration properties, e.g.: <markup lang=\"groovy\" title=\"Build.gradle\" >coherencePof { debug=true } This will instruct Coherence to provide more logging output in regard to the instrumented classes Available Configuration Properties Enable Debugging Set the boolean debug property to true in order to instruct the underlying PortableTypeGenerator to generate debug code in regards the instrumented classes. If not specified, this property defaults to false . Instrumentation of Test Classes Set the boolean instrumentTestClasses property to true in order to instrument test classes. If not specified, this property defaults to false . Set a Custom TestClassesDirectory Provide a path to a custom test classes directory using property testClassesDirectory . If not set, it will default to the default test output directory. Set a Custom MainClassesDirectory Provide a path to a custom classes directory using property mainClassesDirectory . If not set, it will default to the default output directory. What about classes without the @PortableType annotation? In some cases, it may be necessary to expand the type system with the types that are not annotated with the @PortableType annotation, and are not discovered automatically. This is typically the case when some of your portable types have enum values, or existing classes that implement the PortableObject interface explicitly as attributes. You can add those types to the schema by creating a META-INF/schema.xml file and specifying them explicitly. For example, if you assume that the Color class from the earlier code examples: <markup lang=\"xml\" title=\"META-INF/schema.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;schema xmlns=\"http://xmlns.oracle.com/coherence/schema\" xmlns:java=\"http://xmlns.oracle.com/coherence/schema/java\" external=\"true\"&gt; &lt;type name=\"Color\"&gt; &lt;java:type name=\"petstore.Color\"/&gt; &lt;/type&gt; &lt;/schema&gt; ",
            "title": "Usage"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " The portable type discovery feature of Coherence can use index files to speed up the discovery of @PortableType annotated classes. By default, at compile time, the Gradle plugin will generate index files under META-INF/pod.idx that contain class names of @PortableType annotated classes. You can skip the generation of those index files by setting the indexPofClasses property in your Gradle plugin configuration to false . ",
            "title": "Generating POF Index Files"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " You can skip the execution of the coherencePof task by running the Gradle build using the -x flag, e.g.: <markup lang=\"bash\" >gradle clean build -x coherencePof ",
            "title": "Skip Execution"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " An example Person class (below) when processed with the plugin, results in the bytecode shown below. <markup lang=\"java\" title=\"Person.java\" >@PortableType(id=1000) public class Person { public Person() {} public Person(int id, String name, Address address) { super(); this.id = id; this.name = name; this.address = address; } int id; String name; Address address; // getters and setters omitted for brevity } Let&#8217;s inspect the generated bytecode: <markup lang=\"bash\" >javap Person.class This should yield the following output: <markup lang=\"java\" >public class demo.Person implements com.tangosol.io.pof.PortableObject,com.tangosol.io.pof.EvolvableObject { int id; java.lang.String name; demo.Address address; public demo.Person(); public demo.Person(int, java.lang.String, demo.Address); public int getId(); public void setId(int); public java.lang.String getName(); public void setName(java.lang.String); public demo.Address getAddress(); public void setAddress(demo.Address); public java.lang.String toString(); public int hashCode(); public boolean equals(java.lang.Object); public void readExternal(com.tangosol.io.pof.PofReader) throws java.io.IOException; public void writeExternal(com.tangosol.io.pof.PofWriter) throws java.io.IOException; public com.tangosol.io.Evolvable getEvolvable(int); public com.tangosol.io.pof.EvolvableHolder getEvolvableHolder(); } Additional methods generated by Coherence POF plugin. Skip Execution You can skip the execution of the coherencePof task by running the Gradle build using the -x flag, e.g.: <markup lang=\"bash\" >gradle clean build -x coherencePof ",
            "title": "Example"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " During development, it is extremely useful to rapidly test the plugin code against separate example projects. For this, we can use Gradle&#8217;s composite build feature. Therefore, the Coherence POF Gradle Plugin module itself provides a separate sample module. From within the sample directory you can execute: <markup lang=\"bash\" >gradle clean compileJava --include-build ../plugin This will not only build the sample but will also build the plugin and developers can make plugin code changes and see changes rapidly reflected in the execution of the sample module. Alternatively, you can build and install the Coherence Gradle plugin to your local Maven repository using: <markup lang=\"bash\" >gradle publishToMavenLocal For projects to pick up the local changes ensure the following configuration: <markup lang=\"groovy\" title=\"Build.gradle\" >plugins { id 'java' id 'com.oracle.coherence.ce' version '24.03' } <markup lang=\"groovy\" title=\"Settings.gradle\" >pluginManagement { repositories { mavenLocal() gradlePluginPortal() } } ",
            "title": "Development"
        },
        {
            "location": "/docs/core/04_gradle",
            "text": " The POF Gradle Plugin provides automated instrumentation of classes with the @PortableType annotation to generate consistent (and correct) implementations of Evolvable POF serialization methods. It is a far from a trivial exercise to manually write serialization methods that support serializing inheritance hierarchies that support the Evolvable concept. However, with static type analysis these methods can be deterministically generated. This allows developers to focus on business logic rather than implementing boilerplate code for the above-mentioned methods. Please see Portable Types documentation for more information and detailed instructions on Portable Types creation and usage. Usage In order to use the POF Gradle Plugin, you need to declare it as a plugin dependency in your build.gradle file. Furthermore, you should declare your Coherence dependency: <markup lang=\"groovy\" >plugins { id 'java' id 'com.oracle.coherence.ce' version '24.03' } &#8230;&#8203; dependencies { &#8230;&#8203; implementation 'com.oracle.coherence.ce:coherence:24.09.1' } Without any further configuration, the plugin will add a task named coherencePof to your project and you will see the task listed under the task group Coherence when you execute: <markup lang=\"bash\" >gradle tasks The coherencePof task will use the output of the compileJava task as input. As such the coherencePof task will depend on the compileJava task. Executing: <markup lang=\"bash\" >gradle compileJava will NOT execute the coherencePof task but on the other hand executing: <markup lang=\"bash\" >gradle coherencePof will execute the compileJava task first. By default , the coherencePof task will take the build output as input for classes to be instrumented excluding any test classes. The POF Gradle Plugin supports incremental builds . This means that only if Java classes have changed, the coherencePof task will execute (and only for the changed classes). The coherencePof task will also become a dependency to all tasks that depend on the compileJava . Thefore, executing the build or jar task will invoke the coherencePof task in case of class changes. By just adding the plugin using the configuration above, the Coherence Gradle Plugin will discover and instrument all project classes annotated with the @PortableType annotation, excluding test classes. If you do need to instrument test classes, you can add the coherencePof closure and provide additional configuration properties. Custom Configuration The default behavior of the Coherence Gradle Plugin, can be customized using several optional properties. Simply provide a coherencePof closure to your build.gradle script containing any additional configuration properties, e.g.: <markup lang=\"groovy\" title=\"Build.gradle\" >coherencePof { debug=true } This will instruct Coherence to provide more logging output in regard to the instrumented classes Available Configuration Properties Enable Debugging Set the boolean debug property to true in order to instruct the underlying PortableTypeGenerator to generate debug code in regards the instrumented classes. If not specified, this property defaults to false . Instrumentation of Test Classes Set the boolean instrumentTestClasses property to true in order to instrument test classes. If not specified, this property defaults to false . Set a Custom TestClassesDirectory Provide a path to a custom test classes directory using property testClassesDirectory . If not set, it will default to the default test output directory. Set a Custom MainClassesDirectory Provide a path to a custom classes directory using property mainClassesDirectory . If not set, it will default to the default output directory. What about classes without the @PortableType annotation? In some cases, it may be necessary to expand the type system with the types that are not annotated with the @PortableType annotation, and are not discovered automatically. This is typically the case when some of your portable types have enum values, or existing classes that implement the PortableObject interface explicitly as attributes. You can add those types to the schema by creating a META-INF/schema.xml file and specifying them explicitly. For example, if you assume that the Color class from the earlier code examples: <markup lang=\"xml\" title=\"META-INF/schema.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;schema xmlns=\"http://xmlns.oracle.com/coherence/schema\" xmlns:java=\"http://xmlns.oracle.com/coherence/schema/java\" external=\"true\"&gt; &lt;type name=\"Color\"&gt; &lt;java:type name=\"petstore.Color\"/&gt; &lt;/type&gt; &lt;/schema&gt; Generating POF Index Files The portable type discovery feature of Coherence can use index files to speed up the discovery of @PortableType annotated classes. By default, at compile time, the Gradle plugin will generate index files under META-INF/pod.idx that contain class names of @PortableType annotated classes. You can skip the generation of those index files by setting the indexPofClasses property in your Gradle plugin configuration to false . Example An example Person class (below) when processed with the plugin, results in the bytecode shown below. <markup lang=\"java\" title=\"Person.java\" >@PortableType(id=1000) public class Person { public Person() {} public Person(int id, String name, Address address) { super(); this.id = id; this.name = name; this.address = address; } int id; String name; Address address; // getters and setters omitted for brevity } Let&#8217;s inspect the generated bytecode: <markup lang=\"bash\" >javap Person.class This should yield the following output: <markup lang=\"java\" >public class demo.Person implements com.tangosol.io.pof.PortableObject,com.tangosol.io.pof.EvolvableObject { int id; java.lang.String name; demo.Address address; public demo.Person(); public demo.Person(int, java.lang.String, demo.Address); public int getId(); public void setId(int); public java.lang.String getName(); public void setName(java.lang.String); public demo.Address getAddress(); public void setAddress(demo.Address); public java.lang.String toString(); public int hashCode(); public boolean equals(java.lang.Object); public void readExternal(com.tangosol.io.pof.PofReader) throws java.io.IOException; public void writeExternal(com.tangosol.io.pof.PofWriter) throws java.io.IOException; public com.tangosol.io.Evolvable getEvolvable(int); public com.tangosol.io.pof.EvolvableHolder getEvolvableHolder(); } Additional methods generated by Coherence POF plugin. Skip Execution You can skip the execution of the coherencePof task by running the Gradle build using the -x flag, e.g.: <markup lang=\"bash\" >gradle clean build -x coherencePof Development During development, it is extremely useful to rapidly test the plugin code against separate example projects. For this, we can use Gradle&#8217;s composite build feature. Therefore, the Coherence POF Gradle Plugin module itself provides a separate sample module. From within the sample directory you can execute: <markup lang=\"bash\" >gradle clean compileJava --include-build ../plugin This will not only build the sample but will also build the plugin and developers can make plugin code changes and see changes rapidly reflected in the execution of the sample module. Alternatively, you can build and install the Coherence Gradle plugin to your local Maven repository using: <markup lang=\"bash\" >gradle publishToMavenLocal For projects to pick up the local changes ensure the following configuration: <markup lang=\"groovy\" title=\"Build.gradle\" >plugins { id 'java' id 'com.oracle.coherence.ce' version '24.03' } <markup lang=\"groovy\" title=\"Settings.gradle\" >pluginManagement { repositories { mavenLocal() gradlePluginPortal() } } ",
            "title": "Gradle POF Plugin"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " This example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " In this example you will utilize streams methods on the NamedMap API to query and aggregate and group data from a contacts NamedMap . What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " Contacts have various attributes as described below including home and work addresses stored in the Address class. <markup lang=\"java\" >public class Contact implements Serializable { private int id; private String firstName; private String lastName; private LocalDate doB; private int age; private Address homeAddress; private Address workAddress; ",
            "title": "Contact"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " Address contains address details for a Contact . <markup lang=\"java\" >public class Address implements Serializable { private String addressLine1; private String addressLine2; private String city; private String state; private String zip; ",
            "title": "Address"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " The data model consists of the following classes in two maps, customers and orders Contact - Represents a contact Address - Represents an address for a contact Contact Contacts have various attributes as described below including home and work addresses stored in the Address class. <markup lang=\"java\" >public class Contact implements Serializable { private int id; private String firstName; private String lastName; private LocalDate doB; private int age; private Address homeAddress; private Address workAddress; Address Address contains address details for a Contact . <markup lang=\"java\" >public class Address implements Serializable { private String addressLine1; private String addressLine2; private String city; private String state; private String zip; ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " Example Details The runExample() method contains the code that exercises the streams API. Refer to the inline code comments for explanations of what each operation is carrying out. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { NamedMap&lt;Integer, Contact&gt; contacts = getContacts(); System.out.println(\"Cache size is \" + contacts.size()); // get the distinct years that the contacts were born in Set&lt;Integer&gt; setYears = contacts.stream(Contact::getDoB) .map(LocalDate::getYear) .distinct() .collect(RemoteCollectors.toSet()); System.out.println(\"Distinct years the contacts were born in:\" + setYears); // get a set of contact names where the age is &gt; 40 Set&lt;String&gt; setNames = contacts.stream(greater(Contact::getAge, 60)) .map(entry-&gt;entry.extract(Contact::getLastName) + \" \" + entry.extract(Contact::getFirstName) + \" age=\" + entry.extract(Contact::getAge)) .collect(RemoteCollectors.toSet()); System.out.println(\"Set of contact names where age &gt; 60:\" + setNames); // get the distinct set of states for home addresses Set&lt;String&gt; setStates = contacts.stream(Contact::getHomeAddress) .map(Address::getState) .distinct() .collect(RemoteCollectors.toSet()); System.out.println(\"Distinct set of states for home addresses:\" + setStates); // get the average ages of all contacts double avgAge = contacts.stream(Contact::getAge) .mapToInt(Number::intValue) .average() .orElse(0); // in-case of no values System.out.println(\"The average age of all contacts is: \" + avgAge); // get average age using collectors avgAge = contacts.stream() .collect(RemoteCollectors.averagingInt(Contact::getAge)); System.out.println(\"The average age of all contacts using collect() is: \" + avgAge); // get the maximum age of all contacts int maxAge = contacts.stream(Contact::getAge) .mapToInt(Number::intValue) .max() .orElse(0); // in-case of no values System.out.println(\"The maximum age of all contacts is: \" + maxAge); // get average age of contacts who live in MA // Note: The filter should be applied as early as possible, e.g as an argument // to the stream() call in order to take advantage of indexes avgAge = RemoteStream.toIntStream(contacts.stream(equal(homeState(), \"MA\"), Contact::getAge)) .average() .orElse(0); System.out.println(\"The average age of contacts who work in MA is: \" + avgAge); // get a map of birth months and the contact names for that month Map&lt;String, List&lt;Contact&gt;&gt; mapContacts = contacts.stream() .map(Map.Entry::getValue) .collect(RemoteCollectors.groupingBy(birthMonth())); System.out.println(\"Contacts born in each month:\"); mapContacts.forEach( (key, value)-&gt;System.out.println(\"Month: \" + key + \", Contacts:\" + displayNames(value))); // get a map of states and the contacts living in each state Map&lt;String, List&lt;Contact&gt;&gt; mapStateContacts = contacts.stream() .map(Map.Entry::getValue) .collect(RemoteCollectors.groupingBy(homeState())); System.out.println(\"Contacts with home addresses in each state:\"); mapStateContacts.forEach( (key, value)-&gt;System.out.println(\"State \" + key + \" has \" + value.size() + \" Contacts\" + displayNames(value))); } The following static extractors are referenced in the above example: <markup lang=\"java\" >/** * A {@link ValueExtractor} to extract the birth month from a {@link Contact}. * * @return the birth month */ protected static ValueExtractor&lt;Contact, String&gt; birthMonth() { return contact-&gt;contact.getDoB().getMonth().toString(); } /** * A {@link ValueExtractor} to extract the home state from a {@link Contact}. * * @return the home state */ protected static ValueExtractor&lt;Contact, String&gt; homeState() { return contact-&gt;contact.getHomeAddress().getState(); } ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " Carry out the following to run this example: E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following: (output is truncated) <markup lang=\"bash\" >Creating 100 customers Cache size is 100 Distinct years the contacts were born in: [1984, 1985, 1986, 1987, 1989, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1979, 1980, 1981, 1983] Set of contact names where age &gt; 60: [Lastname12 Firstname12 age=64, Lastname100 Firstname100 age=70, Lastname77 Firstname77 age=63, Lastname82 Firstname82 age=66, Lastname45 Firstname45 age=71, Lastname84 Firstname84 age=63, Lastname40 Firstname40 age=62, Lastname20 Firstname20 age=68, Lastname63 Firstname63 age=68, Lastname85 Firstname85 age=69, ... truncated ... Lastname96 Firstname96 age=61, Lastname7 Firstname7 age=71, Lastname73 Firstname73 age=61, Lastname14 Firstname14 age=69, Lastname35 Firstname35 age=61 Distinct set of states for home addresses: [HI, TX, MA, TN, AK, WA, NY, AL, CA] The average age of all contacts is: 52.48 The average age of all contacts using collect() is: 52.48 The maximum age of all contacts is: 72 The average age of contacts who work in MA is: 46.666666666666664 Contacts born in each month: Month: JUNE, Contacts: Firstname77 Lastname77 Firstname38 Lastname38 Firstname32 Lastname32 Firstname91 Lastname91 Firstname48 Lastname48 Firstname92 Lastname92 Firstname80 Lastname80 Firstname34 Lastname34 Month: JANUARY, Contacts: Firstname47 Lastname47 Firstname94 Lastname94 Firstname16 Lastname16 Firstname46 Lastname46 Firstname57 Lastname57 Firstname10 Lastname10 Firstname100 Lastname100 Firstname4 Lastname4 Month: MAY, Contacts: Firstname65 Lastname65 Firstname55 Lastname55 Firstname1 Lastname1 Firstname93 Lastname93 Firstname96 Lastname96 Firstname42 Lastname42 Firstname14 Lastname14 Firstname25 Lastname25 Firstname54 Lastname54 ... truncated ... Month: APRIL, Contacts: Firstname59 Lastname59 Firstname15 Lastname15 Firstname90 Lastname90 Firstname50 Lastname50 Firstname45 Lastname45 Firstname33 Lastname33 Firstname76 Lastname76 Firstname23 Lastname23 Contacts with home addresses in each state: State HI has 6 Contacts Firstname32 Lastname32 Firstname68 Lastname68 Firstname17 Lastname17 Firstname42 Lastname42 Firstname18 Lastname18 Firstname39 Lastname39 State TX has 13 Contacts Firstname71 Lastname71 Firstname30 Lastname30 Firstname82 Lastname82 Firstname62 Lastname62 Firstname40 Lastname40 Firstname43 Lastname43 Firstname93 Lastname93 Firstname11 Lastname11 Firstname92 Lastname92 Firstname96 Lastname96 Firstname7 Lastname7 Firstname58 Lastname58 Firstname76 Lastname76 ... truncated ... State AL has 10 Contacts Firstname47 Lastname47 Firstname46 Lastname46 Firstname22 Lastname22 Firstname66 Lastname66 Firstname81 Lastname81 Firstname15 Lastname15 Firstname25 Lastname25 Firstname35 Lastname35 Firstname34 Lastname34 Firstname89 Lastname89 State CA has 14 Contacts Firstname77 Lastname77 Firstname61 Lastname61 Firstname28 Lastname28 Firstname5 Lastname5 Firstname1 Lastname1 Firstname91 Lastname91 Firstname87 Lastname87 Firstname79 Lastname79 Firstname80 Lastname80 Firstname12 Lastname12 Firstname33 Lastname33 Firstname95 Lastname95 Firstname98 Lastname98 Firstname100 Lastname100 ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " In this example you have seen how to utilized streams methods on the NamedMap API to query and aggregate and group data from a contacts NamedMap . ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " Built in Aggregators Custom Aggregators ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " This guide walks you through how to use the Streams API with Coherence. The Java streams implementation provides an efficient way to query and process data sequentially or in parallel to take advantage of multi-core architectures. The processing occurs in steps: Data is aggregated from a source (such as collections or arrays) into a read-only stream. The stream represents object references and does not actually store the data. Intermediate operations are then declared on the stream. Intermediate operations for filtering, sorting, mapping, and so on are supported. Lambda expressions are often used when declaring intermediate operations and provide a functional way to work on the data. Intermediate operations are aggregated and can be chained together: each subsequent operation is performed on a stream that contains the result of the previous operation. Intermediate operations are lazy and are not actually executed until a final terminal operation is performed. A final terminal operation is declared. Terminal operations for counting, adding, averaging, and so on are supported. The terminal operation automatically iterates over the objects in the stream returns an aggregated result. Java streams provide similar functionality as Coherence data grid aggregation. However, streams are not efficient when executed in a distributed environment. To leverage the stream programming model and also ensure that streams can be executed remotely across the cluster, Coherence has extended the streams API. For details see the com.tangosol.util.stream in the Java API Reference for Oracle Coherence. Table of Contents What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also What You Will Build In this example you will utilize streams methods on the NamedMap API to query and aggregate and group data from a contacts NamedMap . What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Example Data Model The data model consists of the following classes in two maps, customers and orders Contact - Represents a contact Address - Represents an address for a contact Contact Contacts have various attributes as described below including home and work addresses stored in the Address class. <markup lang=\"java\" >public class Contact implements Serializable { private int id; private String firstName; private String lastName; private LocalDate doB; private int age; private Address homeAddress; private Address workAddress; Address Address contains address details for a Contact . <markup lang=\"java\" >public class Address implements Serializable { private String addressLine1; private String addressLine2; private String city; private String state; private String zip; Review the Example Code Example Details The runExample() method contains the code that exercises the streams API. Refer to the inline code comments for explanations of what each operation is carrying out. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { NamedMap&lt;Integer, Contact&gt; contacts = getContacts(); System.out.println(\"Cache size is \" + contacts.size()); // get the distinct years that the contacts were born in Set&lt;Integer&gt; setYears = contacts.stream(Contact::getDoB) .map(LocalDate::getYear) .distinct() .collect(RemoteCollectors.toSet()); System.out.println(\"Distinct years the contacts were born in:\" + setYears); // get a set of contact names where the age is &gt; 40 Set&lt;String&gt; setNames = contacts.stream(greater(Contact::getAge, 60)) .map(entry-&gt;entry.extract(Contact::getLastName) + \" \" + entry.extract(Contact::getFirstName) + \" age=\" + entry.extract(Contact::getAge)) .collect(RemoteCollectors.toSet()); System.out.println(\"Set of contact names where age &gt; 60:\" + setNames); // get the distinct set of states for home addresses Set&lt;String&gt; setStates = contacts.stream(Contact::getHomeAddress) .map(Address::getState) .distinct() .collect(RemoteCollectors.toSet()); System.out.println(\"Distinct set of states for home addresses:\" + setStates); // get the average ages of all contacts double avgAge = contacts.stream(Contact::getAge) .mapToInt(Number::intValue) .average() .orElse(0); // in-case of no values System.out.println(\"The average age of all contacts is: \" + avgAge); // get average age using collectors avgAge = contacts.stream() .collect(RemoteCollectors.averagingInt(Contact::getAge)); System.out.println(\"The average age of all contacts using collect() is: \" + avgAge); // get the maximum age of all contacts int maxAge = contacts.stream(Contact::getAge) .mapToInt(Number::intValue) .max() .orElse(0); // in-case of no values System.out.println(\"The maximum age of all contacts is: \" + maxAge); // get average age of contacts who live in MA // Note: The filter should be applied as early as possible, e.g as an argument // to the stream() call in order to take advantage of indexes avgAge = RemoteStream.toIntStream(contacts.stream(equal(homeState(), \"MA\"), Contact::getAge)) .average() .orElse(0); System.out.println(\"The average age of contacts who work in MA is: \" + avgAge); // get a map of birth months and the contact names for that month Map&lt;String, List&lt;Contact&gt;&gt; mapContacts = contacts.stream() .map(Map.Entry::getValue) .collect(RemoteCollectors.groupingBy(birthMonth())); System.out.println(\"Contacts born in each month:\"); mapContacts.forEach( (key, value)-&gt;System.out.println(\"Month: \" + key + \", Contacts:\" + displayNames(value))); // get a map of states and the contacts living in each state Map&lt;String, List&lt;Contact&gt;&gt; mapStateContacts = contacts.stream() .map(Map.Entry::getValue) .collect(RemoteCollectors.groupingBy(homeState())); System.out.println(\"Contacts with home addresses in each state:\"); mapStateContacts.forEach( (key, value)-&gt;System.out.println(\"State \" + key + \" has \" + value.size() + \" Contacts\" + displayNames(value))); } The following static extractors are referenced in the above example: <markup lang=\"java\" >/** * A {@link ValueExtractor} to extract the birth month from a {@link Contact}. * * @return the birth month */ protected static ValueExtractor&lt;Contact, String&gt; birthMonth() { return contact-&gt;contact.getDoB().getMonth().toString(); } /** * A {@link ValueExtractor} to extract the home state from a {@link Contact}. * * @return the home state */ protected static ValueExtractor&lt;Contact, String&gt; homeState() { return contact-&gt;contact.getHomeAddress().getState(); } Run the Example Carry out the following to run this example: E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following: (output is truncated) <markup lang=\"bash\" >Creating 100 customers Cache size is 100 Distinct years the contacts were born in: [1984, 1985, 1986, 1987, 1989, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1979, 1980, 1981, 1983] Set of contact names where age &gt; 60: [Lastname12 Firstname12 age=64, Lastname100 Firstname100 age=70, Lastname77 Firstname77 age=63, Lastname82 Firstname82 age=66, Lastname45 Firstname45 age=71, Lastname84 Firstname84 age=63, Lastname40 Firstname40 age=62, Lastname20 Firstname20 age=68, Lastname63 Firstname63 age=68, Lastname85 Firstname85 age=69, ... truncated ... Lastname96 Firstname96 age=61, Lastname7 Firstname7 age=71, Lastname73 Firstname73 age=61, Lastname14 Firstname14 age=69, Lastname35 Firstname35 age=61 Distinct set of states for home addresses: [HI, TX, MA, TN, AK, WA, NY, AL, CA] The average age of all contacts is: 52.48 The average age of all contacts using collect() is: 52.48 The maximum age of all contacts is: 72 The average age of contacts who work in MA is: 46.666666666666664 Contacts born in each month: Month: JUNE, Contacts: Firstname77 Lastname77 Firstname38 Lastname38 Firstname32 Lastname32 Firstname91 Lastname91 Firstname48 Lastname48 Firstname92 Lastname92 Firstname80 Lastname80 Firstname34 Lastname34 Month: JANUARY, Contacts: Firstname47 Lastname47 Firstname94 Lastname94 Firstname16 Lastname16 Firstname46 Lastname46 Firstname57 Lastname57 Firstname10 Lastname10 Firstname100 Lastname100 Firstname4 Lastname4 Month: MAY, Contacts: Firstname65 Lastname65 Firstname55 Lastname55 Firstname1 Lastname1 Firstname93 Lastname93 Firstname96 Lastname96 Firstname42 Lastname42 Firstname14 Lastname14 Firstname25 Lastname25 Firstname54 Lastname54 ... truncated ... Month: APRIL, Contacts: Firstname59 Lastname59 Firstname15 Lastname15 Firstname90 Lastname90 Firstname50 Lastname50 Firstname45 Lastname45 Firstname33 Lastname33 Firstname76 Lastname76 Firstname23 Lastname23 Contacts with home addresses in each state: State HI has 6 Contacts Firstname32 Lastname32 Firstname68 Lastname68 Firstname17 Lastname17 Firstname42 Lastname42 Firstname18 Lastname18 Firstname39 Lastname39 State TX has 13 Contacts Firstname71 Lastname71 Firstname30 Lastname30 Firstname82 Lastname82 Firstname62 Lastname62 Firstname40 Lastname40 Firstname43 Lastname43 Firstname93 Lastname93 Firstname11 Lastname11 Firstname92 Lastname92 Firstname96 Lastname96 Firstname7 Lastname7 Firstname58 Lastname58 Firstname76 Lastname76 ... truncated ... State AL has 10 Contacts Firstname47 Lastname47 Firstname46 Lastname46 Firstname22 Lastname22 Firstname66 Lastname66 Firstname81 Lastname81 Firstname15 Lastname15 Firstname25 Lastname25 Firstname35 Lastname35 Firstname34 Lastname34 Firstname89 Lastname89 State CA has 14 Contacts Firstname77 Lastname77 Firstname61 Lastname61 Firstname28 Lastname28 Firstname5 Lastname5 Firstname1 Lastname1 Firstname91 Lastname91 Firstname87 Lastname87 Firstname79 Lastname79 Firstname80 Lastname80 Firstname12 Lastname12 Firstname33 Lastname33 Firstname95 Lastname95 Firstname98 Lastname98 Firstname100 Lastname100 Summary In this example you have seen how to utilized streams methods on the NamedMap API to query and aggregate and group data from a contacts NamedMap . See Also Built in Aggregators Custom Aggregators ",
            "title": "Streams"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Simply Starting Coherence - Running Coherence as the main class. The Coherence Instance - Accessing and using the bootstrapped Coherence instance Ensure Coherence is Started - Obtaining a fully running Coherence instance Coherence Sessions - Obtaining Coherence Session instances and other Coherence resources Application Initialization - Initializing application code without needing a custom main class Bootstrap Coherence - Starting Coherence from Application Code Simple Cluster Member - Start a simple cluster member Configured Cluster Member - Configure and start a simple cluster member ",
            "title": "Contents"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Whether you are running a Coherence cluster member or client, you&#8217;ll need to configure and bootstrap Coherence. Coherence does not actually need any configuring or bootstrapping, you could just do something like CacheFactory.getCache(\"foo\"); , but when there is an alternative, static method calls to get Coherence resources are poor coding practice (especially when it comes to unit testing with mocks and stubs). Coherence CE v20.12 introduced a new bootstrap API for Coherence, which this guide is going to cover. Not only does the bootstrap API make it simpler to start Coherence, it makes some other uses cases simpler, for example where a client application needs to connect to multiple clusters. A number of the integrations between Coherence and application frameworks, such as Coherence Spring , Coherence Micronaut , Coherence CDI and Helidon , use the bootstrap API under the covers to initialize Coherence when using those frameworks. When using these types of \"DI\" frameworks, Coherence and Session instances and other Coherence resources can just be injected into application code without even needing to directly access the bootstrap API. Contents Simply Starting Coherence - Running Coherence as the main class. The Coherence Instance - Accessing and using the bootstrapped Coherence instance Ensure Coherence is Started - Obtaining a fully running Coherence instance Coherence Sessions - Obtaining Coherence Session instances and other Coherence resources Application Initialization - Initializing application code without needing a custom main class Bootstrap Coherence - Starting Coherence from Application Code Simple Cluster Member - Start a simple cluster member Configured Cluster Member - Configure and start a simple cluster member ",
            "title": "Bootstrap Coherence"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " This guide will look at some ways to bootstrap a Coherence application. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " A Coherence application is either a cluster member, or it is a client. Historically a client would be a Coherence*Extend client, but more recently Coherence has also introduced a gRPC client. Prior to CE v20.12, applications typically used Coherence in a couple of ways; either cluster members that started by running DefaultCacheServer , or by running a custom main class and obtaining Coherence resources directly from a Session or ConfigurableCacheFactory instance - possibly using static methods on com.tangosol.net.CacheFactory . By far the majority of applications only had a single ConfigurableCacheFactory instance, but occasionally an application would add more (for example an Extend client connecting to multiple cluster). Adding of additional ConfigurableCacheFactory required custom start-up code and management code. In an effort to make it possible to build more modular applications with multiple ConfigurableCacheFactory or Session instances a new bootstrap API was added. ",
            "title": "A Brief History"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " By default, Coherence will run as a storage enabled cluster member, unless Coherence system properties or environment variables have been used to override this. For example, when using the default coherence-cache-config.xml file from coherence.jar` it is possible to run `Coherence as an Extend client by setting the coherence.client system property (or COHERENCE_CLIENT environment variable) to a value of remote . <markup lang=\"bash\" >java -cp coherence.jar -Dcoherence.client=remote com.tangosol.net.Coherence ",
            "title": "Running Coherence as an Extend Client"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " The Coherence class is the main entry point into a Coherence application. A Coherence server can be started by simply running the Coherence.main() method. From Coherence CE v22.06, this is the default way that Coherence starts using java -jar coherence.jar . An important point when using the Coherence class to start Coherence is that this will automatically include starting some of the additional Coherence extensions if they are on the class path, or module path. For example, starting the Coherence health check http endpoints, or if the Coherence Concurrent module is on the class path, its services will automatically be started. The same applies to the Coherence gRPC server, Coherence metrics and Coherence REST management. <markup lang=\"bash\" >java -cp coherence.jar com.tangosol.net.Coherence <markup lang=\"bash\" >java -jar coherence.jar Or with Java modules <markup lang=\"bash\" >java -p coherence.jar -m com.oracle.coherence Functionally this is almost identical to the old way of running DefaultCacheServer , but will now use the new bootstrap API to configure and start Coherence. When run in this way Coherence will use the default configuration file coherence-cache-config.xml , either from coherence.jar or elsewhere on the classpath. The name of this configuration file can be overridden as normal with the coherence.cacheconfig system property. Running the Coherence class, or using the bootstrap API, will also start various system services, such as the health check http endpoints. Running Coherence as an Extend Client By default, Coherence will run as a storage enabled cluster member, unless Coherence system properties or environment variables have been used to override this. For example, when using the default coherence-cache-config.xml file from coherence.jar` it is possible to run `Coherence as an Extend client by setting the coherence.client system property (or COHERENCE_CLIENT environment variable) to a value of remote . <markup lang=\"bash\" >java -cp coherence.jar -Dcoherence.client=remote com.tangosol.net.Coherence ",
            "title": "Starting Coherence"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Sometimes, application code may need to ensure Coherence has fully started before running. A Coherence instance has a whenStarted() method that returns a CompletableFuture that will be completed when the Coherence instance has finished starting. The example below obtains the default Coherence instance and waits up to five minuts for the instance to be running. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance() .whenStarted() .get(5, TimeUnit.MINUTES); ",
            "title": "Ensure Coherence is Started"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Each Coherence instance will be running one or more uniquely named Session instances, depending on how it was configured. By running Coherence.main() the default Coherence instance will be running the default Session . A Session can be obtained from a Coherence instance using a number of methods. The example below obtains the default Coherence Session from the default Coherence instance. This method would be used if Coherence has been started using the default Coherence.main() method. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); A Session can also be obtained using its name. The example below obtains the Session named \"foo\". <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(\"foo\"); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); It is also possible to use the static Coherence.findSession() method to find a Session by name across all configured Coherence instances. This method returns an optional containing the Session or empty if no Session exists with the requested name. <markup lang=\"java\" > Optional&lt;Session&gt; optional = Coherence.findSession(\"foo\"); if (optional.isPresent()) { Session session = optional.get(); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); } ",
            "title": "Obtain a Coherence Session"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Once a Coherence instance has been started, using either the Coherence.main() method, or one of the other ways described below, application code can obtain the running Coherence instance and obtain a Coherence Session which can then be used to access Coherence resources such as NamedMap , NamedCache , NamedTopic etc. More than one Coherence instance can be running simultaneously (but in the case of a cluster member, all these instances will be a single cluster member, they are not able to be parts of separate clusters). Each Coherence instance has a unique name and can be accessed by name. If Coherence has been started using Coherence.main() there will be a single instance of Coherence with the default name. The simplest way to access the default Coherence instance is using the static accessor. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Coherence instances can also be obtained by name, the default instance&#8217;s name can be accessed using the static field Coherence.DEFAULT_NAME : <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(Coherence.DEFAULT_NAME); Ensure Coherence is Started Sometimes, application code may need to ensure Coherence has fully started before running. A Coherence instance has a whenStarted() method that returns a CompletableFuture that will be completed when the Coherence instance has finished starting. The example below obtains the default Coherence instance and waits up to five minuts for the instance to be running. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance() .whenStarted() .get(5, TimeUnit.MINUTES); Obtain a Coherence Session Each Coherence instance will be running one or more uniquely named Session instances, depending on how it was configured. By running Coherence.main() the default Coherence instance will be running the default Session . A Session can be obtained from a Coherence instance using a number of methods. The example below obtains the default Coherence Session from the default Coherence instance. This method would be used if Coherence has been started using the default Coherence.main() method. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); A Session can also be obtained using its name. The example below obtains the Session named \"foo\". <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(\"foo\"); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); It is also possible to use the static Coherence.findSession() method to find a Session by name across all configured Coherence instances. This method returns an optional containing the Session or empty if no Session exists with the requested name. <markup lang=\"java\" > Optional&lt;Session&gt; optional = Coherence.findSession(\"foo\"); if (optional.isPresent()) { Session session = optional.get(); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); } ",
            "title": "Using a Coherence Instance"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Sometimes an application needs to perform some initialization when it starts up. Before the new bootstrap API existed, this was a common reason for applications having to add a custom main class. The Coherence class has an inner interface LifecycleListener that application code can implement to be notified of Coherence start-up and shutdown events. Instances of LifecycleListener are automatically discovered by Coherence at runtime using the Java ServiceLoader , which means that an applications can be initialised without needing a custom main class, but instead by just implementing a LifecycleListener . This is particularly useful where an application is made up of modules that may or may not be on the class path or module path at runtime. A module just needs to implement a Coherence LifecycleListener as a Java service and whenever it is on the class path it will be initialized. For example, an application that needs to start a web-server could implement LifecycleListener as shown below. The STARTED event type is fired after a Coherence instance is started, the STOPPING event type is fired before a Coherence instance is stopped. <markup lang=\"java\" >import com.tangosol.net.Coherence; import com.tangosol.net.events.CoherenceLifecycleEvent; public class WebServerController implements Coherence.LifecycleListener { private final HttpServer server = new HttpServer(); @Override public void onEvent(CoherenceLifecycleEvent event) { switch (event.getType()) { case STARTED: server.start(); break; case STOPPING: server.stop(); break; } } } The event also contains the Coherence instance that raised the event, so this could then be used to obtain a Session and other Coherence resources that are needed as part of the application initialisation. Adding the WebServerController class above to a META-INF/services file or module-info file will make it discoverable by Coherence. <markup lang=\"java\" title=\"META_INF/services/com.tangosol.net.Coherence$LifecycleListener\" >com.oracle.coherence.guides.bootstrap.WebServerController; <markup lang=\"java\" title=\"module-info.java\" >open module com.oracle.coherence.guides.bootstrap { requires com.oracle.coherence; exports com.oracle.coherence.guides.bootstrap; provides com.tangosol.net.Coherence.LifecycleListener with com.oracle.coherence.guides.bootstrap.WebServerController; } ",
            "title": "Initialize Application Code"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " The simplest way to start Coherence as a cluster member in application code is shown below: <markup lang=\"java\" > Coherence coherence = Coherence.clusterMember(); coherence.start(); The start() method returns a CompletableFuture so application code that needs to wit for start-up to complete can use the future for this purpose. The example below ensures Coherence is started as a cluster member (waiting a maximum of five minutes) before proceeding. <markup lang=\"java\" > Coherence coherence = Coherence.clusterMember() .start() .get(5, TimeUnit.MINUTES); Running Coherence in this way will create a single Session using the default cache configuration file (or another file specified using the -Dcoherence.cacheconfig system property). By default, this will be a storage enabled cluster member, unless Coherence system properties or environment variables have been used to override this. ",
            "title": "Run a Simple Cluster Member"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " The bootstrap API allows the Coherence instance to be configured before starting, for example adding one or more session configurations. In the example below, a Coherence cluster member instance is created using a configuration. The configuration in this case does not specify a name, so the default name will be used. The configuration contains two Session configurations. The first is named \"foo\" and uses the cache configuration loaded from foo-cache-config.xml with the scope name \"Foo\". The second Session will be the default session using the default cache configuration file. <markup lang=\"java\" > SessionConfiguration sessionConfiguration = SessionConfiguration.builder() .named(\"foo\") .withScopeName(\"Foo\") .withConfigUri(\"foo-cache-config.xml\") .build(); CoherenceConfiguration config = CoherenceConfiguration.builder() .withSession(sessionConfiguration) .withSession(SessionConfiguration.defaultSession()) .build(); Coherence coherence = Coherence.clusterMember(config) .start() .join(); There are various other methods on the configuration builders, for example configuring parameters to pass into the cache configuration files, configuring interceptors, etc. ",
            "title": "Configure a Cluster Member"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " If the application code will is an Extend client, then Coherence can be bootstrapped in client mode. The example below starts Coherence as an Extend client, which will use the Coherence NameService to locate the cluster and look up the Extend Proxy to connect to. This works by configuring the client to have the same cluster name and same well-known address list (or multicast settings) as the cluster being connected to, either using System properties or environment variables. <markup lang=\"java\" > Coherence coherence = Coherence.client(); coherence.start(); Alternatively, instead of using the NameService a fixed address and port can be configured for the Extend client to use. If the System property coherence.extend.address is set to the IP address or host name of the Extend proxy, and coherence.extend.port is set to the port of the Extend proxy (or the corresponding environment variables COHERENCE_EXTEND_ADDRESS and COHERENCE_EXTEND_PORT ) then Coherence can be bootstrapped as shown below. <markup lang=\"java\" > Coherence coherence = Coherence.fixedClient(); coherence.start(); Coherence will then be bootstrapped as an Extend client and connect to the proxy on the configured address and port. Note The code snippets above work with the default cache configuration file. The default cache configuration file in the coherence.jar is configured with certain injectable property values, which are configured by the bootstrap API when running as a client. Using other cache configuration files that are not configured with these properties would mean \"client\" mode is effectively ignored. The Coherence instance will still be started and will run correctly, the client mode properties will just have no affect. ",
            "title": "Run Coherence as an Extend Client"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Coherence can be configured in client mode in code. In the example below, a Coherence client instance is created using a configuration. The configuration in this case does not specify a name, so the default name will be used. The configuration contains two Session configurations. The first is named \"foo\" and uses the cache configuration loaded from foo-cache-config.xml with the scope name \"Foo\". The second Session will be the default session using the default cache configuration file. <markup lang=\"java\" > SessionConfiguration sessionConfiguration = SessionConfiguration.builder() .named(\"Foo\") .withScopeName(\"Foo\") .withConfigUri(\"foo-cache-config.xml\") .build(); CoherenceConfiguration config = CoherenceConfiguration.builder() .withSession(sessionConfiguration) .withSession(SessionConfiguration.defaultSession()) .build(); Coherence coherence = Coherence.client(config) .start() .join(); Using Coherence Extend and application can configure in this way, with multiple Session instances, where each session will connect as an Extend client to a different Coherence cluster. Each configured session is given a different name and scope. The required sessions can then be obtained from the running Coherence instance by application code at runtime. ",
            "title": "Configure an Extend Client"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " If your application needs to control start-up and shutdown of Coherence, then the bootstrap API can be called from application code. This is often useful in integration JUnit test code too, where a test class may need to configure and start Coherence for a set of tests. It is possible for application code to run multiple Coherence instances, which each manage one or more scoped Coherence sessions. Where multiple Coherence cluster member instances are created, they will still all be part of a single Coherence cluster member, they cannot be part of separate clusters. Run a Simple Cluster Member The simplest way to start Coherence as a cluster member in application code is shown below: <markup lang=\"java\" > Coherence coherence = Coherence.clusterMember(); coherence.start(); The start() method returns a CompletableFuture so application code that needs to wit for start-up to complete can use the future for this purpose. The example below ensures Coherence is started as a cluster member (waiting a maximum of five minutes) before proceeding. <markup lang=\"java\" > Coherence coherence = Coherence.clusterMember() .start() .get(5, TimeUnit.MINUTES); Running Coherence in this way will create a single Session using the default cache configuration file (or another file specified using the -Dcoherence.cacheconfig system property). By default, this will be a storage enabled cluster member, unless Coherence system properties or environment variables have been used to override this. Configure a Cluster Member The bootstrap API allows the Coherence instance to be configured before starting, for example adding one or more session configurations. In the example below, a Coherence cluster member instance is created using a configuration. The configuration in this case does not specify a name, so the default name will be used. The configuration contains two Session configurations. The first is named \"foo\" and uses the cache configuration loaded from foo-cache-config.xml with the scope name \"Foo\". The second Session will be the default session using the default cache configuration file. <markup lang=\"java\" > SessionConfiguration sessionConfiguration = SessionConfiguration.builder() .named(\"foo\") .withScopeName(\"Foo\") .withConfigUri(\"foo-cache-config.xml\") .build(); CoherenceConfiguration config = CoherenceConfiguration.builder() .withSession(sessionConfiguration) .withSession(SessionConfiguration.defaultSession()) .build(); Coherence coherence = Coherence.clusterMember(config) .start() .join(); There are various other methods on the configuration builders, for example configuring parameters to pass into the cache configuration files, configuring interceptors, etc. Run Coherence as an Extend Client If the application code will is an Extend client, then Coherence can be bootstrapped in client mode. The example below starts Coherence as an Extend client, which will use the Coherence NameService to locate the cluster and look up the Extend Proxy to connect to. This works by configuring the client to have the same cluster name and same well-known address list (or multicast settings) as the cluster being connected to, either using System properties or environment variables. <markup lang=\"java\" > Coherence coherence = Coherence.client(); coherence.start(); Alternatively, instead of using the NameService a fixed address and port can be configured for the Extend client to use. If the System property coherence.extend.address is set to the IP address or host name of the Extend proxy, and coherence.extend.port is set to the port of the Extend proxy (or the corresponding environment variables COHERENCE_EXTEND_ADDRESS and COHERENCE_EXTEND_PORT ) then Coherence can be bootstrapped as shown below. <markup lang=\"java\" > Coherence coherence = Coherence.fixedClient(); coherence.start(); Coherence will then be bootstrapped as an Extend client and connect to the proxy on the configured address and port. Note The code snippets above work with the default cache configuration file. The default cache configuration file in the coherence.jar is configured with certain injectable property values, which are configured by the bootstrap API when running as a client. Using other cache configuration files that are not configured with these properties would mean \"client\" mode is effectively ignored. The Coherence instance will still be started and will run correctly, the client mode properties will just have no affect. Configure an Extend Client Coherence can be configured in client mode in code. In the example below, a Coherence client instance is created using a configuration. The configuration in this case does not specify a name, so the default name will be used. The configuration contains two Session configurations. The first is named \"foo\" and uses the cache configuration loaded from foo-cache-config.xml with the scope name \"Foo\". The second Session will be the default session using the default cache configuration file. <markup lang=\"java\" > SessionConfiguration sessionConfiguration = SessionConfiguration.builder() .named(\"Foo\") .withScopeName(\"Foo\") .withConfigUri(\"foo-cache-config.xml\") .build(); CoherenceConfiguration config = CoherenceConfiguration.builder() .withSession(sessionConfiguration) .withSession(SessionConfiguration.defaultSession()) .build(); Coherence coherence = Coherence.client(config) .start() .join(); Using Coherence Extend and application can configure in this way, with multiple Session instances, where each session will connect as an Extend client to a different Coherence cluster. Each configured session is given a different name and scope. The required sessions can then be obtained from the running Coherence instance by application code at runtime. ",
            "title": "Bootstrap Coherence in Application Code"
        },
        {
            "location": "/docs/README",
            "text": " To build the docs, run the following Maven command from the top-level prj/ directory: <markup lang=\"shell\" >mvn clean install -DskipTests -pl docs -P docs ",
            "title": "Build the Docs"
        },
        {
            "location": "/docs/README",
            "text": " To view the documentation to see what it looks like after building run the following command from the top-level prj/ directory: <markup lang=\"shell\" >mvn exec:exec -pl docs -P docs Docs can be viewd at http://localhost:8080 This requires Python to be installed and runs a small Python http server from the directory where the docs have been built to. ",
            "title": "View the Docs"
        },
        {
            "location": "/docs/README",
            "text": " This is the module that builds the Coherence documentation. The module is not part of the default build and must be built separately. Build the Docs To build the docs, run the following Maven command from the top-level prj/ directory: <markup lang=\"shell\" >mvn clean install -DskipTests -pl docs -P docs View the Docs To view the documentation to see what it looks like after building run the following command from the top-level prj/ directory: <markup lang=\"shell\" >mvn exec:exec -pl docs -P docs Docs can be viewd at http://localhost:8080 This requires Python to be installed and runs a small Python http server from the directory where the docs have been built to. ",
            "title": "Coherence Documentation Module"
        },
        {
            "location": "/docs/README",
            "text": " When putting version numbers in .adoc files, we use attribute substitutions. Attributes are set in the sitegen.yaml file, for example <markup lang=\"yaml\" >engine: asciidoctor: images-dir: \"docs/images\" libraries: - \"asciidoctor-diagram\" attributes: plantumlconfig: \"_plantuml-config.txt\" coherence-maven-group-id: \"${coherence.group.id}\" version-coherence: \"${revision}\" version-commercial-docs: \"14.1.1.0\" version-helidon: \"${helidon.version}\" The format of an attribute is name followed by a colon, and the attribute value in quotes, so above the value of the version-commercial-docs attribute is 14.1.1.0 . Attributes can be taken from Maven build properties by using the normal Maven property replacement string as the value. For example the version-coherence attribute&#8217;s value will be the Maven revision property value. In the .adoc files the attributes are then substituted by putting the attribute name in curly brackets. For example: The current commercial Coherence version is 14.1.2. would become The current commercial Coherence version is 14.1.1.0. ",
            "title": "Version Numbers"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " First and foremost, Coherence provides a fundamental service that is responsible for all facets of clustering and is a common denominator / building block for all other Coherence services. This service, referred to as 'service 0' internally, ensures the mesh of members is maintained and responsive, taking action to collaboratively evict, shun, or in some cases voluntarily depart the cluster when deemed necessary. As members join and leave the cluster, other Coherence services are notified thus allowing those services to react accordingly. This part of the Coherence product has been in production for 20+ years, and has been the subject of some extensive and imaginative testing. While it has been discussed here it certainly is not something that customers, generally, interact with directly but is valuable to be aware of. Coherence services build on top of the clustering service, with the key implementations to be aware of being PartitionedService, InvocationService, and ProxyService. In the majority of cases customers will deal with caches; a cache will be represented by an implementation of NamedCache&lt;K,V&gt; . Cache is an unfortunate name, as many customers use Coherence as a system-of-record rather than a lossy store of data. A cache is hosted by a service, generally the PartitionedService, and is the entry point to storing, retrieving, aggregating, querying, and streaming data. There are a number of features that caches provide: Fundamental key-based access : get/put getAll/putAll Client-side and storage-side events MapListeners to asynchronously notify clients of changes to data EventInterceptors (either sync or async) to be notified storage level events, including mutations, partition transfer, failover, etc NearCaches - locally cached data based on previous requests with local content invalidated upon changes in storage tier ViewCaches - locally stored view of remote data that can be a subset based on a predicate and is kept in sync real time Queries - distributed, parallel query evaluation to return matching key, values or entries with potential to optimize performance with indices Aggregations - a map/reduce style aggregation where data is aggregated in parallel on all storage nodes and results streamed back to the client for aggregation of those results to produce a final result Data local processing - an ability to send a function to the relevant storage node to execute processing logic for the appropriate entries with exclusive access Partition local transactions - an ability to perform scalable transactions by associating data (thus being on the same partition) and manipulating other entries on the same partition potentially across caches Non-blocking / async NamedCache API Polyglot clients - access the same NamedCache API from C++ , Go , Java, JavaScript , .NET , or Python Portable Object Format - optimized serialization format, with the ability to navigate the serialized form for optimized queries, aggregations, or data processing Integration with Databases - Database &amp; third party data integration with CacheStores including both synchronous or asynchronous writes CohQL - ansi-style query language with a console for adhoc queries Topics - distributed topics implementation offering pub/sub messaging with the storage capacity the cluster and parallelizable subscribers Repository API - a framework implementing the Repository pattern from Domain-Driven Design, abstracting persistent storage implementation from application code, with advanced features like support for pagination, projections, streaming, and updating in-place coherence-concurrent - Coherence-backed implementations of types from the java.util.concurrent package enabling distributed process coordination through the grid Atomics - for implementing e.g. atomic counters shared between cluster, with an optional asynchronous API Executors - for submitting tasks to be executed in the cluster Locks - for implementing lock-based concurrency control across multiple cluster members Queues - for implementing blocking queue / dequeue behavior across multiple cluster members Semaphores - for implementing synchronization of execution across multiple cluster members Microservices integration - broad and close integration with Helidon , Micronaut , and Spring for developing microservices applications using Coherence as a data source or cache There are also a number of non-functional features that Coherence provides: Rock solid clustering - highly tuned and robust clustering stack that allows Coherence to scale to thousands of members in a cluster with thousands of partitions and terabytes of data being accessed, mutated, queried and aggregated concurrently Safety first - resilient data management that ensures backup copies are on distinct machines, racks, or sites and the ability to maintain multiple backups 24/7 Availability - zero down time with rolling redeploy of cluster members to upgrade application or product versions Backwards and forwards compatibility of product upgrades, including major versions Persistent Caches - with the ability to use local file system persistence (thus avoid extra network hops) and leverage Coherence consensus protocols to perform distributed disk recovery when appropriate Distributed State Snapshot - ability to perform distributed point-in-time snapshot of cluster state, and recover snapshot in this or a different cluster (leverages persistence feature) Lossy redundancy - ability to reduce the redundancy guarantee by making backups and/or persistence asynchronous from a client perspective Single Mangement View - provides insight into the cluster with a single JMX server that provides a view of all members of the cluster Management over REST - all JMX data and operations can be performed over REST, including cluster wide thread dumps and heapdumps Non-cluster Access - access to the cluster from the outside via proxies, for distant (high latency) clients and for non-Java languages such as C++ , Go , JavaScript , .NET , or Python Kubernetes friendly - seamlessly and safely deploy applications to k8s with our own operator ",
            "title": "Introduction"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " Coherence Community Edition does not include the following Oracle Coherence commercial edition functionality Management of Coherence via the Oracle WebLogic Management Framework WebLogic Server Multi-tenancy support Deployment of Grid Archives (GARs) HTTP session management for application servers (Coherence*Web) GoldenGate HotCache TopLink-based CacheLoaders and CacheStores Elastic Data Federation and WAN (wide area network) support Transaction Framework CommonJ work manager ",
            "title": "Coherence Community Edition Disabled and Excluded Functionality"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Tests Review the cache configuration Run the Tests OCI Test Results Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can skip the tests in the initial build by adding the following options: -DskipTests for Maven or -x test for Gradle. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " This example comprises a number of tests running with difference cache configurations. Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " In this example you will run a test that will demonstrate different methods for improving Coherence read/ write performance at the expense of data consistency and availability. These methods are outlined below with their potential impact. Tested Cache Config Scenarios Method Description Data Read Consistency Impact Data Availability/Loss Impact Other comments Changing the default read-locator from primary to closest Changing the read-locator to closest (primary or backup) can balance request load or reduce latency Dirty / stale read from out of date backup None Using async backups Enabling asynchronous backups allows the client to continue processing without waiting for the backup to complete None Medium - If the node with the primary copy failed before the backup is complete, data may be lost Async backups result in n backup requests, see below Using scheduling backups Enabling scheduled backups allows backups to be scheduled at regular intervals after a delay None Medium&#8594;High - There is potentially more risk of data loss as multiple batched backups could be lost with node failure Can be more efficient than asynchronous backups as backups can be batched Disabling backups Setting backup-count to zero None High - If any node is lost, then the primary data will be lost See below for documentation links: Using the Read Locator Using Asynchronous Backups Using Scheduled Backups Disabling Backups What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can skip the tests in the initial build by adding the following options: -DskipTests for Maven or -x test for Gradle. Running the Examples This example comprises a number of tests running with difference cache configurations. Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " The cache configuration contains various cache-scheme-mappings and related distributed-scheme entries to test various scenarios. Review the Caching Scheme Mapping <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;base-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-base&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;rl-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-rl&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;async-backup-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-async-backup&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;sched-backup-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-sched-backup&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;no-backup-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-no-backup&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; Base mapping with out-of-the box defaults Read locator Async backup Scheduled backup No backup Review the base distributed scheme <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-base&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; Review distributed scheme with read-locator set to closest <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-rl&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheReadLocator&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;read-locator&gt;closest&lt;/read-locator&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; Review distributed scheme with backups set to asynchronous <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-async-backup&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheAsyncBackup&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;async-backup&gt;true&lt;/async-backup&gt; &lt;/distributed-scheme&gt; Review distributed scheme with backups set the scheduled every 2 seconds <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-sched-backup&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheSchedBackup&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;async-backup&gt;2s&lt;/async-backup&gt; &lt;/distributed-scheme&gt; Review distributed scheme with no backups <markup lang=\"xml\" > &lt;distributed-scheme&gt; &lt;scheme-name&gt;server-no-backup&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheNoBackup&lt;/service-name&gt; &lt;backup-count&gt;0&lt;/backup-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Review the cache config"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " The example code comprises a main test class PerformanceOverConsistencyTest which has number of methods. The following JUnit test method creates multiple caches with different setup and runs the runTest() method against each to record times. Each test-run is run once first and then results are recorded to try and get more consistent results. <markup lang=\"java\" >@Test /** * Run the same test against different cache types. */ public void testDifferentScenarios() throws Exception { // set the system properties to join the cluster System.setProperty(\"coherence.wka\", \"127.0.0.1\"); System.setProperty(\"coherence.ttl\", \"0\"); System.setProperty(\"coherence.cluster\", CLUSTER_NAME); System.setProperty(\"coherence.clusterport\", Integer.toString(clusterPort)); System.setProperty(\"coherence.cacheconfig\", CACHE_CONFIG); System.setProperty(\"coherence.log.level\", \"1\"); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); Coherence coh = Coherence.clusterMember().start().get(5, TimeUnit.MINUTES); Session session = coh.getSession(); System.out.println(\"Running Tests\"); System.out.flush(); final String headerFormat = \"%-15s %12s %12s %12s %12s %12s\"; final String lineFormat = \"%-15s %,10dms %,10dms %,10dms %,10dms %,10dms\"; NamedCache&lt;Integer, Customer&gt; cache = session.getCache(\"base-customers\"); NamedCache&lt;Integer, Customer&gt; cacheReadLocator = session.getCache(\"rl-customers\"); NamedCache&lt;Integer, Customer&gt; cacheAsyncBackup = session.getCache(\"async-backup-customers\"); NamedCache&lt;Integer, Customer&gt; cacheSchedBackup = session.getCache(\"sched-backup-customers\"); NamedCache&lt;Integer, Customer&gt; cacheNoBackup = session.getCache(\"no-backup-customers\"); List&lt;TestResult&gt; listResults = new ArrayList&lt;&gt;(); log(\"\"); // discard the first run of each of the tests to ensure we have more consistent test results runTest(cache, \"base\"); listResults.add(runTest(cache, \"base\")); runTest(cacheReadLocator, \"base-rl\"); listResults.add(runTest(cacheReadLocator, \"base-rl\")); runTest(cacheAsyncBackup, \"async-backup\"); listResults.add(runTest(cacheAsyncBackup, \"async-backup\")); runTest(cacheSchedBackup, \"sched-backup\"); listResults.add(runTest(cacheSchedBackup, \"sched-backup\")); runTest(cacheNoBackup, \"no-backup\"); listResults.add(runTest(cacheNoBackup, \"no-backup\")); // output the results System.out.printf(headerFormat, \"Cache Type\", \"2k Put\", \"8 PutAll\",\"100 Invoke\", \"2k Get\", \"100 GetAll\"); listResults.forEach( (v)-&gt;System.out.printf(lineFormat, v.getType(), v.getPutDuration(), v.getPutAllDuration(), v.getInvokeDuration(), v.getGetDuration(), v.getGetAllDuration())); log(\"Note: The above times are to run the individual parts of tests, not to do an individual put/get, etc.\"); } The test results you get may vary as they are run on a single machine. You should carry out tests of different configurations in your own development/test environments to see the effect these scenarios have. The following JUnit test method runs various operations including get() , put() , getAll() and invoke() and times them to compare the different cache configuration. See comments in code for explanations. <markup lang=\"java\" >/** * Run various cache operations to test different configurations to achieve performance over consistency. * * @param cache {@link NamedCache} to test against * @param type type of test * @return the test results */ private TestResult runTest(NamedCache&lt;Integer, Customer&gt; cache, String type) { cache.clear(); long start = System.currentTimeMillis(); // insert multiple customers using individual put() for (int i = 1; i &lt;= 2_000; i++) { Customer c = getCustomer(i); cache.put(c.getId(), c); } long putDuration = System.currentTimeMillis() - start; Map&lt;Integer, Customer&gt; buffer = new HashMap&lt;&gt;(); start = System.currentTimeMillis(); // insert customers using putAll in batches for (int i = 2_001; i &lt;= 10_000; i++) { Customer c = getCustomer(i); buffer.put(c.getId(), c); if (i % 1_000 == 0) { cache.putAll(buffer); buffer.clear(); } } if (!buffer.isEmpty()) { cache.putAll(buffer); } long putAllDuration = System.currentTimeMillis() - start; start = System.currentTimeMillis(); // issue 2,000 get() operations for (int i = 1; i &lt; 2_000; i++) { Customer value = cache.get(i); } long getDuration = System.currentTimeMillis() - start; start = System.currentTimeMillis(); // issue 100 getAll() operations for (int i = 1; i &lt; 100; i++) { Map&lt;Integer, Customer&gt; all = cache.getAll(Set.of(i, i + 1, i + 2, i + 3, i + 4, i + 5)); } long getAllDuration = System.currentTimeMillis() - start; start = System.currentTimeMillis(); // issue 100 entry processor updates which require backup updates for (int i = 1; i &lt; 100L; i++) { cache.invoke(i, Processors.update(Customer::setCustomerType, Customer.GOLD)); } long invokeDuration = System.currentTimeMillis() - start; cache.clear(); return new TestResult(type, putDuration, putAllDuration, getDuration, getAllDuration, invokeDuration); } Review the cache config The cache configuration contains various cache-scheme-mappings and related distributed-scheme entries to test various scenarios. Review the Caching Scheme Mapping <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;base-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-base&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;rl-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-rl&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;async-backup-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-async-backup&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;sched-backup-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-sched-backup&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;no-backup-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-no-backup&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; Base mapping with out-of-the box defaults Read locator Async backup Scheduled backup No backup Review the base distributed scheme <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-base&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; Review distributed scheme with read-locator set to closest <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-rl&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheReadLocator&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;read-locator&gt;closest&lt;/read-locator&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; Review distributed scheme with backups set to asynchronous <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-async-backup&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheAsyncBackup&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;async-backup&gt;true&lt;/async-backup&gt; &lt;/distributed-scheme&gt; Review distributed scheme with backups set the scheduled every 2 seconds <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-sched-backup&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheSchedBackup&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;async-backup&gt;2s&lt;/async-backup&gt; &lt;/distributed-scheme&gt; Review distributed scheme with no backups <markup lang=\"xml\" > &lt;distributed-scheme&gt; &lt;scheme-name&gt;server-no-backup&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheNoBackup&lt;/service-name&gt; &lt;backup-count&gt;0&lt;/backup-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Review the Tests"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " Run the examples using the test case below using Maven or Gradle. E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the test code. The tests with take approximately 5 minutes and there may be a lot of other output regarding server startup, but the main output we are interested is below. Search for Running Tests in your output. Output has been truncated and formatted for easier reading. <markup lang=\"bash\" >[Coherence:err:70562] 6: 2023-07-28 11:35:18.334/2.520 Oracle Coherence GE 14.1.1.2206.5 &lt;Warning&gt; (thread=Coherence, member=n/a): Local address \"127.0.0.1\" is a loopback address; this cluster node will not connect to nodes located on different machines Running Tests #### Cache Type 2k Put 8 PutAll 100 Invoke 2k Get 100 GetAll base 1,245ms 153ms 103ms 600ms 45ms base-rl 904ms 81ms 89ms 414ms 36ms async-backup 541ms 100ms 70ms 379ms 24ms sched-backup 393ms 57ms 65ms 437ms 31ms no-backup 354ms 56ms 50ms 364ms 24ms #### Note: The above times are to run the individual parts of tests, not to do an individual put/get, etc. [Coherence:err:70562] 7: (terminated) [Coherence:out:70562] 1: (terminated) The base results with defaults Read locator set to closest means data could be read from backup or primary which could be on the same machine. As all the members are running on the same machine, the results if this test may not be relevant. See the OCI results below for more relevant test results. Async backup improves put() , putAll() and invoke() operations significantly Scheduled backups can have small improvements over async backups No backups has marginal or negligible improvement from asynchronous oe scheduled backups The test results you get may vary as they are run on a single machine. You should carry out tests of different configurations in your own development/test environments to see the effect these scenarios have. ",
            "title": "Run the Tests"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " From the above you can see the default put time for this environment with standard (one) backup was around 1.8ms and the scheduled, async and no-backup were considerably less. ",
            "title": "Various Backup Types"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " Running random get operations (50,000) using default read-locator of primary and then running using read-locator of closest , the following results were observed: Read Locator Test on OCI Read Locator Test Runner 1 Test Runner 2 Test Runner 3 Average primary 0.439ms 0.517ms 0.575ms 0.518ms closest 0.308ms 0.441ms 0.413ms 0.387ms ",
            "title": "Default and Closest Read Locators"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " Below are the results of additional tests run on an Oracle Cloud Infrastructure (OCI) where there is a some latency between nodes, (rather then being on a single machine) as well as cache servers on multiple machines to show the difference more clearly. The setup was 3 storage nodes across 3 availability domains as well as 3 JMeter runners running tests. Various Backup Types From the above you can see the default put time for this environment with standard (one) backup was around 1.8ms and the scheduled, async and no-backup were considerably less. Default and Closest Read Locators Running random get operations (50,000) using default read-locator of primary and then running using read-locator of closest , the following results were observed: Read Locator Test on OCI Read Locator Test Runner 1 Test Runner 2 Test Runner 3 Average primary 0.439ms 0.517ms 0.575ms 0.518ms closest 0.308ms 0.441ms 0.413ms 0.387ms ",
            "title": "OCI Test Results"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " This guide walks you through how to tweak Coherence to provide more performance at the expense of data consistency and availability. A few notes from the above results: If your application can tolerate some data loss, then rather than using zero backups, you should use async or scheduled backups as this does provide better availability that no backups. If your application can tolerate potential dirty reads, then use the closest read locator. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " Using Read Locator Using Asynchronous Backups Using Scheduled Backups Disabling Backups ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/220-performance/README",
            "text": " This guide walks you through how to tweak Coherence to provide more performance at the expense of data consistency and availability. Out of the box, Coherence provides many features that ensure data consistency, including: Backups - By default there is 1 backup, which will automatically be stored on a separate node, machine or site from the primary to provide redundancy in the case of the loss of a node, machine or site. Synchronous backups - When entries are mutated, control will not be returned to the client until the primary and backup have been written. Data consistency - All data access is always directed to the primary copy of the data to ensure that the data received is the most recent and consistent. These guarantees of data consistency and availability are one of the many hallmarks of Coherence, but there may be cases where you may need to maximize cache reads/ writes by removing some of the above guarantees. This guide will explore and compare various methods of achieving better performance and the expenses of data consistency and availability. Table of Contents What You Will Build What You Need Building the Example Code Review the Tests Review the cache configuration Run the Tests OCI Test Results Summary See Also What You Will Build In this example you will run a test that will demonstrate different methods for improving Coherence read/ write performance at the expense of data consistency and availability. These methods are outlined below with their potential impact. Tested Cache Config Scenarios Method Description Data Read Consistency Impact Data Availability/Loss Impact Other comments Changing the default read-locator from primary to closest Changing the read-locator to closest (primary or backup) can balance request load or reduce latency Dirty / stale read from out of date backup None Using async backups Enabling asynchronous backups allows the client to continue processing without waiting for the backup to complete None Medium - If the node with the primary copy failed before the backup is complete, data may be lost Async backups result in n backup requests, see below Using scheduling backups Enabling scheduled backups allows backups to be scheduled at regular intervals after a delay None Medium&#8594;High - There is potentially more risk of data loss as multiple batched backups could be lost with node failure Can be more efficient than asynchronous backups as backups can be batched Disabling backups Setting backup-count to zero None High - If any node is lost, then the primary data will be lost See below for documentation links: Using the Read Locator Using Asynchronous Backups Using Scheduled Backups Disabling Backups What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can skip the tests in the initial build by adding the following options: -DskipTests for Maven or -x test for Gradle. Running the Examples This example comprises a number of tests running with difference cache configurations. Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Review the Tests The example code comprises a main test class PerformanceOverConsistencyTest which has number of methods. The following JUnit test method creates multiple caches with different setup and runs the runTest() method against each to record times. Each test-run is run once first and then results are recorded to try and get more consistent results. <markup lang=\"java\" >@Test /** * Run the same test against different cache types. */ public void testDifferentScenarios() throws Exception { // set the system properties to join the cluster System.setProperty(\"coherence.wka\", \"127.0.0.1\"); System.setProperty(\"coherence.ttl\", \"0\"); System.setProperty(\"coherence.cluster\", CLUSTER_NAME); System.setProperty(\"coherence.clusterport\", Integer.toString(clusterPort)); System.setProperty(\"coherence.cacheconfig\", CACHE_CONFIG); System.setProperty(\"coherence.log.level\", \"1\"); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); Coherence coh = Coherence.clusterMember().start().get(5, TimeUnit.MINUTES); Session session = coh.getSession(); System.out.println(\"Running Tests\"); System.out.flush(); final String headerFormat = \"%-15s %12s %12s %12s %12s %12s\"; final String lineFormat = \"%-15s %,10dms %,10dms %,10dms %,10dms %,10dms\"; NamedCache&lt;Integer, Customer&gt; cache = session.getCache(\"base-customers\"); NamedCache&lt;Integer, Customer&gt; cacheReadLocator = session.getCache(\"rl-customers\"); NamedCache&lt;Integer, Customer&gt; cacheAsyncBackup = session.getCache(\"async-backup-customers\"); NamedCache&lt;Integer, Customer&gt; cacheSchedBackup = session.getCache(\"sched-backup-customers\"); NamedCache&lt;Integer, Customer&gt; cacheNoBackup = session.getCache(\"no-backup-customers\"); List&lt;TestResult&gt; listResults = new ArrayList&lt;&gt;(); log(\"\"); // discard the first run of each of the tests to ensure we have more consistent test results runTest(cache, \"base\"); listResults.add(runTest(cache, \"base\")); runTest(cacheReadLocator, \"base-rl\"); listResults.add(runTest(cacheReadLocator, \"base-rl\")); runTest(cacheAsyncBackup, \"async-backup\"); listResults.add(runTest(cacheAsyncBackup, \"async-backup\")); runTest(cacheSchedBackup, \"sched-backup\"); listResults.add(runTest(cacheSchedBackup, \"sched-backup\")); runTest(cacheNoBackup, \"no-backup\"); listResults.add(runTest(cacheNoBackup, \"no-backup\")); // output the results System.out.printf(headerFormat, \"Cache Type\", \"2k Put\", \"8 PutAll\",\"100 Invoke\", \"2k Get\", \"100 GetAll\"); listResults.forEach( (v)-&gt;System.out.printf(lineFormat, v.getType(), v.getPutDuration(), v.getPutAllDuration(), v.getInvokeDuration(), v.getGetDuration(), v.getGetAllDuration())); log(\"Note: The above times are to run the individual parts of tests, not to do an individual put/get, etc.\"); } The test results you get may vary as they are run on a single machine. You should carry out tests of different configurations in your own development/test environments to see the effect these scenarios have. The following JUnit test method runs various operations including get() , put() , getAll() and invoke() and times them to compare the different cache configuration. See comments in code for explanations. <markup lang=\"java\" >/** * Run various cache operations to test different configurations to achieve performance over consistency. * * @param cache {@link NamedCache} to test against * @param type type of test * @return the test results */ private TestResult runTest(NamedCache&lt;Integer, Customer&gt; cache, String type) { cache.clear(); long start = System.currentTimeMillis(); // insert multiple customers using individual put() for (int i = 1; i &lt;= 2_000; i++) { Customer c = getCustomer(i); cache.put(c.getId(), c); } long putDuration = System.currentTimeMillis() - start; Map&lt;Integer, Customer&gt; buffer = new HashMap&lt;&gt;(); start = System.currentTimeMillis(); // insert customers using putAll in batches for (int i = 2_001; i &lt;= 10_000; i++) { Customer c = getCustomer(i); buffer.put(c.getId(), c); if (i % 1_000 == 0) { cache.putAll(buffer); buffer.clear(); } } if (!buffer.isEmpty()) { cache.putAll(buffer); } long putAllDuration = System.currentTimeMillis() - start; start = System.currentTimeMillis(); // issue 2,000 get() operations for (int i = 1; i &lt; 2_000; i++) { Customer value = cache.get(i); } long getDuration = System.currentTimeMillis() - start; start = System.currentTimeMillis(); // issue 100 getAll() operations for (int i = 1; i &lt; 100; i++) { Map&lt;Integer, Customer&gt; all = cache.getAll(Set.of(i, i + 1, i + 2, i + 3, i + 4, i + 5)); } long getAllDuration = System.currentTimeMillis() - start; start = System.currentTimeMillis(); // issue 100 entry processor updates which require backup updates for (int i = 1; i &lt; 100L; i++) { cache.invoke(i, Processors.update(Customer::setCustomerType, Customer.GOLD)); } long invokeDuration = System.currentTimeMillis() - start; cache.clear(); return new TestResult(type, putDuration, putAllDuration, getDuration, getAllDuration, invokeDuration); } Review the cache config The cache configuration contains various cache-scheme-mappings and related distributed-scheme entries to test various scenarios. Review the Caching Scheme Mapping <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;base-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-base&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;rl-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-rl&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;async-backup-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-async-backup&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;sched-backup-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-sched-backup&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;no-backup-*&lt;/cache-name&gt; &lt;scheme-name&gt;server-no-backup&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; Base mapping with out-of-the box defaults Read locator Async backup Scheduled backup No backup Review the base distributed scheme <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-base&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; Review distributed scheme with read-locator set to closest <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-rl&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheReadLocator&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;read-locator&gt;closest&lt;/read-locator&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; Review distributed scheme with backups set to asynchronous <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-async-backup&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheAsyncBackup&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;async-backup&gt;true&lt;/async-backup&gt; &lt;/distributed-scheme&gt; Review distributed scheme with backups set the scheduled every 2 seconds <markup lang=\"xml\" >&lt;distributed-scheme&gt; &lt;scheme-name&gt;server-sched-backup&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheSchedBackup&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;async-backup&gt;2s&lt;/async-backup&gt; &lt;/distributed-scheme&gt; Review distributed scheme with no backups <markup lang=\"xml\" > &lt;distributed-scheme&gt; &lt;scheme-name&gt;server-no-backup&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCacheNoBackup&lt;/service-name&gt; &lt;backup-count&gt;0&lt;/backup-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Run the Tests Run the examples using the test case below using Maven or Gradle. E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the test code. The tests with take approximately 5 minutes and there may be a lot of other output regarding server startup, but the main output we are interested is below. Search for Running Tests in your output. Output has been truncated and formatted for easier reading. <markup lang=\"bash\" >[Coherence:err:70562] 6: 2023-07-28 11:35:18.334/2.520 Oracle Coherence GE 14.1.1.2206.5 &lt;Warning&gt; (thread=Coherence, member=n/a): Local address \"127.0.0.1\" is a loopback address; this cluster node will not connect to nodes located on different machines Running Tests #### Cache Type 2k Put 8 PutAll 100 Invoke 2k Get 100 GetAll base 1,245ms 153ms 103ms 600ms 45ms base-rl 904ms 81ms 89ms 414ms 36ms async-backup 541ms 100ms 70ms 379ms 24ms sched-backup 393ms 57ms 65ms 437ms 31ms no-backup 354ms 56ms 50ms 364ms 24ms #### Note: The above times are to run the individual parts of tests, not to do an individual put/get, etc. [Coherence:err:70562] 7: (terminated) [Coherence:out:70562] 1: (terminated) The base results with defaults Read locator set to closest means data could be read from backup or primary which could be on the same machine. As all the members are running on the same machine, the results if this test may not be relevant. See the OCI results below for more relevant test results. Async backup improves put() , putAll() and invoke() operations significantly Scheduled backups can have small improvements over async backups No backups has marginal or negligible improvement from asynchronous oe scheduled backups The test results you get may vary as they are run on a single machine. You should carry out tests of different configurations in your own development/test environments to see the effect these scenarios have. OCI Test Results Below are the results of additional tests run on an Oracle Cloud Infrastructure (OCI) where there is a some latency between nodes, (rather then being on a single machine) as well as cache servers on multiple machines to show the difference more clearly. The setup was 3 storage nodes across 3 availability domains as well as 3 JMeter runners running tests. Various Backup Types From the above you can see the default put time for this environment with standard (one) backup was around 1.8ms and the scheduled, async and no-backup were considerably less. Default and Closest Read Locators Running random get operations (50,000) using default read-locator of primary and then running using read-locator of closest , the following results were observed: Read Locator Test on OCI Read Locator Test Runner 1 Test Runner 2 Test Runner 3 Average primary 0.439ms 0.517ms 0.575ms 0.518ms closest 0.308ms 0.441ms 0.413ms 0.387ms Summary This guide walks you through how to tweak Coherence to provide more performance at the expense of data consistency and availability. A few notes from the above results: If your application can tolerate some data loss, then rather than using zero backups, you should use async or scheduled backups as this does provide better availability that no backups. If your application can tolerate potential dirty reads, then use the closest read locator. See Also Using Read Locator Using Asynchronous Backups Using Scheduled Backups Disabling Backups ",
            "title": "Performance over Consistency &amp; Availability"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence CDI provides support for CDI (Contexts and Dependency Injection) within Coherence cluster members. It allows you both to inject Coherence-managed resources, such as NamedMap , NamedCache and Session instances into CDI managed beans, to inject CDI beans into Coherence-managed resources, such as event interceptors and cache stores, and to handle Coherence server-side events using CDI observer methods. In addition, Coherence CDI provides support for automatic injection of transient objects upon deserialization. This allows you to inject CDI managed beans such as services and repositories (to use DDD nomenclature) into transient objects, such as entry processor and even data class instances, greatly simplifying implementation of true Domain Driven applications. ",
            "title": "Coherence CDI"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import jakarta.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import jakarta.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import jakarta.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . ",
            "title": "Inject Views"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import jakarta.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject @SessionName(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import jakarta.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import jakarta.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import jakarta.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . ",
            "title": "Injecting NamedMap , NamedCache and related objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import jakarta.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; ",
            "title": "Injecting NamedTopic and related objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import jakarta.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import jakarta.inject.Inject; @Inject private OperationalContext ctx; ",
            "title": " Cluster and OperationalContext Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import jakarta.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; ",
            "title": "Named Session Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; ",
            "title": " Serializer Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Inject a POF Serializer With a Specific POF Configuration"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import jakarta.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import jakarta.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import jakarta.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Other Supported Injection Points"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " CDI, and dependency injection in general, make it easy for application classes to declare the dependencies they need and let the runtime provide them when necessary. This makes the applications easier to develop, test and reason about, and the code extremely clean. Coherence CDI allows you to do the same for Coherence objects, such as Cluster , Session , NamedMap , NamedCache , ContinuousQueryCache , ConfigurableCacheFactory , etc. Injecting NamedMap , NamedCache and related objects In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import jakarta.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject @SessionName(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import jakarta.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import jakarta.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import jakarta.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . Injecting NamedTopic and related objects In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import jakarta.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; Other Supported Injection Points While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import jakarta.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import jakarta.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import jakarta.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Injecting Coherence Objects into CDI Beans"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } ",
            "title": "Observe Specific Event Types"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Filter Observed Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. ",
            "title": "Transform Observed Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } ",
            "title": "Observe Events for Maps and Caches in Specific Services and Scopes"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Using Asynchronous Observers"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Using CDI Observers to Handle Coherence Server-Side Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence has a number of server-side extension points, which allow users to customize application behavior in different ways, typically by configuring their extensions within various sections of the cache configuration file. For example, the users can implement event interceptors and cache stores, in order to handle server-side events and integrate with the external data stores and other services. Coherence CDI provides a way to inject named CDI beans into these extension points using custom configuration namespace handler. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; Once you&#8217;ve declared the handler for the cdi namespace above, you can specify &lt;cdi:bean&gt; element in any place where you would normally use &lt;class-name&gt; or &lt;class-factory-name&gt; elements: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;registrationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;activationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;cacheListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;partition-listener&gt; &lt;cdi:bean&gt;partitionListener&lt;/cdi:bean&gt; &lt;/partition-listener&gt; &lt;member-listener&gt; &lt;cdi:bean&gt;memberListener&lt;/cdi:bean&gt; &lt;/member-listener&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;storageListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Note that you can only inject named CDI beans (beans with an explicit @Named annotations) via &lt;cdi:bean&gt; element. For example, the cacheListener interceptor bean used above would look similar to this: <markup lang=\"java\" >@ApplicationScoped @Named(\"cacheListener\") @EntryEvents(INSERTING) public class MyCacheListener implements EventInterceptor&lt;EntryEvent&lt;Long, String&gt;&gt; { @Override public void onEvent(EntryEvent&lt;Long, String&gt; e) { // handle INSERTING event } } Also keep in mind that only @ApplicationScoped beans can be injected, which implies that they may be shared. For example, because we&#8217;ve used a wildcard, * , as a cache name within the cache mapping in the example above, the same instance of cacheListener will receive events from multiple caches. This is typically fine, as the event itself provides the details about the context that raised it, including cache name, and the service it was raised from, but it does imply that any shared state that you may have within your listener class shouldn&#8217;t be context-specific, and it must be safe for concurrent access from multiple threads. If you can&#8217;t guarantee the latter, you may want to declare the onEvent method as synchronized , to ensure only one thread at a time can access any shared state you may have. Using CDI Observers to Handle Coherence Server-Side Events While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Injecting CDI Beans into Coherence-managed Objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. ",
            "title": "Making transient classes Injectable "
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Using CDI to inject Coherence objects into your application classes, and CDI beans into Coherence-managed objects will allow you to support many use cases where dependency injection may be useful, but it doesn&#8217;t cover an important use case that is somewhat specific to Coherence. Coherence is a distributed system, and it uses serialization in order to send both the data and the processing requests from one cluster member (or remote client) to another, as well as to store data, both in memory and on disk. Processing requests, such as entry processors and aggregators, have to be deserialized on a target cluster member(s) in order to be executed. In some cases, they could benefit from dependency injection in order to avoid service lookups. Similarly, while the data is stored in a serialized, binary format, it may need to be deserialized into user supplied classes for server-side processing, such as when executing entry processors and aggregators. In this case, data classes can often also benefit from dependency injection (in order to support Domain-Driven Design (DDD), for example). While these transient objects are not managed by the CDI container, Coherence CDI does support their injection during deserialization, but for performance reasons requires that you explicitly opt-in by implementing com.oracle.coherence.cdi.Injectable interface. Making transient classes Injectable While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. ",
            "title": "Injecting CDI Beans into Transient Objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . ",
            "title": "Create the Custom Filter Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. ",
            "title": "Create the Custom Filter Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " As already mentioned above, when creating views or subscribing to events, the view or events can be modified using Filters . The exact Filter implementation injected will be determined by the view or event observers qualifiers. Specifically any qualifier annotation that is itself annotated with the @FilterBinding annotation. This should be a familiar pattern to anyone who has worked with CDI interceptors. For example, if there is an injection point for a view that is a filtered view of an underlying map, but the filter required is more complex than those provided by the build in qualifiers, or is some custom filter implementation. The steps required are: Create a custom annotation class to represent the required Filter . Create a bean class implementing com.oracle.coherence.cdi.FilterFactory annotated with the custom annotation that will be the factory for producing instances of the custom Filter . Annotate the view injection point with the custom annotation. Create the Custom Filter Annotation Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . Create the Custom Filter Factory Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ",
            "title": "FilterBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ",
            "title": "PropertyExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); ",
            "title": "ChainedExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) ",
            "title": "PofExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) ",
            "title": "Built-In ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. ",
            "title": "Custom ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . ",
            "title": "Create the Custom Extractor Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. ",
            "title": "Create the Custom Extractor Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Extractor bindings are annotations that are themselves annotated with @ExtractorBinding and are used in conjunction with an implementation of com.oracle.coherence.cdi.ExtractorFactory to produce Coherence ValueExtractor instances. There are a number of built-in extractor binding annotations in the Coherence CDI module and it is a simple process to provide custom implementations. Built-In ExtractorBinding Annotations PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) Custom ExtractorBinding Annotations When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . Create the Custom Extractor Factory Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . ",
            "title": "Create the Custom Extractor Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. ",
            "title": "Create the Custom Extractor Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence CDI supports event observers that can observe events for cache, or map, entries (see the Events section). The observer method can be annotated with a MapEventTransformerBinding annotation to indicate that the observer requires a transformer to be applied to the original event before it is observed. There are no built-in MapEventTransformerBinding annotations, this feature is to support use of custom MapEventTransformer implementations. The steps to create and use a MapEventTransformerBinding annotation are: Create a custom annotation class to represent the required MapEventTransformer . Create a bean class implementing com.oracle.coherence.cdi.MapEventTransformerFactory annotated with the custom annotation that will be the factory for producing instances of the custom MapEventTransformer . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . Create the Custom Extractor Factory Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "MapEventTransformerBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to use Coherence CDI, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; Once the necessary dependency is in place, you can start using CDI to inject Coherence objects into managed CDI beans, and vice versa, as the following sections describe. Injecting Coherence Objects into CDI Beans Injecting NamedMap , NamedCache`, and related objects Injecting NamedMap or NamedCache Views Injecting NamedTopic and related objects Other Supported Injection Points Cluster and OperationalContext Injection Named Session Injection Serializer Injection Injecting CDI Beans into Coherence-managed Objects Using CDI Observers to Handle Coherence Server-Side Events Observer specific event types Filter the events to be observed Transform the events to be observed Observe events for maps and caches in specific scopes or services Using Asynchronous Observers Injecting CDI Beans into Transient Objects Making transient classes Injectable Filter Binding Annotations Extractor Binding Annotations Built-In Extractor Binding Annotations @PropertyExtractor @ChainedExtractor @PofExtractor Custom Extractor Binding Annotations MapEventTransformer Binding Annotations Injecting Coherence Objects into CDI Beans CDI, and dependency injection in general, make it easy for application classes to declare the dependencies they need and let the runtime provide them when necessary. This makes the applications easier to develop, test and reason about, and the code extremely clean. Coherence CDI allows you to do the same for Coherence objects, such as Cluster , Session , NamedMap , NamedCache , ContinuousQueryCache , ConfigurableCacheFactory , etc. Injecting NamedMap , NamedCache and related objects In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import jakarta.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject @SessionName(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import jakarta.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import jakarta.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import jakarta.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . Injecting NamedTopic and related objects In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import jakarta.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; Other Supported Injection Points While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import jakarta.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import jakarta.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import jakarta.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. Injecting CDI Beans into Coherence-managed Objects Coherence has a number of server-side extension points, which allow users to customize application behavior in different ways, typically by configuring their extensions within various sections of the cache configuration file. For example, the users can implement event interceptors and cache stores, in order to handle server-side events and integrate with the external data stores and other services. Coherence CDI provides a way to inject named CDI beans into these extension points using custom configuration namespace handler. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; Once you&#8217;ve declared the handler for the cdi namespace above, you can specify &lt;cdi:bean&gt; element in any place where you would normally use &lt;class-name&gt; or &lt;class-factory-name&gt; elements: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;registrationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;activationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;cacheListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;partition-listener&gt; &lt;cdi:bean&gt;partitionListener&lt;/cdi:bean&gt; &lt;/partition-listener&gt; &lt;member-listener&gt; &lt;cdi:bean&gt;memberListener&lt;/cdi:bean&gt; &lt;/member-listener&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;storageListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Note that you can only inject named CDI beans (beans with an explicit @Named annotations) via &lt;cdi:bean&gt; element. For example, the cacheListener interceptor bean used above would look similar to this: <markup lang=\"java\" >@ApplicationScoped @Named(\"cacheListener\") @EntryEvents(INSERTING) public class MyCacheListener implements EventInterceptor&lt;EntryEvent&lt;Long, String&gt;&gt; { @Override public void onEvent(EntryEvent&lt;Long, String&gt; e) { // handle INSERTING event } } Also keep in mind that only @ApplicationScoped beans can be injected, which implies that they may be shared. For example, because we&#8217;ve used a wildcard, * , as a cache name within the cache mapping in the example above, the same instance of cacheListener will receive events from multiple caches. This is typically fine, as the event itself provides the details about the context that raised it, including cache name, and the service it was raised from, but it does imply that any shared state that you may have within your listener class shouldn&#8217;t be context-specific, and it must be safe for concurrent access from multiple threads. If you can&#8217;t guarantee the latter, you may want to declare the onEvent method as synchronized , to ensure only one thread at a time can access any shared state you may have. Using CDI Observers to Handle Coherence Server-Side Events While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. Injecting CDI Beans into Transient Objects Using CDI to inject Coherence objects into your application classes, and CDI beans into Coherence-managed objects will allow you to support many use cases where dependency injection may be useful, but it doesn&#8217;t cover an important use case that is somewhat specific to Coherence. Coherence is a distributed system, and it uses serialization in order to send both the data and the processing requests from one cluster member (or remote client) to another, as well as to store data, both in memory and on disk. Processing requests, such as entry processors and aggregators, have to be deserialized on a target cluster member(s) in order to be executed. In some cases, they could benefit from dependency injection in order to avoid service lookups. Similarly, while the data is stored in a serialized, binary format, it may need to be deserialized into user supplied classes for server-side processing, such as when executing entry processors and aggregators. In this case, data classes can often also benefit from dependency injection (in order to support Domain-Driven Design (DDD), for example). While these transient objects are not managed by the CDI container, Coherence CDI does support their injection during deserialization, but for performance reasons requires that you explicitly opt-in by implementing com.oracle.coherence.cdi.Injectable interface. Making transient classes Injectable While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. FilterBinding Annotations As already mentioned above, when creating views or subscribing to events, the view or events can be modified using Filters . The exact Filter implementation injected will be determined by the view or event observers qualifiers. Specifically any qualifier annotation that is itself annotated with the @FilterBinding annotation. This should be a familiar pattern to anyone who has worked with CDI interceptors. For example, if there is an injection point for a view that is a filtered view of an underlying map, but the filter required is more complex than those provided by the build in qualifiers, or is some custom filter implementation. The steps required are: Create a custom annotation class to represent the required Filter . Create a bean class implementing com.oracle.coherence.cdi.FilterFactory annotated with the custom annotation that will be the factory for producing instances of the custom Filter . Annotate the view injection point with the custom annotation. Create the Custom Filter Annotation Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . Create the Custom Filter Factory Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ExtractorBinding Annotations Extractor bindings are annotations that are themselves annotated with @ExtractorBinding and are used in conjunction with an implementation of com.oracle.coherence.cdi.ExtractorFactory to produce Coherence ValueExtractor instances. There are a number of built-in extractor binding annotations in the Coherence CDI module and it is a simple process to provide custom implementations. Built-In ExtractorBinding Annotations PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) Custom ExtractorBinding Annotations When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . Create the Custom Extractor Factory Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { MapEventTransformerBinding Annotations Coherence CDI supports event observers that can observe events for cache, or map, entries (see the Events section). The observer method can be annotated with a MapEventTransformerBinding annotation to indicate that the observer requires a transformer to be applied to the original event before it is observed. There are no built-in MapEventTransformerBinding annotations, this feature is to support use of custom MapEventTransformer implementations. The steps to create and use a MapEventTransformerBinding annotation are: Create a custom annotation class to represent the required MapEventTransformer . Create a bean class implementing com.oracle.coherence.cdi.MapEventTransformerFactory annotated with the custom annotation that will be the factory for producing instances of the custom MapEventTransformer . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . Create the Custom Extractor Factory Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Usage"
        },
        {
            "location": "/docs/about/05_api",
            "text": " Coherence CE Java API Coherence .NET Client API Coherence C++ Client API Coherence Python Client API Coherence Go Client API Coherence JavaScript Client API ",
            "title": "API Docs"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " What You Will Build What You Need Building The Example Code Example Data Model Why to use Views Using a ContinuousQueryCache Observing Continuous Query Caches Continuous Aggregation Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " The example code is written as a set of unit tests, showing you how can create Views against your Coherence data. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " With Views , also referred to as Continuous Queries , you can ensure that a query always retrieves the latest results from a cache in real-time. For instance, in the queries guide , we used a Filter to query for a subset of data from a Coherence cache. However, what happens with the underlying cache if changes DO happen, and you need the updates immediately? Queries, as used previously, will only retrieve a snapshot of the underlying data. They will not reflect future data changes. Thus, let&#8217;s revisit a previous example that queries a cache containing Countries using a Filter . The Filter , as in the previous query example, will ensure that only countries with a population of 60 million or more people are returned. <markup lang=\"java\" >@Test void testGreaterEqualsFilterWithChanges() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); Set&lt;Map.Entry&lt;String, Country&gt;&gt; results = map.entrySet(filter); assertThat(results, hasSize(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results, hasSize(2)); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Apply the Filter by invoking entrySet(filter) on the NamedCache The result should be 2 countries only We add a new country Mexico to the map Assert that still only France and Germany were selected In this test we have added a new country Mexico to the countries Map but as you can see, the change will not be reflected in the already filtered results map. In order to get updates in real-time, we have to use a ContinuousQueryCache . Views are extremely useful in all those situations where we need immediate access to any changes of the underlying data, such as trading systems or Complex Event Processing (CEP) systems. They can be used in both client-based and server-based applications and are reminiscent of SQL Views. ",
            "title": "Why to use Views"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " The following test will look almost exactly the same as the previous test. However, instead of calling the entrySet() method on the NamedCache , we will create a new instance of ContinuousQueryCache and pass in the Filter and the Coherence map as constructor arguments. <markup lang=\"java\" >@Test void testGreaterEqualsFilterWithContinuousQueryCache() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); ContinuousQueryCache results = new ContinuousQueryCache(map, filter); assertThat(results.size(), is(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); } Get the countries Map We create a Filter that will select only countries with more than 60 million people using a GreaterEqualsFilter Create a new instance of ContinuousQueryCache The result should consist of 2 countries only We add a new country Mexico to the original map Assert that the ContinuousQueryCache now contains 3 countries Under the covers, the ContinuousQueryCache will use Coherence cache events on the map to react to changes in the Coherence NamedCache . In order to create a ContinuousQueryCache without filtering, use the AlwaysFilter , e.g. new ContinuousQueryCache(map, AlwaysFilter.instance) . ",
            "title": "Using a ContinuousQueryCache"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " Proactively querying for updates is all fun and games but what if you need to execute logic as soon as data changes happen? The ContinuousQueryCache implements the ObservableMap interface to react to Coherence cache events. As such, you can subscribe to cache events by registering MapListener implementations. In the following test, we will add a MapListener to keep track of countries being added to the underlying NamedCache . But because this listener is added to the ContinuousQueryCache , the listener will only get invoked for countries that have a population of 60 million or more. <markup lang=\"java\" >@Test void testContinuousQueryCacheWithListener() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); ContinuousQueryCache results = new ContinuousQueryCache(map, filter); AtomicInteger counter = new AtomicInteger(0); MapListener&lt;String, Double&gt; listener = new SimpleMapListener&lt;String, Double&gt;() .addInsertHandler((event) -&gt; { counter.incrementAndGet(); }); results.addMapListener(listener); assertThat(results.size(), is(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); Eventually.assertDeferred(counter::get, is(1)); } Create a counter to keep track of added countries Instantiate a MapListener that will increment the counter for each new country being added Add the MapListener to the ContinuousQueryCache Assert that the ContinuousQueryCache contains 2 countries Add a new country with a population larger than 60 million Assert that the ContinuousQueryCache now contain 3 countries The counter of the MapListener should have increased by 1 ",
            "title": "Observing Continuous Query Caches"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " What about aggregated results? In an earlier example for Queries, we had used a Filter and a BigDecimalSum aggregator to calculate the sum of the population for those countries whose population is at least 60 million. We can use a MapListener to achieve that, as the ContinuousQueryCache does not directly support aggregators. <markup lang=\"java\" >@Test void testAggregate() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); ReflectionExtractor&lt;Country, Double&gt; extractor = new ReflectionExtractor&lt;&gt;(\"getPopulation\"); ContinuousQueryCache&lt;String, Country, Double&gt; results = new ContinuousQueryCache(map, filter, extractor); BigDecimalSum&lt;BigDecimal&gt; aggregator = new BigDecimalSum(new IdentityExtractor&lt;&gt;()); AtomicReference&lt;BigDecimal&gt; aggregatedPopulation = new AtomicReference&lt;&gt;(formatNumber(results.aggregate(aggregator))); MapListener&lt;String, Double&gt; listener = new SimpleMapListener&lt;String, Double&gt;() .addInsertHandler((event) -&gt; { aggregatedPopulation.set(formatNumber(results.aggregate(aggregator))); }); results.addMapListener(listener); assertThat(aggregatedPopulation.get(), is(formatNumber(150.6))); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); Eventually.assertDeferred(aggregatedPopulation::get, is(formatNumber(276.61))); } Create a BigDecimalSum aggregator. The IdentityExtractor will use the actual value (does not actually extract anything) Create a holder for the aggregated population and trigger the initial aggregation explicitly Instantiate a MapListener that will trigger the aggregation of the population for each new country being added Add the MapListener to the ContinuousQueryCache Assert that the aggregated population is initially 150.6 million Add a new country with a population larger than 60 million Assert that the ContinuousQueryCache now contain 3 countries The aggregated population should now be 276.61 million ",
            "title": "Continuous Aggregation"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " In this guide we showed, how you can easily create Views with a ContinuousQueryCache that reflects changes of the data in the underlying Coherence NamedCache in real-time. Please see the Coherence reference guide, specifically the chapter Using Continuous Query Caching for more details. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " Using Continuous Query Caching Using Map Events Querying Caches Built-In Aggregators ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " This guide walks you through the concepts of creating Views , also known as Continuous Queries . Views allow you to execute queries against your Coherence data with the added benefit that Views stay up-to-date, allowing you to retrieve the latest results of your query from the Coherence cache in real-time. Table of Contents What You Will Build What You Need Building The Example Code Example Data Model Why to use Views Using a ContinuousQueryCache Observing Continuous Query Caches Continuous Aggregation Summary See Also What You Will Build The example code is written as a set of unit tests, showing you how can create Views against your Coherence data. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Example Data Model The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . Why to use Views With Views , also referred to as Continuous Queries , you can ensure that a query always retrieves the latest results from a cache in real-time. For instance, in the queries guide , we used a Filter to query for a subset of data from a Coherence cache. However, what happens with the underlying cache if changes DO happen, and you need the updates immediately? Queries, as used previously, will only retrieve a snapshot of the underlying data. They will not reflect future data changes. Thus, let&#8217;s revisit a previous example that queries a cache containing Countries using a Filter . The Filter , as in the previous query example, will ensure that only countries with a population of 60 million or more people are returned. <markup lang=\"java\" >@Test void testGreaterEqualsFilterWithChanges() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); Set&lt;Map.Entry&lt;String, Country&gt;&gt; results = map.entrySet(filter); assertThat(results, hasSize(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results, hasSize(2)); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Apply the Filter by invoking entrySet(filter) on the NamedCache The result should be 2 countries only We add a new country Mexico to the map Assert that still only France and Germany were selected In this test we have added a new country Mexico to the countries Map but as you can see, the change will not be reflected in the already filtered results map. In order to get updates in real-time, we have to use a ContinuousQueryCache . Views are extremely useful in all those situations where we need immediate access to any changes of the underlying data, such as trading systems or Complex Event Processing (CEP) systems. They can be used in both client-based and server-based applications and are reminiscent of SQL Views. Using a ContinuousQueryCache The following test will look almost exactly the same as the previous test. However, instead of calling the entrySet() method on the NamedCache , we will create a new instance of ContinuousQueryCache and pass in the Filter and the Coherence map as constructor arguments. <markup lang=\"java\" >@Test void testGreaterEqualsFilterWithContinuousQueryCache() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); ContinuousQueryCache results = new ContinuousQueryCache(map, filter); assertThat(results.size(), is(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); } Get the countries Map We create a Filter that will select only countries with more than 60 million people using a GreaterEqualsFilter Create a new instance of ContinuousQueryCache The result should consist of 2 countries only We add a new country Mexico to the original map Assert that the ContinuousQueryCache now contains 3 countries Under the covers, the ContinuousQueryCache will use Coherence cache events on the map to react to changes in the Coherence NamedCache . In order to create a ContinuousQueryCache without filtering, use the AlwaysFilter , e.g. new ContinuousQueryCache(map, AlwaysFilter.instance) . Observing Continuous Query Caches Proactively querying for updates is all fun and games but what if you need to execute logic as soon as data changes happen? The ContinuousQueryCache implements the ObservableMap interface to react to Coherence cache events. As such, you can subscribe to cache events by registering MapListener implementations. In the following test, we will add a MapListener to keep track of countries being added to the underlying NamedCache . But because this listener is added to the ContinuousQueryCache , the listener will only get invoked for countries that have a population of 60 million or more. <markup lang=\"java\" >@Test void testContinuousQueryCacheWithListener() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); ContinuousQueryCache results = new ContinuousQueryCache(map, filter); AtomicInteger counter = new AtomicInteger(0); MapListener&lt;String, Double&gt; listener = new SimpleMapListener&lt;String, Double&gt;() .addInsertHandler((event) -&gt; { counter.incrementAndGet(); }); results.addMapListener(listener); assertThat(results.size(), is(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); Eventually.assertDeferred(counter::get, is(1)); } Create a counter to keep track of added countries Instantiate a MapListener that will increment the counter for each new country being added Add the MapListener to the ContinuousQueryCache Assert that the ContinuousQueryCache contains 2 countries Add a new country with a population larger than 60 million Assert that the ContinuousQueryCache now contain 3 countries The counter of the MapListener should have increased by 1 Continuous Aggregation What about aggregated results? In an earlier example for Queries, we had used a Filter and a BigDecimalSum aggregator to calculate the sum of the population for those countries whose population is at least 60 million. We can use a MapListener to achieve that, as the ContinuousQueryCache does not directly support aggregators. <markup lang=\"java\" >@Test void testAggregate() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter&lt;Country&gt; filter = Filters.greaterEqual(Country::getPopulation, 60.0); ReflectionExtractor&lt;Country, Double&gt; extractor = new ReflectionExtractor&lt;&gt;(\"getPopulation\"); ContinuousQueryCache&lt;String, Country, Double&gt; results = new ContinuousQueryCache(map, filter, extractor); BigDecimalSum&lt;BigDecimal&gt; aggregator = new BigDecimalSum(new IdentityExtractor&lt;&gt;()); AtomicReference&lt;BigDecimal&gt; aggregatedPopulation = new AtomicReference&lt;&gt;(formatNumber(results.aggregate(aggregator))); MapListener&lt;String, Double&gt; listener = new SimpleMapListener&lt;String, Double&gt;() .addInsertHandler((event) -&gt; { aggregatedPopulation.set(formatNumber(results.aggregate(aggregator))); }); results.addMapListener(listener); assertThat(aggregatedPopulation.get(), is(formatNumber(150.6))); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); Eventually.assertDeferred(aggregatedPopulation::get, is(formatNumber(276.61))); } Create a BigDecimalSum aggregator. The IdentityExtractor will use the actual value (does not actually extract anything) Create a holder for the aggregated population and trigger the initial aggregation explicitly Instantiate a MapListener that will trigger the aggregation of the population for each new country being added Add the MapListener to the ContinuousQueryCache Assert that the aggregated population is initially 150.6 million Add a new country with a population larger than 60 million Assert that the ContinuousQueryCache now contain 3 countries The aggregated population should now be 276.61 million Summary In this guide we showed, how you can easily create Views with a ContinuousQueryCache that reflects changes of the data in the underlying Coherence NamedCache in real-time. Please see the Coherence reference guide, specifically the chapter Using Continuous Query Caching for more details. See Also Using Continuous Query Caching Using Map Events Querying Caches Built-In Aggregators ",
            "title": "Views"
        },
        {
            "location": "/docs/core/01_overview",
            "text": " Learn about the features, enhancements, and changes made in Oracle Coherence CE since release 22.06. fa-rocket Topics Management Improvements Explore various Topics management improvements. fa-stethoscope Microprofile Health Integrate Eclipse MicroProfile Health in Coherence. fa-cogs Gradle POF Plugin Instrument Portable Types using Gradle. fa-exchange CDI Response Caching Use caching annotations for response caching. fa-random Virtual Threads Support Use Java 21 virtual threads within Coherence. fa-sort-alpha-asc Sorted Views Create sorted client-side views of cached data. fa-database Vector DB Use Coherence as a Vector Database. line_weight Queues Use Coherence blocking queue implementations. control_camera gRPC Use Coherence with gRPC clients speed Open Telemetry Use Coherence wth Open Telemetry. ",
            "title": "Overview"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " The coherence-micrometer module provides integration between Coherence metrics and Micrometer allowing Coherence metrics to be published via any of the Micrometer registries. ",
            "title": "Coherence Micrometer Metrics"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " In order to use Coherence Micrometer metrics, you need to declare the module as a dependency in your pom.xml and bind your Micrometer registry with the Coherence metrics adapter: <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-micrometer&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; The coherence-micrometer provides a Micrometer MeterBinder implementation class called CoherenceMicrometerMetrics . This class is a singleton and cannot be constructed, to access it use the CoherenceMicrometerMetrics.INSTANCE field. Micrometer provides many registry implementations to support different metrics applications and formats. For example, to bind Coherence metrics to the Micrometer PrometheusMeterRegistry , create the PrometheusMeterRegistry as documented in the Micrometer documentation , and call the CoherenceMicrometerMetrics class&#8217;s bindTo method: <markup lang=\"java\" >PrometheusMeterRegistry prometheusRegistry = new PrometheusMeterRegistry(PrometheusConfig.DEFAULT); // complete registy configuration... CoherenceMicrometerMetrics.INSTANCE.bindTo(prometheusRegistry); Micrometer registries can be bound to Coherence at any time, before or after Coherence starts. As Coherence creates or removed metrics they will be registered with or removed from the Micrometer registries. ",
            "title": "Usage"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " Micrometer has a global registry available which Coherence will bind to automatically if the coherence.micrometer.bind.to.global system property has been set to true (this property is false by default). ",
            "title": "Automatic Global Registry Binding"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " Imagine we were building a banking system where we had a Customer cache and an Accounts cache. A customer would have one entry in the Customers cache, but may have multiple accounts in the Accounts cache. In our application we want to write some functionality to transfer money between accounts, i.e. take from one account and add to another. We would like to do this atomically so that we do not take more money from an account than the current balance and also ensure the customer&#8217;s account balances are consistent. Say for customer \"Foo\", we want to transfer $100 from account \"A\" to account \"B\" Without a partition level transaction, the flow might look like this: <markup lang=\"java\" >accounts.invoke(new AccountKey(\"Foo\", \"A\"), new DebitProcessor(100)); accounts.invoke(new AccountKey(\"Foo\", \"B\"), new CreditProcessor(100)); The problem here is that the two account entries are in a distributed cache and may be on different JVMs in a cluster, probably on different physical servers. There will be a time window between the first invoke and the second where the total balance of the customer&#8217;s accounts is $100 less than it really is. There is also an issue where the second invoke fails for some reason, and we then need to handle this and put the $100 back into account \"A\" or keep re-trying. A better solution would be a single invoke, where we execute a single EntryProcessor that updates both accounts in a single transaction. The total balance of the accounts will remain consistent, and if the invoke call fails Coherence will automatically roll back all the affected entries. <markup lang=\"java\" >customers.invoke(new CustomerKey(\"Foo\"), new TransferProcessor(\"A\", \"B\", 100)); The rest of this guide explain how we accomplish this and some other techniques for handling related data. ",
            "title": "A Simple Use Case"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " In a Coherence distributed cache, data is managed in partitions. A cache entry is assigned to a partition and partition ownership is distributed over the storage enabled members of a cluster. When updating a cache entry using an EntryProcessor , Coherence performs this update atomically in what is known as a partition level transaction. In the EntryProcessor code it is possible to \"enlist\" other entries owned by the same partition into the transaction so that the mutations to those entries are committed to the partition&#8217;s storage as a single atomic update. The other entries enlisted into the transaction can be from the same cache, or from other caches that are managed by the same cache service. This is a very useful feature for a number of use cases that need to ensure atomic updates of related entries. A Simple Use Case Imagine we were building a banking system where we had a Customer cache and an Accounts cache. A customer would have one entry in the Customers cache, but may have multiple accounts in the Accounts cache. In our application we want to write some functionality to transfer money between accounts, i.e. take from one account and add to another. We would like to do this atomically so that we do not take more money from an account than the current balance and also ensure the customer&#8217;s account balances are consistent. Say for customer \"Foo\", we want to transfer $100 from account \"A\" to account \"B\" Without a partition level transaction, the flow might look like this: <markup lang=\"java\" >accounts.invoke(new AccountKey(\"Foo\", \"A\"), new DebitProcessor(100)); accounts.invoke(new AccountKey(\"Foo\", \"B\"), new CreditProcessor(100)); The problem here is that the two account entries are in a distributed cache and may be on different JVMs in a cluster, probably on different physical servers. There will be a time window between the first invoke and the second where the total balance of the customer&#8217;s accounts is $100 less than it really is. There is also an issue where the second invoke fails for some reason, and we then need to handle this and put the $100 back into account \"A\" or keep re-trying. A better solution would be a single invoke, where we execute a single EntryProcessor that updates both accounts in a single transaction. The total balance of the accounts will remain consistent, and if the invoke call fails Coherence will automatically roll back all the affected entries. <markup lang=\"java\" >customers.invoke(new CustomerKey(\"Foo\"), new TransferProcessor(\"A\", \"B\", 100)); The rest of this guide explain how we accomplish this and some other techniques for handling related data. ",
            "title": "What is a Partition Level Transaction"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " An important point about partition level transactions is that all the entries enlisted in the transaction must be owned by the same partition. In normal operation a cache entry is assigned to a partition based on a hash of the serialized key of the entry. This helps ensure an even distribution of entries over the partitions. When using partition level transactions we usually need to influence which partition a key is assigned to so that we ensure related entries are stored in the same partition, this is known as \"key affinity\", as it is the cache keys that control the owning partition. In most use cases we do not care exactly which partition a set of related keys belong to, rather we just need to ensure they all belong to the same partition. The Working with Partitions section of the official documentation covers the different techniques that can be used to control the partition an entry is assigned to. ",
            "title": "Key Affinity"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " Imagine a case where a customer \"Foo\" has two accounts, \"A\" and \"B\" and we initiate two transfers, one from \"A\" to \"B\" and the other from \"B\" to \"A\" using two separate invocations of TransferProcessor . Say we invoked TransferProcessor against the source account, we could see the following: Invoke TransferProcessor against account \"A\", the entry for account \"A\" is now locked in the accounts cache. Invoke TransferProcessor against account \"B\", the entry for account \"B\" is now locked in the accounts cache. The TransferProcessor that has locked account \"A\" now tries to enlist account \"B\", but account \"B\" is locked, so the TransferProcessor will now block waiting for account \"B\" to be unlocked. The TransferProcessor that has locked account \"B\" now tries to enlist account \"A\", but account \"A\" is locked, so the TransferProcessor will now block waiting for account \"A\" to be unlocked. We now have a deadlock, both TransferProcessor invocations are blocked waiting for each other to complete. When we invoke the TransferProcessor against the entry fo \"Foo\" in the customers cache the operation changes to look like this: Invoke TransferProcessor for \"A\" &#8594; \"B\" against customer \"Foo\", the entry for customer \"Foo\" is now locked in the customers cache. Invoke TransferProcessor for \"B\" &#8594; \"A\" against customer \"Foo\", the entry for customer \"Foo\" is already locked so this processor will wait. The TransferProcessor for \"A\" &#8594; \"B\" enlists accounts \"A\" and \"B\" and completes the transfer, unlocking customer \"Foo\" The TransferProcessor for \"B\" &#8594; \"A\" is now unblocked and can execute. By invoking the TransferProcessor against the customer entry we avoid the risk of deadlocks occurring. ",
            "title": "Invoke Against a Common Key"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " A second method to avoid deadlocks where multiple entries will be enlisted is to sort the keys that will be enlisted. Sometimes it is not possible, or it is inefficient, to invoke every EntryProcessor against a common key. In the example above, if we invoked every cache operation the customer entry, this could impact performance for some use cases because all the operations for a customer would queue and execute one at a time. Sometimes, for performance we&#8217;d like to execute operations in parallel as much as possible. An example of this might be where we have a cache that stores data that forms a nested tree type hierarchy, i.e a parent node that has child nodes which also have sub-child nodes. Say we have a use case where if we update a sub-child node, we have to apply a corresponding update to its parents. We could do what we did above and only ever run the EntryProcessor against the top level parent, then enlist entries going down the hierarchy to the sub-child we want to update. If our application applies a lot of updates to sub-child nodes, this would be a big performance bottleneck, as updates could only be applied sequentially. Instead, in this use case the EntryProcessor would execute against the sub-child, then search for and enlist its parents. How we might search for parents will be covered in an example below. For example, we have a parent node \"A\", that has a child node \"A-1\", which has two sub-child nodes \"A-1.1\" and \"A-1.2\". Without sorting the enlisted entries this is what could happen. Invoke the EntryProcessor \"One\" against node \"A-1.1\", so entry \"A-1.1\" is locked. Invoke the EntryProcessor \"Two\" against node \"A-1.2\", so entry \"A-1.2\" is locked. EntryProcessor \"One\" searches for its parent nodes, the resulting keys come back as `[\"A\", \"A-1\"], so the first of those \"A\" is serialized and used to enlist entry \"A\", and locked. Before EntryProcessor \"One\" can lock the second result, \"A-1\", EntryProcessor \"Two\" has searched for its parents and has a result of `[\"A-1\", \"A\"] (the opposite order to \"One\"), so it enlists and locks \"A-1\". EntryProcessor \"One\" now tries to enlist \"A-1\", but is blocked because this is locked by \"Two\". EntryProcessor \"Two\" now tries to enlist \"A\", but is blocked because this is locked by \"One\". We now have a deadlock The solution to the deadlock above is for the EntryProcessor to sort the keys of the entries that it will enlist before enlisting them. The simplest way to do this is to sort the serialized Binary keys that will be used to enlist the entries, because com.tangosol.util.Binary implements Comparable . The flow then becomes: Invoke the EntryProcessor \"One\" against node \"A-1.1\", so entry \"A-1.1\" is locked. Invoke the EntryProcessor \"Two\" against node \"A-1.2\", so entry \"A-1.2\" is locked. EntryProcessor \"One\" searches for its parent nodes, the result comes back as `[\"A\", \"A-1\"] EntryProcessor \"One\" serializes the keys to Binary keys sorts them, e.g. in a sorted list or TreeSet . EntryProcessor \"One\" now enlists the first sorted key, say it is \"A\". Before EntryProcessor \"One\" can lock the second result, \"A-1\", EntryProcessor \"Two\" has searched for its parents and has a result of `[\"A-1\", \"A\"] (the opposite order to \"One\") EntryProcessor \"Two\" serializes the keys to Binary keys sorts them. EntryProcessor \"Two\" now enlists the first sorted key, which will also now be \"A\", but \"A\" is already locked, so EntryProcessor \"Two\" is blocked. EntryProcessor \"One\" can now proceed to enlist and lock \"A-1\", perform its updates of \"A\" and \"A-1\", then complete, unlocking all the entries EntryProcessor \"Two\" can now enist and lock \"A-1\" and complete its processing. ",
            "title": "Sort the Keys to be Enlisted"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " It is very important when enlisting other entries in an EntryProcessor that this is done in a way that avoids deadlocks. There are two ways to avoid deadlocks covered below. Invoke Against a Common Key Imagine a case where a customer \"Foo\" has two accounts, \"A\" and \"B\" and we initiate two transfers, one from \"A\" to \"B\" and the other from \"B\" to \"A\" using two separate invocations of TransferProcessor . Say we invoked TransferProcessor against the source account, we could see the following: Invoke TransferProcessor against account \"A\", the entry for account \"A\" is now locked in the accounts cache. Invoke TransferProcessor against account \"B\", the entry for account \"B\" is now locked in the accounts cache. The TransferProcessor that has locked account \"A\" now tries to enlist account \"B\", but account \"B\" is locked, so the TransferProcessor will now block waiting for account \"B\" to be unlocked. The TransferProcessor that has locked account \"B\" now tries to enlist account \"A\", but account \"A\" is locked, so the TransferProcessor will now block waiting for account \"A\" to be unlocked. We now have a deadlock, both TransferProcessor invocations are blocked waiting for each other to complete. When we invoke the TransferProcessor against the entry fo \"Foo\" in the customers cache the operation changes to look like this: Invoke TransferProcessor for \"A\" &#8594; \"B\" against customer \"Foo\", the entry for customer \"Foo\" is now locked in the customers cache. Invoke TransferProcessor for \"B\" &#8594; \"A\" against customer \"Foo\", the entry for customer \"Foo\" is already locked so this processor will wait. The TransferProcessor for \"A\" &#8594; \"B\" enlists accounts \"A\" and \"B\" and completes the transfer, unlocking customer \"Foo\" The TransferProcessor for \"B\" &#8594; \"A\" is now unblocked and can execute. By invoking the TransferProcessor against the customer entry we avoid the risk of deadlocks occurring. Sort the Keys to be Enlisted A second method to avoid deadlocks where multiple entries will be enlisted is to sort the keys that will be enlisted. Sometimes it is not possible, or it is inefficient, to invoke every EntryProcessor against a common key. In the example above, if we invoked every cache operation the customer entry, this could impact performance for some use cases because all the operations for a customer would queue and execute one at a time. Sometimes, for performance we&#8217;d like to execute operations in parallel as much as possible. An example of this might be where we have a cache that stores data that forms a nested tree type hierarchy, i.e a parent node that has child nodes which also have sub-child nodes. Say we have a use case where if we update a sub-child node, we have to apply a corresponding update to its parents. We could do what we did above and only ever run the EntryProcessor against the top level parent, then enlist entries going down the hierarchy to the sub-child we want to update. If our application applies a lot of updates to sub-child nodes, this would be a big performance bottleneck, as updates could only be applied sequentially. Instead, in this use case the EntryProcessor would execute against the sub-child, then search for and enlist its parents. How we might search for parents will be covered in an example below. For example, we have a parent node \"A\", that has a child node \"A-1\", which has two sub-child nodes \"A-1.1\" and \"A-1.2\". Without sorting the enlisted entries this is what could happen. Invoke the EntryProcessor \"One\" against node \"A-1.1\", so entry \"A-1.1\" is locked. Invoke the EntryProcessor \"Two\" against node \"A-1.2\", so entry \"A-1.2\" is locked. EntryProcessor \"One\" searches for its parent nodes, the resulting keys come back as `[\"A\", \"A-1\"], so the first of those \"A\" is serialized and used to enlist entry \"A\", and locked. Before EntryProcessor \"One\" can lock the second result, \"A-1\", EntryProcessor \"Two\" has searched for its parents and has a result of `[\"A-1\", \"A\"] (the opposite order to \"One\"), so it enlists and locks \"A-1\". EntryProcessor \"One\" now tries to enlist \"A-1\", but is blocked because this is locked by \"Two\". EntryProcessor \"Two\" now tries to enlist \"A\", but is blocked because this is locked by \"One\". We now have a deadlock The solution to the deadlock above is for the EntryProcessor to sort the keys of the entries that it will enlist before enlisting them. The simplest way to do this is to sort the serialized Binary keys that will be used to enlist the entries, because com.tangosol.util.Binary implements Comparable . The flow then becomes: Invoke the EntryProcessor \"One\" against node \"A-1.1\", so entry \"A-1.1\" is locked. Invoke the EntryProcessor \"Two\" against node \"A-1.2\", so entry \"A-1.2\" is locked. EntryProcessor \"One\" searches for its parent nodes, the result comes back as `[\"A\", \"A-1\"] EntryProcessor \"One\" serializes the keys to Binary keys sorts them, e.g. in a sorted list or TreeSet . EntryProcessor \"One\" now enlists the first sorted key, say it is \"A\". Before EntryProcessor \"One\" can lock the second result, \"A-1\", EntryProcessor \"Two\" has searched for its parents and has a result of `[\"A-1\", \"A\"] (the opposite order to \"One\") EntryProcessor \"Two\" serializes the keys to Binary keys sorts them. EntryProcessor \"Two\" now enlists the first sorted key, which will also now be \"A\", but \"A\" is already locked, so EntryProcessor \"Two\" is blocked. EntryProcessor \"One\" can now proceed to enlist and lock \"A-1\", perform its updates of \"A\" and \"A-1\", then complete, unlocking all the entries EntryProcessor \"Two\" can now enist and lock \"A-1\" and complete its processing. ",
            "title": "Avoiding Deadlocks in EntryProcessors"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " We will need a few entity classes, a simple Customer class and an Account class. We will also create key classes for both of these. We could use a simple class from the JVM for the customer key such as a String , a number or a UUID, but in this case we&#8217;ll create a custom class. We need to use key affinity to ensure that the accounts for a customer a located in the same partition as the customer. To do this we will make the AccountId class implement the Coherence com.tangosol.net.cache.KeyAssociation interface. The getAssociatedKey method just returns the customerId field. <markup lang=\"java\" >@Override public CustomerId getAssociatedKey() { return customerId; } The full AccountId class is shown below: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/bank/AccountId.java\" >public class AccountId implements ExternalizableLite, PortableObject, KeyAssociation&lt;CustomerId&gt; { // ----- constructors --------------------------------------------------- /** * A default no-args constructor required for serialization. */ public AccountId() { } /** * Create a {@link AccountId}. * * @param id the id of the customer */ public AccountId(CustomerId customerId, String id) { this.customerId = customerId; this.id = id; } // ----- KeyAssociation methods ----------------------------------------- @Override public CustomerId getAssociatedKey() { return customerId; } // ----- accessors ------------------------------------------------------ /** * Return the id. * * @return the customer id */ public CustomerId getCustomerId() { return customerId; } /** * Return the account id. * * @return the customer id */ public String getId() { return id; } // ----- Object methods ------------------------------------------------- // Coherence key classes must properly implement equals() using // all the fields in the class @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } AccountId accountId = (AccountId) o; return Objects.equals(customerId, accountId.customerId) &amp;&amp; Objects.equals(id, accountId.id); } // Coherence key classes must properly implement hashCode() using // all the fields in the class @Override public int hashCode() { return Objects.hash(customerId, id); } // ----- ExternalizableLite methods ------------------------------------- @Override public void readExternal(DataInput in) throws IOException { customerId = ExternalizableHelper.readObject(in); id = ExternalizableHelper.readSafeUTF(in); } @Override public void writeExternal(DataOutput out) throws IOException { ExternalizableHelper.writeObject(out, customerId); ExternalizableHelper.writeSafeUTF(out, id); } // ----- PortableObject methods ----------------------------------------- @Override public void readExternal(PofReader in) throws IOException { customerId = in.readObject(0); id = in.readString(1); } @Override public void writeExternal(PofWriter out) throws IOException { out.writeObject(0, customerId); out.writeString(1, id); } // ----- data members --------------------------------------------------- /** * The id of the customer. */ private CustomerId customerId; /** * The id of the account. */ private String id; } ",
            "title": "Entity Classes"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " Now we know customers and accounts are co-located we can write the TransferProcessor . <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/bank/TransferProcessor.java\" >public class TransferProcessor extends AbstractEvolvable implements InvocableMap.EntryProcessor&lt;CustomerId, Customer, Void&gt;, ExternalizableLite, PortableObject { } The generic parameters for the TransferProcessor are &lt;CustomerId, Customer, Void&gt; because the processor will be invoked against the customers cache, which has a key of CustomerId and a value of Customer . In this case we do not return a result from the processor, so its return type is Void . The process method would look like this (with comments to explain the code): <markup lang=\"java\" > @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;CustomerId, Customer&gt; entry) { // Convert the entry to a BinaryEntry BinaryEntry&lt;CustomerId, Customer&gt; binaryEntry = entry.asBinaryEntry(); // Obtain the backing map manager context BackingMapManagerContext context = binaryEntry.getContext(); // Obtain the converter to use to convert the account identifiers // into Coherence internal serialized binary format // It is important to use the correct key converter for this conversion Converter&lt;AccountId, Binary&gt; keyConverter = context.getKeyToInternalConverter(); // Obtain the backing map context for the accounts cache BackingMapContext accountsContext = context.getBackingMapContext(\"accounts\"); // Convert the source account id to a binary key and obtain the cache entry for the source account Binary sourceKey = keyConverter.convert(sourceAccount); InvocableMap.Entry&lt;AccountId, Account&gt; sourceEntry = accountsContext.getBackingMapEntry(sourceKey); // Convert the destination account id to a binary key and obtain the cache entry for the destination account Binary destinationKey = keyConverter.convert(destinationAccount); InvocableMap.Entry&lt;AccountId, Account&gt; destinationEntry = accountsContext.getBackingMapEntry(destinationKey); // adjust the values for the two accounts Account sourceAccount = sourceEntry.getValue(); sourceAccount.adjustBalance(amount.negate()); // set the updated source account back into the entry so that the cache is updated sourceEntry.setValue(sourceAccount); Account destinationAccount = destinationEntry.getValue(); destinationAccount.adjustBalance(amount); // set the updated destination account back into the entry so that the cache is updated destinationEntry.setValue(destinationAccount); return null; } Note You must remember to call setValue() on the entries that have been updated passing in the updated values so that Coherence knows to update the underlying cache entry. Just mutating the value returned from entry.getValue() will not cause a cache update. In the call to BackingMapContext accountsContext = context.getBackingMapContext(\"accounts\"); better coding practice would be to have a common set of a static constants for the cache names in our application code instead of using hard coded string values. ",
            "title": "The Transfer EntryProcessor"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " For the first example in this guide, we will implement the simple bank account use case described above to transfer money between two accounts for the same customer. Entity Classes We will need a few entity classes, a simple Customer class and an Account class. We will also create key classes for both of these. We could use a simple class from the JVM for the customer key such as a String , a number or a UUID, but in this case we&#8217;ll create a custom class. We need to use key affinity to ensure that the accounts for a customer a located in the same partition as the customer. To do this we will make the AccountId class implement the Coherence com.tangosol.net.cache.KeyAssociation interface. The getAssociatedKey method just returns the customerId field. <markup lang=\"java\" >@Override public CustomerId getAssociatedKey() { return customerId; } The full AccountId class is shown below: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/bank/AccountId.java\" >public class AccountId implements ExternalizableLite, PortableObject, KeyAssociation&lt;CustomerId&gt; { // ----- constructors --------------------------------------------------- /** * A default no-args constructor required for serialization. */ public AccountId() { } /** * Create a {@link AccountId}. * * @param id the id of the customer */ public AccountId(CustomerId customerId, String id) { this.customerId = customerId; this.id = id; } // ----- KeyAssociation methods ----------------------------------------- @Override public CustomerId getAssociatedKey() { return customerId; } // ----- accessors ------------------------------------------------------ /** * Return the id. * * @return the customer id */ public CustomerId getCustomerId() { return customerId; } /** * Return the account id. * * @return the customer id */ public String getId() { return id; } // ----- Object methods ------------------------------------------------- // Coherence key classes must properly implement equals() using // all the fields in the class @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } AccountId accountId = (AccountId) o; return Objects.equals(customerId, accountId.customerId) &amp;&amp; Objects.equals(id, accountId.id); } // Coherence key classes must properly implement hashCode() using // all the fields in the class @Override public int hashCode() { return Objects.hash(customerId, id); } // ----- ExternalizableLite methods ------------------------------------- @Override public void readExternal(DataInput in) throws IOException { customerId = ExternalizableHelper.readObject(in); id = ExternalizableHelper.readSafeUTF(in); } @Override public void writeExternal(DataOutput out) throws IOException { ExternalizableHelper.writeObject(out, customerId); ExternalizableHelper.writeSafeUTF(out, id); } // ----- PortableObject methods ----------------------------------------- @Override public void readExternal(PofReader in) throws IOException { customerId = in.readObject(0); id = in.readString(1); } @Override public void writeExternal(PofWriter out) throws IOException { out.writeObject(0, customerId); out.writeString(1, id); } // ----- data members --------------------------------------------------- /** * The id of the customer. */ private CustomerId customerId; /** * The id of the account. */ private String id; } The Transfer EntryProcessor Now we know customers and accounts are co-located we can write the TransferProcessor . <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/bank/TransferProcessor.java\" >public class TransferProcessor extends AbstractEvolvable implements InvocableMap.EntryProcessor&lt;CustomerId, Customer, Void&gt;, ExternalizableLite, PortableObject { } The generic parameters for the TransferProcessor are &lt;CustomerId, Customer, Void&gt; because the processor will be invoked against the customers cache, which has a key of CustomerId and a value of Customer . In this case we do not return a result from the processor, so its return type is Void . The process method would look like this (with comments to explain the code): <markup lang=\"java\" > @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;CustomerId, Customer&gt; entry) { // Convert the entry to a BinaryEntry BinaryEntry&lt;CustomerId, Customer&gt; binaryEntry = entry.asBinaryEntry(); // Obtain the backing map manager context BackingMapManagerContext context = binaryEntry.getContext(); // Obtain the converter to use to convert the account identifiers // into Coherence internal serialized binary format // It is important to use the correct key converter for this conversion Converter&lt;AccountId, Binary&gt; keyConverter = context.getKeyToInternalConverter(); // Obtain the backing map context for the accounts cache BackingMapContext accountsContext = context.getBackingMapContext(\"accounts\"); // Convert the source account id to a binary key and obtain the cache entry for the source account Binary sourceKey = keyConverter.convert(sourceAccount); InvocableMap.Entry&lt;AccountId, Account&gt; sourceEntry = accountsContext.getBackingMapEntry(sourceKey); // Convert the destination account id to a binary key and obtain the cache entry for the destination account Binary destinationKey = keyConverter.convert(destinationAccount); InvocableMap.Entry&lt;AccountId, Account&gt; destinationEntry = accountsContext.getBackingMapEntry(destinationKey); // adjust the values for the two accounts Account sourceAccount = sourceEntry.getValue(); sourceAccount.adjustBalance(amount.negate()); // set the updated source account back into the entry so that the cache is updated sourceEntry.setValue(sourceAccount); Account destinationAccount = destinationEntry.getValue(); destinationAccount.adjustBalance(amount); // set the updated destination account back into the entry so that the cache is updated destinationEntry.setValue(destinationAccount); return null; } Note You must remember to call setValue() on the entries that have been updated passing in the updated values so that Coherence knows to update the underlying cache entry. Just mutating the value returned from entry.getValue() will not cause a cache update. In the call to BackingMapContext accountsContext = context.getBackingMapContext(\"accounts\"); better coding practice would be to have a common set of a static constants for the cache names in our application code instead of using hard coded string values. ",
            "title": "Bank Account Example"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " In this example the data model is a bookseller, with a lot of books. We have a cache holding sales for books by region, where the region forms a hierarchy, e.g. \"World\", \"Continent\", and \"Country\". We might have data like this: <markup >- \"The Great Gatsby\", \"World\", 26 - \"The Great Gatsby\", \"North America\", 22 - \"The Great Gatsby\", \"US\", 19 - \"The Great Gatsby\", \"Canada\", 3 - \"The Great Gatsby\", \"Europe\", 4 - \"The Great Gatsby\", \"France\", 1 - \"The Great Gatsby\", \"UK\", 3 The key class might look like this: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/SalesId.java\" >public class SalesId implements ExternalizableLite, PortableObject, KeyAssociation&lt;String&gt; { // ----- constructors --------------------------------------------------- /** * A default no-args constructor required for serialization. */ public SalesId() { } /** * Create a sales identifier. * * @param bookId the book identifier * @param regionCode the region identifier * @param parentRegionCode the parent region identifier, or {@code null} * if this is a top level region */ public SalesId(String bookId, String regionCode, String parentRegionCode) { this.bookId = bookId; this.regionCode = regionCode; this.parentRegionCode = parentRegionCode; } // ----- KeyAssociation methods ----------------------------------------- @Override public String getAssociatedKey() { return bookId; } // ----- accessors ------------------------------------------------------ /** * Return the book identifier. * * @return the book identifier */ public String getBookId() { return bookId; } /** * Return the region identifier. * * @return the region identifier */ public String getRegionCode() { return regionCode; } /** * Return the parent region identifier, or {@code null} * if this is a top level region. * * @return the parent region identifier, or {@code null} * if this is a top level region */ public String getParentRegionCode() { return parentRegionCode; } // ----- Object methods ------------------------------------------------- // Coherence key classes must properly implement hashCode() using // all the fields in the class @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } SalesId salesId = (SalesId) o; return Objects.equals(bookId, salesId.bookId) &amp;&amp; Objects.equals(regionCode, salesId.regionCode) &amp;&amp; Objects.equals(parentRegionCode, salesId.parentRegionCode); } // Coherence key classes must properly implement hashCode() using // all the fields in the class @Override public int hashCode() { return Objects.hash(bookId, regionCode, parentRegionCode); } // serialization methods omitted... // ----- data members --------------------------------------------------- /** * The identifier for the ook */ private String bookId; /** * The region code for the sales data. */ private String regionCode; /** * The parent region code, or {@code null} if this is a top level region. */ private String parentRegionCode; } The SalesId class uses the bookId as the associated key so that all sales for a book are co-located in a single partition. The SalesId has a reference to the parent region, for example the parent of the region \"France\", is \"Europe\" and the parent of \"Europe\" is \"World\". We can also create a simple BookSales class to hold the sales information for a book, we might hold sales for e-books, audiobooks and paper books. We can add methods on the BookSales class to get, set and increment the different sales numbers. ",
            "title": "Entity Classes"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " Now we have some entity classes to hold the sales data we can create an EntryProcessor that will update the sales for a book. The operation of the EntryProcessor would be the following. The EntryProcessor has parameters for the additional sales in each category, paper book, e-book and audiobook for a region. The EntryProcessor is invoked against the SalesId to be updated The EntryProcessor updates the sales data for the region The EntryProcessor then finds the parent regions in the hierarchy and updates them The EntryProcessor class might look like the code below. The process() method is empty and will be completed in the next section. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >public class IncrementSalesProcessor extends AbstractEvolvable implements InvocableMap.EntryProcessor&lt;SalesId, BookSales, Void&gt;, ExternalizableLite, PortableObject { // ----- constructors --------------------------------------------------- /** * Default no-args constructor required for serialization. */ public IncrementSalesProcessor() { } /** * Create a {@link IncrementSalesProcessor}. * * @param eBook the e-book sales * @param audio the audiobook sales * @param paper the paper book sales */ public IncrementSalesProcessor(long eBook, long audio, long paper) { this.eBook = eBook; this.audio = audio; this.paper = paper; } // ----- EntryProcessor methods ----------------------------------------- @Override public Void process(InvocableMap.Entry&lt;CustomerId, Customer&gt; entry) { return null; } // ----- serialization methods ------------------------------------------ @Override public int getImplVersion() { return IMPLEMENTATION_VERSION; } @Override public void readExternal(DataInput in) throws IOException { eBook = in.readLong(); audio = in.readLong(); paper = in.readLong(); } @Override public void writeExternal(DataOutput out) throws IOException { out.writeLong(eBook); out.writeLong(audio); out.writeLong(paper); } @Override public void readExternal(PofReader in) throws IOException { eBook = in.readLong(0); audio = in.readLong(1); paper = in.readLong(2); } @Override public void writeExternal(PofWriter out) throws IOException { out.writeLong(0, eBook); out.writeLong(1, audio); out.writeLong(2, paper); } // ----- data members --------------------------------------------------- /** * The evolvable POF implementation version of this class. */ public static final int IMPLEMENTATION_VERSION = 1; /** * The number of e-books sold. */ private long eBook; /** * The number of audiobooks sold. */ private long audio; /** * The number of paper sold. */ private long paper; } In the IncrementSalesProcessor we can start to create the process method to update the sales data in the entry that the IncrementSalesProcessor will be invoked on, as shown below: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >@Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;SalesId, BookSales&gt; entry) { // Obtain a BinaryEntry from the entry being processes BinaryEntry&lt;SalesId, BookSales&gt; binaryEntry = entry.asBinaryEntry(); // update the entry sales data BookSales sales; if (entry.isPresent()) { // the parent entry is present sales = entry.getValue(); } else { // The parent entry is not present, so set a new BookSales value sales = new BookSales(); } sales.incrementEBookSales(eBook); sales.incrementAudioSales(audio); sales.incrementPaperSales(paper); // set the updated sale value back into the entry so that Coherence updates the cache entry.setValue(sales); // Obtain a sorted set of the Binary keys of the parents of the entry being processed // We do not need to return anything return null; } ",
            "title": "The Increment Sales EntryProcessor"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " One of the functions of the IncrementSalesProcessor class that is missing from the process method above is to update the parent regions in the hierarchy. When the entry processor is invoked against an entry, we do not have the full key for the parent entry, we only know the book and region. This means we cannot just enlist the entry based on a key, as we do not know the key. The solution to this is to perform a query to locate the parent, as we know the parent will be in the same partition, as it has the same book id. The entry that is passed to the process method can be turned into a BinaryEntry and from this we can obtain the cache&#8217;s backing map. The backing map in Coherence is the actual Map instance that holds the data for the partition. This is typically serialized binary data. So even if the generics for the entry are real types (in this case the SalesId and BookSales classes) the backing map keys and values will be Binary instances. The two lines below show how to convert the entry passed to the process method into a BinaryEntry . <markup lang=\"java\" >BinaryEntry&lt;SalesId, BookSales&gt; binaryEntry = entry.asBinaryEntry(); BackingMapContext backingMapContext = binaryEntry.getBackingMapContext(); Now we have the BackingMapContext we can write some additional methods to search for the parents of the entry being processed. ",
            "title": "Partition Local Queries"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " When inside an entry processor or aggregator, the backing map of a cache is of the type Map&lt;Binary, Binary&gt; , because the backing map holds the serialized cache data. The simplest way to query the backing map is to use normal Coherence Filter classes, which typically take a ValueExtractor to extract the field to be tested in the filter. We could write a ValueExtractor that can deserialize the Binary key or value and extract the required field, and that would work fine. But, we can be a bit smarter here, so that we can utilize any indexes that may exist on the cache. If we wrote a custom binary extractor, that in this example extracted the bookId from the key, and then created in index on that, we would need to create a second index for any normal cache queries that used the bookId . It would be far better to be able to just create a single index. In this example we create a special wrapper class named BinaryValueExtractor that will extract values from a serialized Binary using a wrapped ValueExtractor . When the BinaryValueExtractor is used in a Filter that may use an index, we want the index lookup to retrieve any index created by the wrapped extractor, and to do this the BinaryValueExtractor.getCanonicalName() method will return the same value as the delegate ValueExtractor . <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/BinaryValueExtractor.java\" >public class BinaryValueExtractor&lt;T, E&gt; implements ValueExtractor&lt;Binary, E&gt; { /** * Create a {@link BinaryValueExtractor}. * * @param delegate the extractor to delegate to * @param converter the {@link Converter} to convert the {@link Binary} value * to a value to pass to the delegate {@link ValueExtractor} */ public BinaryValueExtractor(ValueExtractor&lt;T, E&gt; delegate, Converter&lt;Binary, T&gt; converter) { m_delegate = delegate; m_converter = converter; } // ----- ValueExtractor ------------------------------------------------- @Override public E extract(Binary target) { T value = m_converter.convert(target); return m_delegate.extract(value); } @Override public int getTarget() { return m_delegate.getTarget(); } @Override public String getCanonicalName() { return m_delegate.getCanonicalName(); } // ----- helper methods ------------------------------------------------- /** * A factory method to create a {@link BinaryValueExtractor}. * * @param delegate the extractor to delegate to * @param converter the {@link Converter} to convert the {@link Binary} value * to a value to pass to the delegate {@link ValueExtractor} * * @return the {@link BinaryValueExtractor} that will extract from a {@link Binary} value * * @param &lt;T&gt; the underlying type to extract from after being deserialized * @param &lt;E&gt; the type of the extracted value */ public static &lt;T, E&gt; ValueExtractor&lt;Binary, E&gt; of(ValueExtractor&lt;T, E&gt; delegate, Converter&lt;Binary, T&gt; converter) { return new BinaryValueExtractor&lt;&gt;(delegate, converter); } // ----- data members --------------------------------------------------- /** * The delegate {@link ValueExtractor}. */ private final ValueExtractor&lt;T, E&gt; m_delegate; /** * The {@link Converter} to convert the {@link Binary} value * to a value to pass to the delegate {@link ValueExtractor} */ private final Converter&lt;Binary, T&gt; m_converter; } We can now use the BinaryValueExtractor in a Filter inside an entry processor or aggregator. For example the code below creates an EqualsFilter that can execute against the binary backing map. The filter will match any entry in the backing map that has a key ( SalesId ) where the getBookId() method returns \"Foo\": <markup lang=\"java\" >Filter&lt;?&gt; filter = Filters.equal(BinaryValueExtractor.of(SalesId::getBookId, converter).fromKey(), \"Foo\"); Outside an entry processor or aggregator, the same query using the normal cache API would be: <markup lang=\"java\" >Filter&lt;?&gt; filter = Filters.equal(ValueExtractor.of(SalesId::getBookId, converter).fromKey(), \"Foo\"); If we want to make the filter more efficient in both cases, we would create an index on the cache as normal using the normal non-binary extractor: <markup lang=\"java\" >NamedCache&lt;SalesId, BookSales&gt; cache = session.getCache(\"book-sales\"); cache.addIndex(ValueExtractor.of(SalesId::getBookId).fromKey()); Note The BinaryValueExtractor is intentionally not serializable as it is not meant ot be used in normal filter queries. ",
            "title": "Querying a Binary Backing Map"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " The method below shows how to use the BinaryValueExtractor in filters to query the cache backing map, in this case to obtain the single parent entry, but it may be used in other use-cases to query for multiple entries. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >private Map.Entry&lt;SalesId, BookSales&gt; getParent(SalesId id, BackingMapContext backingMapContext) { ObservableMap&lt;Binary, Binary&gt; backingMap = backingMapContext.getBackingMap(); Map&lt;ValueExtractor, MapIndex&gt; indexMap = backingMapContext.getIndexMap(); String bookId = id.getBookId(); String region = id.getParentRegionCode(); Filter&lt;?&gt; filter = Filters.equal(BinaryValueExtractor.of(SalesId::getBookId, converter).fromKey(), bookId) .and(Filters.equal(BinaryValueExtractor.of(SalesId::getRegionCode, converter).fromKey(), region)); Set&lt;Map.Entry&lt;Binary, Binary&gt;&gt; setEntries = InvocableMapHelper.query(backingMap, indexMap, filter, true, false, null); // there should only ever be zero or one matching entry return setEntries.stream() .findFirst() .orElse(null); } The code above works as follows: * Obtain the backing map from the backing map context * Obtain the map of indexes present on the cache * Obtain the bookId and region for the parent entry we want to find * Create a Coherence Filter that will query the baking map keys for a matching entry. * Execute the query on the backing map using Coherence&#8217;s InvocableMapHelper utility class * There should only be a single matching entry (or maybe no match) so extract the first entry from the query results and return it Note Accessing the backing map using backingMapContext.getBackingMap() is marked as deprecated. The main reason for the deprecation is to discourage direct use of the backing map in application code. Direct manipulation of the data in the backing map by application code is dangerous and could result in corruption of the cache. In this use-case there is currently no alternative API to perform a partition local query, but as we are only reading data from the map it is safe. The contents of the map may be changed by other threads that are executing Coherence requests on the same partition while the query is in progress. This means that any result returned by a query should be considered transitive, just like any query results from an active cache. Now we have a method that can obtain the parent entry for a SalesId key we can write another utility method that will obtain all the parent keys in the hierarchy for a given SalesId . As already mentioned above, we will sort this set of keys so that when the entries are enlisted we avoid deadlocks. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >private SortedSet&lt;Binary&gt; getParentKeys(SalesId key, BackingMapContext backingMapContext) { TreeSet&lt;Binary&gt; parents = new TreeSet&lt;&gt;(); Converter&lt;Binary, SalesId&gt; converter = backingMapContext.getManagerContext().getKeyFromInternalConverter(); Map.Entry&lt;Binary, Binary&gt; parent = getParent(key, backingMapContext); while (parent != null) { Binary binaryKey = parent.getKey(); parents.add(binaryKey); key = converter.convert(binaryKey); parent = getParent(key, backingMapContext); } return parents; } With the additional methods above, we can now complete the process method in the IncrementSalesProcessor class as shown below. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >public Void process(InvocableMap.Entry&lt;SalesId, BookSales&gt; entry) { // update the entry sales data BookSales sales; if (entry.isPresent()) { // the entry is present sales = entry.getValue(); } else { // The parent entry is not present, so create a new BookSales value sales = new BookSales(); } sales.incrementEBookSales(eBook); sales.incrementAudioSales(audio); sales.incrementPaperSales(paper); // set the updated sale value back into the entry so that Coherence updates the cache entry.setValue(sales); // Obtain a BinaryEntry from the entry being processes BinaryEntry&lt;SalesId, BookSales&gt; binaryEntry = entry.asBinaryEntry(); // Obtain the BackingMapContext for the entry BackingMapContext backingMapContext = binaryEntry.getBackingMapContext(); // Obtain a sorted set of the Binary keys of the parents of the entry being processed SortedSet&lt;Binary&gt; parentKeys = getParentKeys(entry.getKey(), backingMapContext); // Iterate over the parent keys, enlisting and updating each parent entry for (Binary binaryKey : parentKeys) { InvocableMap.Entry&lt;SalesId, BookSales&gt; parentEntry = backingMapContext.getBackingMapEntry(binaryKey); if (parentEntry.isPresent()) { // the parent entry is present sales = parentEntry.getValue(); } else { // The parent entry is not present, so create a new BookSales value sales = new BookSales(); } // update the parent sales data sales.incrementEBookSales(eBook); sales.incrementAudioSales(audio); sales.incrementPaperSales(paper); // set the updated sale value back into the entry so that Coherence updates the cache parentEntry.setValue(sales); } return null; } ",
            "title": "Finding the Book Sales parent"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " In the code above that queries the backing map, the actual query call looks like this: <markup lang=\"java\" >Set&lt;Map.Entry&lt;SalesId, BookSales&gt;&gt; setEntries = InvocableMapHelper.query(map, indexMap, filter, true, false, null); You can see that the second parameter is a Map of indexes, which is obtained from the BackingMapContext by calling backingMapContext.getIndexMap() . This Map is the map of indexes that have been added to the cache by calls to one of the NamedCache.addIndex() methods. Using the cache indexes in this way can make a big difference to the speed and efficiently of the query execution. In the example above the Filter for the query is created like this: <markup lang=\"java\" >Filter&lt;?&gt; filter = Filters.equal(ValueExtractor.of(SalesId::getBookId).fromKey(), bookId) .and(Filters.equal(ValueExtractor.of(SalesId::getRegionCode).fromKey(), region)); The Filter is an \"and\" filter which uses two ValueExtractor instances: <markup lang=\"java\" >ValueExtractor.of(SalesId::getBookId).fromKey() ValueExtractor.of(SalesId::getRegionCode).fromKey() If we create indexes on the cache using the same extractors, then the query will be much faster: <markup lang=\"java\" >NamedCache&lt;SalesId, BookSales&gt; cache = session.getCache(\"book-sales\"); cache.addIndex(ValueExtractor.of(SalesId::getBookId).fromKey()); cache.addIndex(ValueExtractor.of(SalesId::getRegionCode).fromKey()); ",
            "title": "Using Indexes in Queries"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " The example above uses the indexes to improve query performance, but in this case as we are querying for the keys, we could have just used the indexes directly in the IncrementSalesProcessor instead of querying the backing map. If we know that our application has always created the required indexes before invoking the IncrementSalesProcessor we can simplify the code and not bother executing a query at all. We could change the getParentKeys method to use indexes directly: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >private SortedSet&lt;Binary&gt; getParentKeys(SalesId key, BackingMapContext backingMapContext) { BackingMapManagerContext managerContext = backingMapContext.getManagerContext(); Converter&lt;Binary, SalesId&gt; converter = managerContext.getKeyFromInternalConverter(); Map&lt;ValueExtractor, MapIndex&gt; indexMap = backingMapContext.getIndexMap(); // Get the two indexes from the index Map MapIndex indexBookId = indexMap.get(ValueExtractor.of(SalesId::getBookId).fromKey()); Map&lt;String, Set&lt;Binary&gt;&gt; indexBookIdContents = indexBookId.getIndexContents(); MapIndex indexRegion = indexMap.get(ValueExtractor.of(SalesId::getRegionCode).fromKey()); Map&lt;String, Set&lt;Binary&gt;&gt; indexRegionContents = indexRegion.getIndexContents(); // Obtain the set of Binary keys that have the required BookId Set&lt;Binary&gt; setBookId = indexBookIdContents.get(key.getBookId()); SortedSet&lt;Binary&gt; parents = new TreeSet&lt;&gt;(); SalesId parent = key; while (parent != null) { String region = parent.getParentRegionCode(); if (region == null) { // we're finished, the key has no parent region break; } // Obtain the set of Binary keys that have the parent region // and wrap them in a SubSet, so we do not mutate the real set Set&lt;Binary&gt; setRegion = new SubSet&lt;&gt;(indexBookIdContents.get(key.getBookId())); // remove any values from the set that are not in the BookId key set setRegion.retainAll(setBookId); // setRegion \"should\" now contain zero or one entry Binary binaryKey = setRegion.stream().findFirst().orElse(null); if (binaryKey == null) { // we're finished, there was no parent break; } // add the parent to the result set parents.add(binaryKey); // set the next parent parent = converter.convert(binaryKey); } return parents; } Warning When directly accessing the backing map or the index map inside an entry processor` or an aggregator in Coherence, you should never mutate these structures directly from application code. They should be treated as read-only resources. ",
            "title": "Using Indexes Directly"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " Using key affinity for hierarchical data can work, but may not always be advisable. If you have a lot of parent nodes and the hierarchies are small, i.e. a parent only has a small number of children, and they have a small number of children, that would be workable in Coherence. You would have a lot of small trees distributed over the cluster os storage enabled members. If there was only a small number of parents, then there would only be a small number of trees, and hence those would all live on only a small number of members of the cluster, the remaining members would hold no data. This would mean some JVMs would be using a lot more heap to hold data than others. If some trees had a lot more nodes that others, this would also mean the JVMs holding the larger trees would be using more heap than others in the cluster. So, the rule for storing hierarchical data with key affinity is lots of small trees is better. Entity Classes In this example the data model is a bookseller, with a lot of books. We have a cache holding sales for books by region, where the region forms a hierarchy, e.g. \"World\", \"Continent\", and \"Country\". We might have data like this: <markup >- \"The Great Gatsby\", \"World\", 26 - \"The Great Gatsby\", \"North America\", 22 - \"The Great Gatsby\", \"US\", 19 - \"The Great Gatsby\", \"Canada\", 3 - \"The Great Gatsby\", \"Europe\", 4 - \"The Great Gatsby\", \"France\", 1 - \"The Great Gatsby\", \"UK\", 3 The key class might look like this: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/SalesId.java\" >public class SalesId implements ExternalizableLite, PortableObject, KeyAssociation&lt;String&gt; { // ----- constructors --------------------------------------------------- /** * A default no-args constructor required for serialization. */ public SalesId() { } /** * Create a sales identifier. * * @param bookId the book identifier * @param regionCode the region identifier * @param parentRegionCode the parent region identifier, or {@code null} * if this is a top level region */ public SalesId(String bookId, String regionCode, String parentRegionCode) { this.bookId = bookId; this.regionCode = regionCode; this.parentRegionCode = parentRegionCode; } // ----- KeyAssociation methods ----------------------------------------- @Override public String getAssociatedKey() { return bookId; } // ----- accessors ------------------------------------------------------ /** * Return the book identifier. * * @return the book identifier */ public String getBookId() { return bookId; } /** * Return the region identifier. * * @return the region identifier */ public String getRegionCode() { return regionCode; } /** * Return the parent region identifier, or {@code null} * if this is a top level region. * * @return the parent region identifier, or {@code null} * if this is a top level region */ public String getParentRegionCode() { return parentRegionCode; } // ----- Object methods ------------------------------------------------- // Coherence key classes must properly implement hashCode() using // all the fields in the class @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } SalesId salesId = (SalesId) o; return Objects.equals(bookId, salesId.bookId) &amp;&amp; Objects.equals(regionCode, salesId.regionCode) &amp;&amp; Objects.equals(parentRegionCode, salesId.parentRegionCode); } // Coherence key classes must properly implement hashCode() using // all the fields in the class @Override public int hashCode() { return Objects.hash(bookId, regionCode, parentRegionCode); } // serialization methods omitted... // ----- data members --------------------------------------------------- /** * The identifier for the ook */ private String bookId; /** * The region code for the sales data. */ private String regionCode; /** * The parent region code, or {@code null} if this is a top level region. */ private String parentRegionCode; } The SalesId class uses the bookId as the associated key so that all sales for a book are co-located in a single partition. The SalesId has a reference to the parent region, for example the parent of the region \"France\", is \"Europe\" and the parent of \"Europe\" is \"World\". We can also create a simple BookSales class to hold the sales information for a book, we might hold sales for e-books, audiobooks and paper books. We can add methods on the BookSales class to get, set and increment the different sales numbers. The Increment Sales EntryProcessor Now we have some entity classes to hold the sales data we can create an EntryProcessor that will update the sales for a book. The operation of the EntryProcessor would be the following. The EntryProcessor has parameters for the additional sales in each category, paper book, e-book and audiobook for a region. The EntryProcessor is invoked against the SalesId to be updated The EntryProcessor updates the sales data for the region The EntryProcessor then finds the parent regions in the hierarchy and updates them The EntryProcessor class might look like the code below. The process() method is empty and will be completed in the next section. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >public class IncrementSalesProcessor extends AbstractEvolvable implements InvocableMap.EntryProcessor&lt;SalesId, BookSales, Void&gt;, ExternalizableLite, PortableObject { // ----- constructors --------------------------------------------------- /** * Default no-args constructor required for serialization. */ public IncrementSalesProcessor() { } /** * Create a {@link IncrementSalesProcessor}. * * @param eBook the e-book sales * @param audio the audiobook sales * @param paper the paper book sales */ public IncrementSalesProcessor(long eBook, long audio, long paper) { this.eBook = eBook; this.audio = audio; this.paper = paper; } // ----- EntryProcessor methods ----------------------------------------- @Override public Void process(InvocableMap.Entry&lt;CustomerId, Customer&gt; entry) { return null; } // ----- serialization methods ------------------------------------------ @Override public int getImplVersion() { return IMPLEMENTATION_VERSION; } @Override public void readExternal(DataInput in) throws IOException { eBook = in.readLong(); audio = in.readLong(); paper = in.readLong(); } @Override public void writeExternal(DataOutput out) throws IOException { out.writeLong(eBook); out.writeLong(audio); out.writeLong(paper); } @Override public void readExternal(PofReader in) throws IOException { eBook = in.readLong(0); audio = in.readLong(1); paper = in.readLong(2); } @Override public void writeExternal(PofWriter out) throws IOException { out.writeLong(0, eBook); out.writeLong(1, audio); out.writeLong(2, paper); } // ----- data members --------------------------------------------------- /** * The evolvable POF implementation version of this class. */ public static final int IMPLEMENTATION_VERSION = 1; /** * The number of e-books sold. */ private long eBook; /** * The number of audiobooks sold. */ private long audio; /** * The number of paper sold. */ private long paper; } In the IncrementSalesProcessor we can start to create the process method to update the sales data in the entry that the IncrementSalesProcessor will be invoked on, as shown below: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >@Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;SalesId, BookSales&gt; entry) { // Obtain a BinaryEntry from the entry being processes BinaryEntry&lt;SalesId, BookSales&gt; binaryEntry = entry.asBinaryEntry(); // update the entry sales data BookSales sales; if (entry.isPresent()) { // the parent entry is present sales = entry.getValue(); } else { // The parent entry is not present, so set a new BookSales value sales = new BookSales(); } sales.incrementEBookSales(eBook); sales.incrementAudioSales(audio); sales.incrementPaperSales(paper); // set the updated sale value back into the entry so that Coherence updates the cache entry.setValue(sales); // Obtain a sorted set of the Binary keys of the parents of the entry being processed // We do not need to return anything return null; } Partition Local Queries One of the functions of the IncrementSalesProcessor class that is missing from the process method above is to update the parent regions in the hierarchy. When the entry processor is invoked against an entry, we do not have the full key for the parent entry, we only know the book and region. This means we cannot just enlist the entry based on a key, as we do not know the key. The solution to this is to perform a query to locate the parent, as we know the parent will be in the same partition, as it has the same book id. The entry that is passed to the process method can be turned into a BinaryEntry and from this we can obtain the cache&#8217;s backing map. The backing map in Coherence is the actual Map instance that holds the data for the partition. This is typically serialized binary data. So even if the generics for the entry are real types (in this case the SalesId and BookSales classes) the backing map keys and values will be Binary instances. The two lines below show how to convert the entry passed to the process method into a BinaryEntry . <markup lang=\"java\" >BinaryEntry&lt;SalesId, BookSales&gt; binaryEntry = entry.asBinaryEntry(); BackingMapContext backingMapContext = binaryEntry.getBackingMapContext(); Now we have the BackingMapContext we can write some additional methods to search for the parents of the entry being processed. Querying a Binary Backing Map When inside an entry processor or aggregator, the backing map of a cache is of the type Map&lt;Binary, Binary&gt; , because the backing map holds the serialized cache data. The simplest way to query the backing map is to use normal Coherence Filter classes, which typically take a ValueExtractor to extract the field to be tested in the filter. We could write a ValueExtractor that can deserialize the Binary key or value and extract the required field, and that would work fine. But, we can be a bit smarter here, so that we can utilize any indexes that may exist on the cache. If we wrote a custom binary extractor, that in this example extracted the bookId from the key, and then created in index on that, we would need to create a second index for any normal cache queries that used the bookId . It would be far better to be able to just create a single index. In this example we create a special wrapper class named BinaryValueExtractor that will extract values from a serialized Binary using a wrapped ValueExtractor . When the BinaryValueExtractor is used in a Filter that may use an index, we want the index lookup to retrieve any index created by the wrapped extractor, and to do this the BinaryValueExtractor.getCanonicalName() method will return the same value as the delegate ValueExtractor . <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/BinaryValueExtractor.java\" >public class BinaryValueExtractor&lt;T, E&gt; implements ValueExtractor&lt;Binary, E&gt; { /** * Create a {@link BinaryValueExtractor}. * * @param delegate the extractor to delegate to * @param converter the {@link Converter} to convert the {@link Binary} value * to a value to pass to the delegate {@link ValueExtractor} */ public BinaryValueExtractor(ValueExtractor&lt;T, E&gt; delegate, Converter&lt;Binary, T&gt; converter) { m_delegate = delegate; m_converter = converter; } // ----- ValueExtractor ------------------------------------------------- @Override public E extract(Binary target) { T value = m_converter.convert(target); return m_delegate.extract(value); } @Override public int getTarget() { return m_delegate.getTarget(); } @Override public String getCanonicalName() { return m_delegate.getCanonicalName(); } // ----- helper methods ------------------------------------------------- /** * A factory method to create a {@link BinaryValueExtractor}. * * @param delegate the extractor to delegate to * @param converter the {@link Converter} to convert the {@link Binary} value * to a value to pass to the delegate {@link ValueExtractor} * * @return the {@link BinaryValueExtractor} that will extract from a {@link Binary} value * * @param &lt;T&gt; the underlying type to extract from after being deserialized * @param &lt;E&gt; the type of the extracted value */ public static &lt;T, E&gt; ValueExtractor&lt;Binary, E&gt; of(ValueExtractor&lt;T, E&gt; delegate, Converter&lt;Binary, T&gt; converter) { return new BinaryValueExtractor&lt;&gt;(delegate, converter); } // ----- data members --------------------------------------------------- /** * The delegate {@link ValueExtractor}. */ private final ValueExtractor&lt;T, E&gt; m_delegate; /** * The {@link Converter} to convert the {@link Binary} value * to a value to pass to the delegate {@link ValueExtractor} */ private final Converter&lt;Binary, T&gt; m_converter; } We can now use the BinaryValueExtractor in a Filter inside an entry processor or aggregator. For example the code below creates an EqualsFilter that can execute against the binary backing map. The filter will match any entry in the backing map that has a key ( SalesId ) where the getBookId() method returns \"Foo\": <markup lang=\"java\" >Filter&lt;?&gt; filter = Filters.equal(BinaryValueExtractor.of(SalesId::getBookId, converter).fromKey(), \"Foo\"); Outside an entry processor or aggregator, the same query using the normal cache API would be: <markup lang=\"java\" >Filter&lt;?&gt; filter = Filters.equal(ValueExtractor.of(SalesId::getBookId, converter).fromKey(), \"Foo\"); If we want to make the filter more efficient in both cases, we would create an index on the cache as normal using the normal non-binary extractor: <markup lang=\"java\" >NamedCache&lt;SalesId, BookSales&gt; cache = session.getCache(\"book-sales\"); cache.addIndex(ValueExtractor.of(SalesId::getBookId).fromKey()); Note The BinaryValueExtractor is intentionally not serializable as it is not meant ot be used in normal filter queries. Finding the Book Sales parent The method below shows how to use the BinaryValueExtractor in filters to query the cache backing map, in this case to obtain the single parent entry, but it may be used in other use-cases to query for multiple entries. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >private Map.Entry&lt;SalesId, BookSales&gt; getParent(SalesId id, BackingMapContext backingMapContext) { ObservableMap&lt;Binary, Binary&gt; backingMap = backingMapContext.getBackingMap(); Map&lt;ValueExtractor, MapIndex&gt; indexMap = backingMapContext.getIndexMap(); String bookId = id.getBookId(); String region = id.getParentRegionCode(); Filter&lt;?&gt; filter = Filters.equal(BinaryValueExtractor.of(SalesId::getBookId, converter).fromKey(), bookId) .and(Filters.equal(BinaryValueExtractor.of(SalesId::getRegionCode, converter).fromKey(), region)); Set&lt;Map.Entry&lt;Binary, Binary&gt;&gt; setEntries = InvocableMapHelper.query(backingMap, indexMap, filter, true, false, null); // there should only ever be zero or one matching entry return setEntries.stream() .findFirst() .orElse(null); } The code above works as follows: * Obtain the backing map from the backing map context * Obtain the map of indexes present on the cache * Obtain the bookId and region for the parent entry we want to find * Create a Coherence Filter that will query the baking map keys for a matching entry. * Execute the query on the backing map using Coherence&#8217;s InvocableMapHelper utility class * There should only be a single matching entry (or maybe no match) so extract the first entry from the query results and return it Note Accessing the backing map using backingMapContext.getBackingMap() is marked as deprecated. The main reason for the deprecation is to discourage direct use of the backing map in application code. Direct manipulation of the data in the backing map by application code is dangerous and could result in corruption of the cache. In this use-case there is currently no alternative API to perform a partition local query, but as we are only reading data from the map it is safe. The contents of the map may be changed by other threads that are executing Coherence requests on the same partition while the query is in progress. This means that any result returned by a query should be considered transitive, just like any query results from an active cache. Now we have a method that can obtain the parent entry for a SalesId key we can write another utility method that will obtain all the parent keys in the hierarchy for a given SalesId . As already mentioned above, we will sort this set of keys so that when the entries are enlisted we avoid deadlocks. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >private SortedSet&lt;Binary&gt; getParentKeys(SalesId key, BackingMapContext backingMapContext) { TreeSet&lt;Binary&gt; parents = new TreeSet&lt;&gt;(); Converter&lt;Binary, SalesId&gt; converter = backingMapContext.getManagerContext().getKeyFromInternalConverter(); Map.Entry&lt;Binary, Binary&gt; parent = getParent(key, backingMapContext); while (parent != null) { Binary binaryKey = parent.getKey(); parents.add(binaryKey); key = converter.convert(binaryKey); parent = getParent(key, backingMapContext); } return parents; } With the additional methods above, we can now complete the process method in the IncrementSalesProcessor class as shown below. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >public Void process(InvocableMap.Entry&lt;SalesId, BookSales&gt; entry) { // update the entry sales data BookSales sales; if (entry.isPresent()) { // the entry is present sales = entry.getValue(); } else { // The parent entry is not present, so create a new BookSales value sales = new BookSales(); } sales.incrementEBookSales(eBook); sales.incrementAudioSales(audio); sales.incrementPaperSales(paper); // set the updated sale value back into the entry so that Coherence updates the cache entry.setValue(sales); // Obtain a BinaryEntry from the entry being processes BinaryEntry&lt;SalesId, BookSales&gt; binaryEntry = entry.asBinaryEntry(); // Obtain the BackingMapContext for the entry BackingMapContext backingMapContext = binaryEntry.getBackingMapContext(); // Obtain a sorted set of the Binary keys of the parents of the entry being processed SortedSet&lt;Binary&gt; parentKeys = getParentKeys(entry.getKey(), backingMapContext); // Iterate over the parent keys, enlisting and updating each parent entry for (Binary binaryKey : parentKeys) { InvocableMap.Entry&lt;SalesId, BookSales&gt; parentEntry = backingMapContext.getBackingMapEntry(binaryKey); if (parentEntry.isPresent()) { // the parent entry is present sales = parentEntry.getValue(); } else { // The parent entry is not present, so create a new BookSales value sales = new BookSales(); } // update the parent sales data sales.incrementEBookSales(eBook); sales.incrementAudioSales(audio); sales.incrementPaperSales(paper); // set the updated sale value back into the entry so that Coherence updates the cache parentEntry.setValue(sales); } return null; } Using Indexes in Queries In the code above that queries the backing map, the actual query call looks like this: <markup lang=\"java\" >Set&lt;Map.Entry&lt;SalesId, BookSales&gt;&gt; setEntries = InvocableMapHelper.query(map, indexMap, filter, true, false, null); You can see that the second parameter is a Map of indexes, which is obtained from the BackingMapContext by calling backingMapContext.getIndexMap() . This Map is the map of indexes that have been added to the cache by calls to one of the NamedCache.addIndex() methods. Using the cache indexes in this way can make a big difference to the speed and efficiently of the query execution. In the example above the Filter for the query is created like this: <markup lang=\"java\" >Filter&lt;?&gt; filter = Filters.equal(ValueExtractor.of(SalesId::getBookId).fromKey(), bookId) .and(Filters.equal(ValueExtractor.of(SalesId::getRegionCode).fromKey(), region)); The Filter is an \"and\" filter which uses two ValueExtractor instances: <markup lang=\"java\" >ValueExtractor.of(SalesId::getBookId).fromKey() ValueExtractor.of(SalesId::getRegionCode).fromKey() If we create indexes on the cache using the same extractors, then the query will be much faster: <markup lang=\"java\" >NamedCache&lt;SalesId, BookSales&gt; cache = session.getCache(\"book-sales\"); cache.addIndex(ValueExtractor.of(SalesId::getBookId).fromKey()); cache.addIndex(ValueExtractor.of(SalesId::getRegionCode).fromKey()); Using Indexes Directly The example above uses the indexes to improve query performance, but in this case as we are querying for the keys, we could have just used the indexes directly in the IncrementSalesProcessor instead of querying the backing map. If we know that our application has always created the required indexes before invoking the IncrementSalesProcessor we can simplify the code and not bother executing a query at all. We could change the getParentKeys method to use indexes directly: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >private SortedSet&lt;Binary&gt; getParentKeys(SalesId key, BackingMapContext backingMapContext) { BackingMapManagerContext managerContext = backingMapContext.getManagerContext(); Converter&lt;Binary, SalesId&gt; converter = managerContext.getKeyFromInternalConverter(); Map&lt;ValueExtractor, MapIndex&gt; indexMap = backingMapContext.getIndexMap(); // Get the two indexes from the index Map MapIndex indexBookId = indexMap.get(ValueExtractor.of(SalesId::getBookId).fromKey()); Map&lt;String, Set&lt;Binary&gt;&gt; indexBookIdContents = indexBookId.getIndexContents(); MapIndex indexRegion = indexMap.get(ValueExtractor.of(SalesId::getRegionCode).fromKey()); Map&lt;String, Set&lt;Binary&gt;&gt; indexRegionContents = indexRegion.getIndexContents(); // Obtain the set of Binary keys that have the required BookId Set&lt;Binary&gt; setBookId = indexBookIdContents.get(key.getBookId()); SortedSet&lt;Binary&gt; parents = new TreeSet&lt;&gt;(); SalesId parent = key; while (parent != null) { String region = parent.getParentRegionCode(); if (region == null) { // we're finished, the key has no parent region break; } // Obtain the set of Binary keys that have the parent region // and wrap them in a SubSet, so we do not mutate the real set Set&lt;Binary&gt; setRegion = new SubSet&lt;&gt;(indexBookIdContents.get(key.getBookId())); // remove any values from the set that are not in the BookId key set setRegion.retainAll(setBookId); // setRegion \"should\" now contain zero or one entry Binary binaryKey = setRegion.stream().findFirst().orElse(null); if (binaryKey == null) { // we're finished, there was no parent break; } // add the parent to the result set parents.add(binaryKey); // set the next parent parent = converter.convert(binaryKey); } return parents; } Warning When directly accessing the backing map or the index map inside an entry processor` or an aggregator in Coherence, you should never mutate these structures directly from application code. They should be treated as read-only resources. ",
            "title": "Hierarchical Data Example"
        },
        {
            "location": "/examples/guides/906-partition-level-transactions/README",
            "text": " This guide covers how to update multiple cache entries using partition level transactions. What is a Partition Level Transaction In a Coherence distributed cache, data is managed in partitions. A cache entry is assigned to a partition and partition ownership is distributed over the storage enabled members of a cluster. When updating a cache entry using an EntryProcessor , Coherence performs this update atomically in what is known as a partition level transaction. In the EntryProcessor code it is possible to \"enlist\" other entries owned by the same partition into the transaction so that the mutations to those entries are committed to the partition&#8217;s storage as a single atomic update. The other entries enlisted into the transaction can be from the same cache, or from other caches that are managed by the same cache service. This is a very useful feature for a number of use cases that need to ensure atomic updates of related entries. A Simple Use Case Imagine we were building a banking system where we had a Customer cache and an Accounts cache. A customer would have one entry in the Customers cache, but may have multiple accounts in the Accounts cache. In our application we want to write some functionality to transfer money between accounts, i.e. take from one account and add to another. We would like to do this atomically so that we do not take more money from an account than the current balance and also ensure the customer&#8217;s account balances are consistent. Say for customer \"Foo\", we want to transfer $100 from account \"A\" to account \"B\" Without a partition level transaction, the flow might look like this: <markup lang=\"java\" >accounts.invoke(new AccountKey(\"Foo\", \"A\"), new DebitProcessor(100)); accounts.invoke(new AccountKey(\"Foo\", \"B\"), new CreditProcessor(100)); The problem here is that the two account entries are in a distributed cache and may be on different JVMs in a cluster, probably on different physical servers. There will be a time window between the first invoke and the second where the total balance of the customer&#8217;s accounts is $100 less than it really is. There is also an issue where the second invoke fails for some reason, and we then need to handle this and put the $100 back into account \"A\" or keep re-trying. A better solution would be a single invoke, where we execute a single EntryProcessor that updates both accounts in a single transaction. The total balance of the accounts will remain consistent, and if the invoke call fails Coherence will automatically roll back all the affected entries. <markup lang=\"java\" >customers.invoke(new CustomerKey(\"Foo\"), new TransferProcessor(\"A\", \"B\", 100)); The rest of this guide explain how we accomplish this and some other techniques for handling related data. Key Affinity An important point about partition level transactions is that all the entries enlisted in the transaction must be owned by the same partition. In normal operation a cache entry is assigned to a partition based on a hash of the serialized key of the entry. This helps ensure an even distribution of entries over the partitions. When using partition level transactions we usually need to influence which partition a key is assigned to so that we ensure related entries are stored in the same partition, this is known as \"key affinity\", as it is the cache keys that control the owning partition. In most use cases we do not care exactly which partition a set of related keys belong to, rather we just need to ensure they all belong to the same partition. The Working with Partitions section of the official documentation covers the different techniques that can be used to control the partition an entry is assigned to. Avoiding Deadlocks in EntryProcessors It is very important when enlisting other entries in an EntryProcessor that this is done in a way that avoids deadlocks. There are two ways to avoid deadlocks covered below. Invoke Against a Common Key Imagine a case where a customer \"Foo\" has two accounts, \"A\" and \"B\" and we initiate two transfers, one from \"A\" to \"B\" and the other from \"B\" to \"A\" using two separate invocations of TransferProcessor . Say we invoked TransferProcessor against the source account, we could see the following: Invoke TransferProcessor against account \"A\", the entry for account \"A\" is now locked in the accounts cache. Invoke TransferProcessor against account \"B\", the entry for account \"B\" is now locked in the accounts cache. The TransferProcessor that has locked account \"A\" now tries to enlist account \"B\", but account \"B\" is locked, so the TransferProcessor will now block waiting for account \"B\" to be unlocked. The TransferProcessor that has locked account \"B\" now tries to enlist account \"A\", but account \"A\" is locked, so the TransferProcessor will now block waiting for account \"A\" to be unlocked. We now have a deadlock, both TransferProcessor invocations are blocked waiting for each other to complete. When we invoke the TransferProcessor against the entry fo \"Foo\" in the customers cache the operation changes to look like this: Invoke TransferProcessor for \"A\" &#8594; \"B\" against customer \"Foo\", the entry for customer \"Foo\" is now locked in the customers cache. Invoke TransferProcessor for \"B\" &#8594; \"A\" against customer \"Foo\", the entry for customer \"Foo\" is already locked so this processor will wait. The TransferProcessor for \"A\" &#8594; \"B\" enlists accounts \"A\" and \"B\" and completes the transfer, unlocking customer \"Foo\" The TransferProcessor for \"B\" &#8594; \"A\" is now unblocked and can execute. By invoking the TransferProcessor against the customer entry we avoid the risk of deadlocks occurring. Sort the Keys to be Enlisted A second method to avoid deadlocks where multiple entries will be enlisted is to sort the keys that will be enlisted. Sometimes it is not possible, or it is inefficient, to invoke every EntryProcessor against a common key. In the example above, if we invoked every cache operation the customer entry, this could impact performance for some use cases because all the operations for a customer would queue and execute one at a time. Sometimes, for performance we&#8217;d like to execute operations in parallel as much as possible. An example of this might be where we have a cache that stores data that forms a nested tree type hierarchy, i.e a parent node that has child nodes which also have sub-child nodes. Say we have a use case where if we update a sub-child node, we have to apply a corresponding update to its parents. We could do what we did above and only ever run the EntryProcessor against the top level parent, then enlist entries going down the hierarchy to the sub-child we want to update. If our application applies a lot of updates to sub-child nodes, this would be a big performance bottleneck, as updates could only be applied sequentially. Instead, in this use case the EntryProcessor would execute against the sub-child, then search for and enlist its parents. How we might search for parents will be covered in an example below. For example, we have a parent node \"A\", that has a child node \"A-1\", which has two sub-child nodes \"A-1.1\" and \"A-1.2\". Without sorting the enlisted entries this is what could happen. Invoke the EntryProcessor \"One\" against node \"A-1.1\", so entry \"A-1.1\" is locked. Invoke the EntryProcessor \"Two\" against node \"A-1.2\", so entry \"A-1.2\" is locked. EntryProcessor \"One\" searches for its parent nodes, the resulting keys come back as `[\"A\", \"A-1\"], so the first of those \"A\" is serialized and used to enlist entry \"A\", and locked. Before EntryProcessor \"One\" can lock the second result, \"A-1\", EntryProcessor \"Two\" has searched for its parents and has a result of `[\"A-1\", \"A\"] (the opposite order to \"One\"), so it enlists and locks \"A-1\". EntryProcessor \"One\" now tries to enlist \"A-1\", but is blocked because this is locked by \"Two\". EntryProcessor \"Two\" now tries to enlist \"A\", but is blocked because this is locked by \"One\". We now have a deadlock The solution to the deadlock above is for the EntryProcessor to sort the keys of the entries that it will enlist before enlisting them. The simplest way to do this is to sort the serialized Binary keys that will be used to enlist the entries, because com.tangosol.util.Binary implements Comparable . The flow then becomes: Invoke the EntryProcessor \"One\" against node \"A-1.1\", so entry \"A-1.1\" is locked. Invoke the EntryProcessor \"Two\" against node \"A-1.2\", so entry \"A-1.2\" is locked. EntryProcessor \"One\" searches for its parent nodes, the result comes back as `[\"A\", \"A-1\"] EntryProcessor \"One\" serializes the keys to Binary keys sorts them, e.g. in a sorted list or TreeSet . EntryProcessor \"One\" now enlists the first sorted key, say it is \"A\". Before EntryProcessor \"One\" can lock the second result, \"A-1\", EntryProcessor \"Two\" has searched for its parents and has a result of `[\"A-1\", \"A\"] (the opposite order to \"One\") EntryProcessor \"Two\" serializes the keys to Binary keys sorts them. EntryProcessor \"Two\" now enlists the first sorted key, which will also now be \"A\", but \"A\" is already locked, so EntryProcessor \"Two\" is blocked. EntryProcessor \"One\" can now proceed to enlist and lock \"A-1\", perform its updates of \"A\" and \"A-1\", then complete, unlocking all the entries EntryProcessor \"Two\" can now enist and lock \"A-1\" and complete its processing. Bank Account Example For the first example in this guide, we will implement the simple bank account use case described above to transfer money between two accounts for the same customer. Entity Classes We will need a few entity classes, a simple Customer class and an Account class. We will also create key classes for both of these. We could use a simple class from the JVM for the customer key such as a String , a number or a UUID, but in this case we&#8217;ll create a custom class. We need to use key affinity to ensure that the accounts for a customer a located in the same partition as the customer. To do this we will make the AccountId class implement the Coherence com.tangosol.net.cache.KeyAssociation interface. The getAssociatedKey method just returns the customerId field. <markup lang=\"java\" >@Override public CustomerId getAssociatedKey() { return customerId; } The full AccountId class is shown below: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/bank/AccountId.java\" >public class AccountId implements ExternalizableLite, PortableObject, KeyAssociation&lt;CustomerId&gt; { // ----- constructors --------------------------------------------------- /** * A default no-args constructor required for serialization. */ public AccountId() { } /** * Create a {@link AccountId}. * * @param id the id of the customer */ public AccountId(CustomerId customerId, String id) { this.customerId = customerId; this.id = id; } // ----- KeyAssociation methods ----------------------------------------- @Override public CustomerId getAssociatedKey() { return customerId; } // ----- accessors ------------------------------------------------------ /** * Return the id. * * @return the customer id */ public CustomerId getCustomerId() { return customerId; } /** * Return the account id. * * @return the customer id */ public String getId() { return id; } // ----- Object methods ------------------------------------------------- // Coherence key classes must properly implement equals() using // all the fields in the class @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } AccountId accountId = (AccountId) o; return Objects.equals(customerId, accountId.customerId) &amp;&amp; Objects.equals(id, accountId.id); } // Coherence key classes must properly implement hashCode() using // all the fields in the class @Override public int hashCode() { return Objects.hash(customerId, id); } // ----- ExternalizableLite methods ------------------------------------- @Override public void readExternal(DataInput in) throws IOException { customerId = ExternalizableHelper.readObject(in); id = ExternalizableHelper.readSafeUTF(in); } @Override public void writeExternal(DataOutput out) throws IOException { ExternalizableHelper.writeObject(out, customerId); ExternalizableHelper.writeSafeUTF(out, id); } // ----- PortableObject methods ----------------------------------------- @Override public void readExternal(PofReader in) throws IOException { customerId = in.readObject(0); id = in.readString(1); } @Override public void writeExternal(PofWriter out) throws IOException { out.writeObject(0, customerId); out.writeString(1, id); } // ----- data members --------------------------------------------------- /** * The id of the customer. */ private CustomerId customerId; /** * The id of the account. */ private String id; } The Transfer EntryProcessor Now we know customers and accounts are co-located we can write the TransferProcessor . <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/bank/TransferProcessor.java\" >public class TransferProcessor extends AbstractEvolvable implements InvocableMap.EntryProcessor&lt;CustomerId, Customer, Void&gt;, ExternalizableLite, PortableObject { } The generic parameters for the TransferProcessor are &lt;CustomerId, Customer, Void&gt; because the processor will be invoked against the customers cache, which has a key of CustomerId and a value of Customer . In this case we do not return a result from the processor, so its return type is Void . The process method would look like this (with comments to explain the code): <markup lang=\"java\" > @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;CustomerId, Customer&gt; entry) { // Convert the entry to a BinaryEntry BinaryEntry&lt;CustomerId, Customer&gt; binaryEntry = entry.asBinaryEntry(); // Obtain the backing map manager context BackingMapManagerContext context = binaryEntry.getContext(); // Obtain the converter to use to convert the account identifiers // into Coherence internal serialized binary format // It is important to use the correct key converter for this conversion Converter&lt;AccountId, Binary&gt; keyConverter = context.getKeyToInternalConverter(); // Obtain the backing map context for the accounts cache BackingMapContext accountsContext = context.getBackingMapContext(\"accounts\"); // Convert the source account id to a binary key and obtain the cache entry for the source account Binary sourceKey = keyConverter.convert(sourceAccount); InvocableMap.Entry&lt;AccountId, Account&gt; sourceEntry = accountsContext.getBackingMapEntry(sourceKey); // Convert the destination account id to a binary key and obtain the cache entry for the destination account Binary destinationKey = keyConverter.convert(destinationAccount); InvocableMap.Entry&lt;AccountId, Account&gt; destinationEntry = accountsContext.getBackingMapEntry(destinationKey); // adjust the values for the two accounts Account sourceAccount = sourceEntry.getValue(); sourceAccount.adjustBalance(amount.negate()); // set the updated source account back into the entry so that the cache is updated sourceEntry.setValue(sourceAccount); Account destinationAccount = destinationEntry.getValue(); destinationAccount.adjustBalance(amount); // set the updated destination account back into the entry so that the cache is updated destinationEntry.setValue(destinationAccount); return null; } Note You must remember to call setValue() on the entries that have been updated passing in the updated values so that Coherence knows to update the underlying cache entry. Just mutating the value returned from entry.getValue() will not cause a cache update. In the call to BackingMapContext accountsContext = context.getBackingMapContext(\"accounts\"); better coding practice would be to have a common set of a static constants for the cache names in our application code instead of using hard coded string values. Hierarchical Data Example Using key affinity for hierarchical data can work, but may not always be advisable. If you have a lot of parent nodes and the hierarchies are small, i.e. a parent only has a small number of children, and they have a small number of children, that would be workable in Coherence. You would have a lot of small trees distributed over the cluster os storage enabled members. If there was only a small number of parents, then there would only be a small number of trees, and hence those would all live on only a small number of members of the cluster, the remaining members would hold no data. This would mean some JVMs would be using a lot more heap to hold data than others. If some trees had a lot more nodes that others, this would also mean the JVMs holding the larger trees would be using more heap than others in the cluster. So, the rule for storing hierarchical data with key affinity is lots of small trees is better. Entity Classes In this example the data model is a bookseller, with a lot of books. We have a cache holding sales for books by region, where the region forms a hierarchy, e.g. \"World\", \"Continent\", and \"Country\". We might have data like this: <markup >- \"The Great Gatsby\", \"World\", 26 - \"The Great Gatsby\", \"North America\", 22 - \"The Great Gatsby\", \"US\", 19 - \"The Great Gatsby\", \"Canada\", 3 - \"The Great Gatsby\", \"Europe\", 4 - \"The Great Gatsby\", \"France\", 1 - \"The Great Gatsby\", \"UK\", 3 The key class might look like this: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/SalesId.java\" >public class SalesId implements ExternalizableLite, PortableObject, KeyAssociation&lt;String&gt; { // ----- constructors --------------------------------------------------- /** * A default no-args constructor required for serialization. */ public SalesId() { } /** * Create a sales identifier. * * @param bookId the book identifier * @param regionCode the region identifier * @param parentRegionCode the parent region identifier, or {@code null} * if this is a top level region */ public SalesId(String bookId, String regionCode, String parentRegionCode) { this.bookId = bookId; this.regionCode = regionCode; this.parentRegionCode = parentRegionCode; } // ----- KeyAssociation methods ----------------------------------------- @Override public String getAssociatedKey() { return bookId; } // ----- accessors ------------------------------------------------------ /** * Return the book identifier. * * @return the book identifier */ public String getBookId() { return bookId; } /** * Return the region identifier. * * @return the region identifier */ public String getRegionCode() { return regionCode; } /** * Return the parent region identifier, or {@code null} * if this is a top level region. * * @return the parent region identifier, or {@code null} * if this is a top level region */ public String getParentRegionCode() { return parentRegionCode; } // ----- Object methods ------------------------------------------------- // Coherence key classes must properly implement hashCode() using // all the fields in the class @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } SalesId salesId = (SalesId) o; return Objects.equals(bookId, salesId.bookId) &amp;&amp; Objects.equals(regionCode, salesId.regionCode) &amp;&amp; Objects.equals(parentRegionCode, salesId.parentRegionCode); } // Coherence key classes must properly implement hashCode() using // all the fields in the class @Override public int hashCode() { return Objects.hash(bookId, regionCode, parentRegionCode); } // serialization methods omitted... // ----- data members --------------------------------------------------- /** * The identifier for the ook */ private String bookId; /** * The region code for the sales data. */ private String regionCode; /** * The parent region code, or {@code null} if this is a top level region. */ private String parentRegionCode; } The SalesId class uses the bookId as the associated key so that all sales for a book are co-located in a single partition. The SalesId has a reference to the parent region, for example the parent of the region \"France\", is \"Europe\" and the parent of \"Europe\" is \"World\". We can also create a simple BookSales class to hold the sales information for a book, we might hold sales for e-books, audiobooks and paper books. We can add methods on the BookSales class to get, set and increment the different sales numbers. The Increment Sales EntryProcessor Now we have some entity classes to hold the sales data we can create an EntryProcessor that will update the sales for a book. The operation of the EntryProcessor would be the following. The EntryProcessor has parameters for the additional sales in each category, paper book, e-book and audiobook for a region. The EntryProcessor is invoked against the SalesId to be updated The EntryProcessor updates the sales data for the region The EntryProcessor then finds the parent regions in the hierarchy and updates them The EntryProcessor class might look like the code below. The process() method is empty and will be completed in the next section. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >public class IncrementSalesProcessor extends AbstractEvolvable implements InvocableMap.EntryProcessor&lt;SalesId, BookSales, Void&gt;, ExternalizableLite, PortableObject { // ----- constructors --------------------------------------------------- /** * Default no-args constructor required for serialization. */ public IncrementSalesProcessor() { } /** * Create a {@link IncrementSalesProcessor}. * * @param eBook the e-book sales * @param audio the audiobook sales * @param paper the paper book sales */ public IncrementSalesProcessor(long eBook, long audio, long paper) { this.eBook = eBook; this.audio = audio; this.paper = paper; } // ----- EntryProcessor methods ----------------------------------------- @Override public Void process(InvocableMap.Entry&lt;CustomerId, Customer&gt; entry) { return null; } // ----- serialization methods ------------------------------------------ @Override public int getImplVersion() { return IMPLEMENTATION_VERSION; } @Override public void readExternal(DataInput in) throws IOException { eBook = in.readLong(); audio = in.readLong(); paper = in.readLong(); } @Override public void writeExternal(DataOutput out) throws IOException { out.writeLong(eBook); out.writeLong(audio); out.writeLong(paper); } @Override public void readExternal(PofReader in) throws IOException { eBook = in.readLong(0); audio = in.readLong(1); paper = in.readLong(2); } @Override public void writeExternal(PofWriter out) throws IOException { out.writeLong(0, eBook); out.writeLong(1, audio); out.writeLong(2, paper); } // ----- data members --------------------------------------------------- /** * The evolvable POF implementation version of this class. */ public static final int IMPLEMENTATION_VERSION = 1; /** * The number of e-books sold. */ private long eBook; /** * The number of audiobooks sold. */ private long audio; /** * The number of paper sold. */ private long paper; } In the IncrementSalesProcessor we can start to create the process method to update the sales data in the entry that the IncrementSalesProcessor will be invoked on, as shown below: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >@Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;SalesId, BookSales&gt; entry) { // Obtain a BinaryEntry from the entry being processes BinaryEntry&lt;SalesId, BookSales&gt; binaryEntry = entry.asBinaryEntry(); // update the entry sales data BookSales sales; if (entry.isPresent()) { // the parent entry is present sales = entry.getValue(); } else { // The parent entry is not present, so set a new BookSales value sales = new BookSales(); } sales.incrementEBookSales(eBook); sales.incrementAudioSales(audio); sales.incrementPaperSales(paper); // set the updated sale value back into the entry so that Coherence updates the cache entry.setValue(sales); // Obtain a sorted set of the Binary keys of the parents of the entry being processed // We do not need to return anything return null; } Partition Local Queries One of the functions of the IncrementSalesProcessor class that is missing from the process method above is to update the parent regions in the hierarchy. When the entry processor is invoked against an entry, we do not have the full key for the parent entry, we only know the book and region. This means we cannot just enlist the entry based on a key, as we do not know the key. The solution to this is to perform a query to locate the parent, as we know the parent will be in the same partition, as it has the same book id. The entry that is passed to the process method can be turned into a BinaryEntry and from this we can obtain the cache&#8217;s backing map. The backing map in Coherence is the actual Map instance that holds the data for the partition. This is typically serialized binary data. So even if the generics for the entry are real types (in this case the SalesId and BookSales classes) the backing map keys and values will be Binary instances. The two lines below show how to convert the entry passed to the process method into a BinaryEntry . <markup lang=\"java\" >BinaryEntry&lt;SalesId, BookSales&gt; binaryEntry = entry.asBinaryEntry(); BackingMapContext backingMapContext = binaryEntry.getBackingMapContext(); Now we have the BackingMapContext we can write some additional methods to search for the parents of the entry being processed. Querying a Binary Backing Map When inside an entry processor or aggregator, the backing map of a cache is of the type Map&lt;Binary, Binary&gt; , because the backing map holds the serialized cache data. The simplest way to query the backing map is to use normal Coherence Filter classes, which typically take a ValueExtractor to extract the field to be tested in the filter. We could write a ValueExtractor that can deserialize the Binary key or value and extract the required field, and that would work fine. But, we can be a bit smarter here, so that we can utilize any indexes that may exist on the cache. If we wrote a custom binary extractor, that in this example extracted the bookId from the key, and then created in index on that, we would need to create a second index for any normal cache queries that used the bookId . It would be far better to be able to just create a single index. In this example we create a special wrapper class named BinaryValueExtractor that will extract values from a serialized Binary using a wrapped ValueExtractor . When the BinaryValueExtractor is used in a Filter that may use an index, we want the index lookup to retrieve any index created by the wrapped extractor, and to do this the BinaryValueExtractor.getCanonicalName() method will return the same value as the delegate ValueExtractor . <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/BinaryValueExtractor.java\" >public class BinaryValueExtractor&lt;T, E&gt; implements ValueExtractor&lt;Binary, E&gt; { /** * Create a {@link BinaryValueExtractor}. * * @param delegate the extractor to delegate to * @param converter the {@link Converter} to convert the {@link Binary} value * to a value to pass to the delegate {@link ValueExtractor} */ public BinaryValueExtractor(ValueExtractor&lt;T, E&gt; delegate, Converter&lt;Binary, T&gt; converter) { m_delegate = delegate; m_converter = converter; } // ----- ValueExtractor ------------------------------------------------- @Override public E extract(Binary target) { T value = m_converter.convert(target); return m_delegate.extract(value); } @Override public int getTarget() { return m_delegate.getTarget(); } @Override public String getCanonicalName() { return m_delegate.getCanonicalName(); } // ----- helper methods ------------------------------------------------- /** * A factory method to create a {@link BinaryValueExtractor}. * * @param delegate the extractor to delegate to * @param converter the {@link Converter} to convert the {@link Binary} value * to a value to pass to the delegate {@link ValueExtractor} * * @return the {@link BinaryValueExtractor} that will extract from a {@link Binary} value * * @param &lt;T&gt; the underlying type to extract from after being deserialized * @param &lt;E&gt; the type of the extracted value */ public static &lt;T, E&gt; ValueExtractor&lt;Binary, E&gt; of(ValueExtractor&lt;T, E&gt; delegate, Converter&lt;Binary, T&gt; converter) { return new BinaryValueExtractor&lt;&gt;(delegate, converter); } // ----- data members --------------------------------------------------- /** * The delegate {@link ValueExtractor}. */ private final ValueExtractor&lt;T, E&gt; m_delegate; /** * The {@link Converter} to convert the {@link Binary} value * to a value to pass to the delegate {@link ValueExtractor} */ private final Converter&lt;Binary, T&gt; m_converter; } We can now use the BinaryValueExtractor in a Filter inside an entry processor or aggregator. For example the code below creates an EqualsFilter that can execute against the binary backing map. The filter will match any entry in the backing map that has a key ( SalesId ) where the getBookId() method returns \"Foo\": <markup lang=\"java\" >Filter&lt;?&gt; filter = Filters.equal(BinaryValueExtractor.of(SalesId::getBookId, converter).fromKey(), \"Foo\"); Outside an entry processor or aggregator, the same query using the normal cache API would be: <markup lang=\"java\" >Filter&lt;?&gt; filter = Filters.equal(ValueExtractor.of(SalesId::getBookId, converter).fromKey(), \"Foo\"); If we want to make the filter more efficient in both cases, we would create an index on the cache as normal using the normal non-binary extractor: <markup lang=\"java\" >NamedCache&lt;SalesId, BookSales&gt; cache = session.getCache(\"book-sales\"); cache.addIndex(ValueExtractor.of(SalesId::getBookId).fromKey()); Note The BinaryValueExtractor is intentionally not serializable as it is not meant ot be used in normal filter queries. Finding the Book Sales parent The method below shows how to use the BinaryValueExtractor in filters to query the cache backing map, in this case to obtain the single parent entry, but it may be used in other use-cases to query for multiple entries. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >private Map.Entry&lt;SalesId, BookSales&gt; getParent(SalesId id, BackingMapContext backingMapContext) { ObservableMap&lt;Binary, Binary&gt; backingMap = backingMapContext.getBackingMap(); Map&lt;ValueExtractor, MapIndex&gt; indexMap = backingMapContext.getIndexMap(); String bookId = id.getBookId(); String region = id.getParentRegionCode(); Filter&lt;?&gt; filter = Filters.equal(BinaryValueExtractor.of(SalesId::getBookId, converter).fromKey(), bookId) .and(Filters.equal(BinaryValueExtractor.of(SalesId::getRegionCode, converter).fromKey(), region)); Set&lt;Map.Entry&lt;Binary, Binary&gt;&gt; setEntries = InvocableMapHelper.query(backingMap, indexMap, filter, true, false, null); // there should only ever be zero or one matching entry return setEntries.stream() .findFirst() .orElse(null); } The code above works as follows: * Obtain the backing map from the backing map context * Obtain the map of indexes present on the cache * Obtain the bookId and region for the parent entry we want to find * Create a Coherence Filter that will query the baking map keys for a matching entry. * Execute the query on the backing map using Coherence&#8217;s InvocableMapHelper utility class * There should only be a single matching entry (or maybe no match) so extract the first entry from the query results and return it Note Accessing the backing map using backingMapContext.getBackingMap() is marked as deprecated. The main reason for the deprecation is to discourage direct use of the backing map in application code. Direct manipulation of the data in the backing map by application code is dangerous and could result in corruption of the cache. In this use-case there is currently no alternative API to perform a partition local query, but as we are only reading data from the map it is safe. The contents of the map may be changed by other threads that are executing Coherence requests on the same partition while the query is in progress. This means that any result returned by a query should be considered transitive, just like any query results from an active cache. Now we have a method that can obtain the parent entry for a SalesId key we can write another utility method that will obtain all the parent keys in the hierarchy for a given SalesId . As already mentioned above, we will sort this set of keys so that when the entries are enlisted we avoid deadlocks. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >private SortedSet&lt;Binary&gt; getParentKeys(SalesId key, BackingMapContext backingMapContext) { TreeSet&lt;Binary&gt; parents = new TreeSet&lt;&gt;(); Converter&lt;Binary, SalesId&gt; converter = backingMapContext.getManagerContext().getKeyFromInternalConverter(); Map.Entry&lt;Binary, Binary&gt; parent = getParent(key, backingMapContext); while (parent != null) { Binary binaryKey = parent.getKey(); parents.add(binaryKey); key = converter.convert(binaryKey); parent = getParent(key, backingMapContext); } return parents; } With the additional methods above, we can now complete the process method in the IncrementSalesProcessor class as shown below. <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >public Void process(InvocableMap.Entry&lt;SalesId, BookSales&gt; entry) { // update the entry sales data BookSales sales; if (entry.isPresent()) { // the entry is present sales = entry.getValue(); } else { // The parent entry is not present, so create a new BookSales value sales = new BookSales(); } sales.incrementEBookSales(eBook); sales.incrementAudioSales(audio); sales.incrementPaperSales(paper); // set the updated sale value back into the entry so that Coherence updates the cache entry.setValue(sales); // Obtain a BinaryEntry from the entry being processes BinaryEntry&lt;SalesId, BookSales&gt; binaryEntry = entry.asBinaryEntry(); // Obtain the BackingMapContext for the entry BackingMapContext backingMapContext = binaryEntry.getBackingMapContext(); // Obtain a sorted set of the Binary keys of the parents of the entry being processed SortedSet&lt;Binary&gt; parentKeys = getParentKeys(entry.getKey(), backingMapContext); // Iterate over the parent keys, enlisting and updating each parent entry for (Binary binaryKey : parentKeys) { InvocableMap.Entry&lt;SalesId, BookSales&gt; parentEntry = backingMapContext.getBackingMapEntry(binaryKey); if (parentEntry.isPresent()) { // the parent entry is present sales = parentEntry.getValue(); } else { // The parent entry is not present, so create a new BookSales value sales = new BookSales(); } // update the parent sales data sales.incrementEBookSales(eBook); sales.incrementAudioSales(audio); sales.incrementPaperSales(paper); // set the updated sale value back into the entry so that Coherence updates the cache parentEntry.setValue(sales); } return null; } Using Indexes in Queries In the code above that queries the backing map, the actual query call looks like this: <markup lang=\"java\" >Set&lt;Map.Entry&lt;SalesId, BookSales&gt;&gt; setEntries = InvocableMapHelper.query(map, indexMap, filter, true, false, null); You can see that the second parameter is a Map of indexes, which is obtained from the BackingMapContext by calling backingMapContext.getIndexMap() . This Map is the map of indexes that have been added to the cache by calls to one of the NamedCache.addIndex() methods. Using the cache indexes in this way can make a big difference to the speed and efficiently of the query execution. In the example above the Filter for the query is created like this: <markup lang=\"java\" >Filter&lt;?&gt; filter = Filters.equal(ValueExtractor.of(SalesId::getBookId).fromKey(), bookId) .and(Filters.equal(ValueExtractor.of(SalesId::getRegionCode).fromKey(), region)); The Filter is an \"and\" filter which uses two ValueExtractor instances: <markup lang=\"java\" >ValueExtractor.of(SalesId::getBookId).fromKey() ValueExtractor.of(SalesId::getRegionCode).fromKey() If we create indexes on the cache using the same extractors, then the query will be much faster: <markup lang=\"java\" >NamedCache&lt;SalesId, BookSales&gt; cache = session.getCache(\"book-sales\"); cache.addIndex(ValueExtractor.of(SalesId::getBookId).fromKey()); cache.addIndex(ValueExtractor.of(SalesId::getRegionCode).fromKey()); Using Indexes Directly The example above uses the indexes to improve query performance, but in this case as we are querying for the keys, we could have just used the indexes directly in the IncrementSalesProcessor instead of querying the backing map. If we know that our application has always created the required indexes before invoking the IncrementSalesProcessor we can simplify the code and not bother executing a query at all. We could change the getParentKeys method to use indexes directly: <markup lang=\"java\" title=\"src/main/java/com/oracle/coherence/guides/partitions/books/IncrementSalesProcessor.java\" >private SortedSet&lt;Binary&gt; getParentKeys(SalesId key, BackingMapContext backingMapContext) { BackingMapManagerContext managerContext = backingMapContext.getManagerContext(); Converter&lt;Binary, SalesId&gt; converter = managerContext.getKeyFromInternalConverter(); Map&lt;ValueExtractor, MapIndex&gt; indexMap = backingMapContext.getIndexMap(); // Get the two indexes from the index Map MapIndex indexBookId = indexMap.get(ValueExtractor.of(SalesId::getBookId).fromKey()); Map&lt;String, Set&lt;Binary&gt;&gt; indexBookIdContents = indexBookId.getIndexContents(); MapIndex indexRegion = indexMap.get(ValueExtractor.of(SalesId::getRegionCode).fromKey()); Map&lt;String, Set&lt;Binary&gt;&gt; indexRegionContents = indexRegion.getIndexContents(); // Obtain the set of Binary keys that have the required BookId Set&lt;Binary&gt; setBookId = indexBookIdContents.get(key.getBookId()); SortedSet&lt;Binary&gt; parents = new TreeSet&lt;&gt;(); SalesId parent = key; while (parent != null) { String region = parent.getParentRegionCode(); if (region == null) { // we're finished, the key has no parent region break; } // Obtain the set of Binary keys that have the parent region // and wrap them in a SubSet, so we do not mutate the real set Set&lt;Binary&gt; setRegion = new SubSet&lt;&gt;(indexBookIdContents.get(key.getBookId())); // remove any values from the set that are not in the BookId key set setRegion.retainAll(setBookId); // setRegion \"should\" now contain zero or one entry Binary binaryKey = setRegion.stream().findFirst().orElse(null); if (binaryKey == null) { // we're finished, there was no parent break; } // add the parent to the result set parents.add(binaryKey); // set the next parent parent = converter.convert(binaryKey); } return parents; } Warning When directly accessing the backing map or the index map inside an entry processor` or an aggregator in Coherence, you should never mutate these structures directly from application code. They should be treated as read-only resources. ",
            "title": "Partition Level Transactions"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " About 30 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " This example shows how to build a custom aggregator which we will use to count how many times a particular word occurs in documents stored in Coherence maps. The Document class is a standard POJO with an identifier, and a string for the document contents. What You Need About 30 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " The WordCount class implements the InvocableMap.StreamingAggregator as well as Serializable for serialization. When you implement InvocableMap.StreamingAggregator , you must implement the following methods: supply() - creates an instance we can accumulate into in parallel accumulate() - adds single entry to partial result when executing on storage members getPartialResult() - returns the partial result combine() - combines partial results on the client finalizeResult() - applies finishing transformation to the final result and returns it See below for details of each of the WordCount class. Implementing interfaces <markup lang=\"java\" >public class WordCount&lt;K extends String, V extends Document&gt; implements InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt;, Serializable { Implement the InvocableMap.StreamingAggregator with key, value, partial result and final result Implement serialization The following constructor is used to set the words to search for. <markup lang=\"java\" >/** * Constructs a {@link WordCount}. * * @param setWords {@link Set} of words to search for */ public WordCount(Set&lt;String&gt; setWords) { this.setWords = setWords; } Creates an instance we can accumulate into in parallel when executing on the storage members <markup lang=\"java\" >@Override public InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt; supply() { return new WordCount&lt;&gt;(setWords); } Adds single entry to partial result when executing on storage members <markup lang=\"java\" >@Override public boolean accumulate(InvocableMap.Entry&lt;? extends K, ? extends V&gt; entry) { Document document = entry.getValue(); for (String word : setWords) { // count how many times the word exists in the documents and accumulate int count = document.getContents().split(\"\\\\b\" + word + \"\\\\b\", -1).length - 1; this.mapResults.compute(word, (k, v) -&gt; v == null ? count : v + count); } return true; } Count the number of times the word occurs in the document Add or update the count for the word in the results Map Return the partial result <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; getPartialResult() { Logger.info(\"getPartialResult: \" + mapResults); return mapResults; } Combine all the partial results on the client <markup lang=\"java\" >@Override public boolean combine(Map&lt;String, Integer&gt; mapPartialResult) { Logger.info(\"combine: Received partial result \" + mapPartialResult); // combine the results passed in with the current set of results. if (!mapPartialResult.isEmpty()) { mapPartialResult.forEach((k, v) -&gt; mapResults.compute(k, (key, value) -&gt; value == null ? v : value + v)); } return true; } This method is called on the client to combine the results passed in with the current result. This is used to get the final set of results from all members. Take the final partial result and applies any finishing transformation <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; finalizeResult() { return mapResults; } Characteristics for the aggregator <markup lang=\"java\" >@Override public int characteristics() { return PARALLEL | PRESENT_ONLY; } We specifically set the PARALLEL and PRESENT_ONLY characteristics to indicate this can be run in parallel and to execute to only run on entries that are present. CustomAggregationExample Class The runExample() method contains the code that exercises the above custom aggregator. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { System.out.println(\"Documents added \" + documents.size()); // choose up to 5 random words from the list to search for Set&lt;String&gt; setWords = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 5; i++) { setWords.add(getRandomValue(WORDS)); } System.out.println(\"Running against the following words: \" + setWords); Map&lt;String, Integer&gt; results = documents.aggregate(new WordCount&lt;&gt;(setWords)); results.forEach((k, v) -&gt; System.out.println(\"Word \" + k + \", number of occurrences: \" + v)); } Run the aggregator against 5 randomly chosen words ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running the example The example can be run direct from the IDE by directly running the CustomAggregationExample class , or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following indicating the documents and times that a word exists in a document. <markup lang=\"bash\" >Documents added 2000 Running against the following words: [fifteen, tv, trumpet, this, launch] &lt;Info&gt; (thread=PartitionedCacheWorker:0x0000:2, member=2): ***** getPartialResult: {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=177, tv=376, trumpet=210, this=1173, launch=193} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} Word fifteen, number of occurrences: 358 Word tv, number of occurrences: 726 Word trumpet, number of occurrences: 404 Word this, number of occurrences: 2328 Word launch, number of occurrences: 382 The messages above containing combine are when the client called the combine() method to aggregate the final results returned from the storage members. In this case we had 2 storage members including the test itself. ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " The data model consists of the Document class which represents a document with text contents that we are going to search. <markup lang=\"java\" >public class Document implements Serializable { private String id; private String contents; Review the Example Code The WordCount class implements the InvocableMap.StreamingAggregator as well as Serializable for serialization. When you implement InvocableMap.StreamingAggregator , you must implement the following methods: supply() - creates an instance we can accumulate into in parallel accumulate() - adds single entry to partial result when executing on storage members getPartialResult() - returns the partial result combine() - combines partial results on the client finalizeResult() - applies finishing transformation to the final result and returns it See below for details of each of the WordCount class. Implementing interfaces <markup lang=\"java\" >public class WordCount&lt;K extends String, V extends Document&gt; implements InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt;, Serializable { Implement the InvocableMap.StreamingAggregator with key, value, partial result and final result Implement serialization The following constructor is used to set the words to search for. <markup lang=\"java\" >/** * Constructs a {@link WordCount}. * * @param setWords {@link Set} of words to search for */ public WordCount(Set&lt;String&gt; setWords) { this.setWords = setWords; } Creates an instance we can accumulate into in parallel when executing on the storage members <markup lang=\"java\" >@Override public InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt; supply() { return new WordCount&lt;&gt;(setWords); } Adds single entry to partial result when executing on storage members <markup lang=\"java\" >@Override public boolean accumulate(InvocableMap.Entry&lt;? extends K, ? extends V&gt; entry) { Document document = entry.getValue(); for (String word : setWords) { // count how many times the word exists in the documents and accumulate int count = document.getContents().split(\"\\\\b\" + word + \"\\\\b\", -1).length - 1; this.mapResults.compute(word, (k, v) -&gt; v == null ? count : v + count); } return true; } Count the number of times the word occurs in the document Add or update the count for the word in the results Map Return the partial result <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; getPartialResult() { Logger.info(\"getPartialResult: \" + mapResults); return mapResults; } Combine all the partial results on the client <markup lang=\"java\" >@Override public boolean combine(Map&lt;String, Integer&gt; mapPartialResult) { Logger.info(\"combine: Received partial result \" + mapPartialResult); // combine the results passed in with the current set of results. if (!mapPartialResult.isEmpty()) { mapPartialResult.forEach((k, v) -&gt; mapResults.compute(k, (key, value) -&gt; value == null ? v : value + v)); } return true; } This method is called on the client to combine the results passed in with the current result. This is used to get the final set of results from all members. Take the final partial result and applies any finishing transformation <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; finalizeResult() { return mapResults; } Characteristics for the aggregator <markup lang=\"java\" >@Override public int characteristics() { return PARALLEL | PRESENT_ONLY; } We specifically set the PARALLEL and PRESENT_ONLY characteristics to indicate this can be run in parallel and to execute to only run on entries that are present. CustomAggregationExample Class The runExample() method contains the code that exercises the above custom aggregator. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { System.out.println(\"Documents added \" + documents.size()); // choose up to 5 random words from the list to search for Set&lt;String&gt; setWords = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 5; i++) { setWords.add(getRandomValue(WORDS)); } System.out.println(\"Running against the following words: \" + setWords); Map&lt;String, Integer&gt; results = documents.aggregate(new WordCount&lt;&gt;(setWords)); results.forEach((k, v) -&gt; System.out.println(\"Word \" + k + \", number of occurrences: \" + v)); } Run the aggregator against 5 randomly chosen words Run the Example Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running the example The example can be run direct from the IDE by directly running the CustomAggregationExample class , or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following indicating the documents and times that a word exists in a document. <markup lang=\"bash\" >Documents added 2000 Running against the following words: [fifteen, tv, trumpet, this, launch] &lt;Info&gt; (thread=PartitionedCacheWorker:0x0000:2, member=2): ***** getPartialResult: {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=177, tv=376, trumpet=210, this=1173, launch=193} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} Word fifteen, number of occurrences: 358 Word tv, number of occurrences: 726 Word trumpet, number of occurrences: 404 Word this, number of occurrences: 2328 Word launch, number of occurrences: 382 The messages above containing combine are when the client called the combine() method to aggregate the final results returned from the storage members. In this case we had 2 storage members including the test itself. ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " In this guide we have shown you how to create custom aggregators that allow you to process data stored in Coherence in parallel. You have created a custom aggregator to count the number of times a word appears in documents stored in Coherence. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " Performing Data Grid Operations Streams ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " This guide walks you through how to create custom aggregators that allow you to process data stored in Coherence in parallel. Coherence supports entry aggregators that perform operations against all, or a subset of entries to obtain a single result. This aggregation is carried out in parallel across the cluster and is a map-reduce type of operation which can be performed efficiently across large amounts of data. See the Coherence Documentation for detailed information on Aggregations. Table of Contents What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also What You Will Build This example shows how to build a custom aggregator which we will use to count how many times a particular word occurs in documents stored in Coherence maps. The Document class is a standard POJO with an identifier, and a string for the document contents. What You Need About 30 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Example Data Model The data model consists of the Document class which represents a document with text contents that we are going to search. <markup lang=\"java\" >public class Document implements Serializable { private String id; private String contents; Review the Example Code The WordCount class implements the InvocableMap.StreamingAggregator as well as Serializable for serialization. When you implement InvocableMap.StreamingAggregator , you must implement the following methods: supply() - creates an instance we can accumulate into in parallel accumulate() - adds single entry to partial result when executing on storage members getPartialResult() - returns the partial result combine() - combines partial results on the client finalizeResult() - applies finishing transformation to the final result and returns it See below for details of each of the WordCount class. Implementing interfaces <markup lang=\"java\" >public class WordCount&lt;K extends String, V extends Document&gt; implements InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt;, Serializable { Implement the InvocableMap.StreamingAggregator with key, value, partial result and final result Implement serialization The following constructor is used to set the words to search for. <markup lang=\"java\" >/** * Constructs a {@link WordCount}. * * @param setWords {@link Set} of words to search for */ public WordCount(Set&lt;String&gt; setWords) { this.setWords = setWords; } Creates an instance we can accumulate into in parallel when executing on the storage members <markup lang=\"java\" >@Override public InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt; supply() { return new WordCount&lt;&gt;(setWords); } Adds single entry to partial result when executing on storage members <markup lang=\"java\" >@Override public boolean accumulate(InvocableMap.Entry&lt;? extends K, ? extends V&gt; entry) { Document document = entry.getValue(); for (String word : setWords) { // count how many times the word exists in the documents and accumulate int count = document.getContents().split(\"\\\\b\" + word + \"\\\\b\", -1).length - 1; this.mapResults.compute(word, (k, v) -&gt; v == null ? count : v + count); } return true; } Count the number of times the word occurs in the document Add or update the count for the word in the results Map Return the partial result <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; getPartialResult() { Logger.info(\"getPartialResult: \" + mapResults); return mapResults; } Combine all the partial results on the client <markup lang=\"java\" >@Override public boolean combine(Map&lt;String, Integer&gt; mapPartialResult) { Logger.info(\"combine: Received partial result \" + mapPartialResult); // combine the results passed in with the current set of results. if (!mapPartialResult.isEmpty()) { mapPartialResult.forEach((k, v) -&gt; mapResults.compute(k, (key, value) -&gt; value == null ? v : value + v)); } return true; } This method is called on the client to combine the results passed in with the current result. This is used to get the final set of results from all members. Take the final partial result and applies any finishing transformation <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; finalizeResult() { return mapResults; } Characteristics for the aggregator <markup lang=\"java\" >@Override public int characteristics() { return PARALLEL | PRESENT_ONLY; } We specifically set the PARALLEL and PRESENT_ONLY characteristics to indicate this can be run in parallel and to execute to only run on entries that are present. CustomAggregationExample Class The runExample() method contains the code that exercises the above custom aggregator. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { System.out.println(\"Documents added \" + documents.size()); // choose up to 5 random words from the list to search for Set&lt;String&gt; setWords = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 5; i++) { setWords.add(getRandomValue(WORDS)); } System.out.println(\"Running against the following words: \" + setWords); Map&lt;String, Integer&gt; results = documents.aggregate(new WordCount&lt;&gt;(setWords)); results.forEach((k, v) -&gt; System.out.println(\"Word \" + k + \", number of occurrences: \" + v)); } Run the aggregator against 5 randomly chosen words Run the Example Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running the example The example can be run direct from the IDE by directly running the CustomAggregationExample class , or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following indicating the documents and times that a word exists in a document. <markup lang=\"bash\" >Documents added 2000 Running against the following words: [fifteen, tv, trumpet, this, launch] &lt;Info&gt; (thread=PartitionedCacheWorker:0x0000:2, member=2): ***** getPartialResult: {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=177, tv=376, trumpet=210, this=1173, launch=193} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} Word fifteen, number of occurrences: 358 Word tv, number of occurrences: 726 Word trumpet, number of occurrences: 404 Word this, number of occurrences: 2328 Word launch, number of occurrences: 382 The messages above containing combine are when the client called the combine() method to aggregate the final results returned from the storage members. In this case we had 2 storage members including the test itself. Summary In this guide we have shown you how to create custom aggregators that allow you to process data stored in Coherence in parallel. You have created a custom aggregator to count the number of times a word appears in documents stored in Coherence. See Also Performing Data Grid Operations Streams ",
            "title": "Custom Aggregators"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The example code is written as a set of unit tests, as this is the simplest way to demonstrate something as basic as individual NamedMap operations. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The Coherence NamedMap is an extension of Java&#8217;s java.util.Map interface and as such, it has all the Map methods that a Java developer is familiar with. Coherence also has a NamedCache which extends NamedMap and is form more transient data storage in caching use cases. The most basic operations on a NamedMap are the simple CRUD methods, put , get and remove , which this guide is all about. ",
            "title": "Coherence NamedMap "
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. ",
            "title": "Bootstrap Coherence"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. ",
            "title": "Obtain a NamedMap Instance"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The first step is to create the test class that will show and test the various NamedMap operations, we&#8217;ll call this class BasicCrudTest . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class BasicCrudTest { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. Obtain a NamedMap Instance All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. ",
            "title": "Create the Test Class"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " In almost every case a NamedMap is backed by a distributed, clustered, Coherence resource. For this reason all Objects used as keys and values must be serializable so that they can be transferred between cluster members and clients during requests. Coherence Serialization support is a topic that deserves a guide of its own The Serializer implementation used by a NamedMap is configurable and Coherence comes with some out of the box Serializer implementations. The default is Java serialization, so all keys and values must be Java Serializable or implement Coherence ExternalizableLite interface for more control of serialization. Alternatively Coherence can also be configured to use Portable Object Format for serialization and additionally there is a JSON Coherence module that provides a JSON serializer that may be used. To keep this guide simple we are going to stick with the default serializer, so all NamedMap operations will use classes that are Serializable . ",
            "title": "A Quick Word About Serialization"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The obvious place to start is to add data to a NamedMap using the put method. We will create a simple test method that uses put to add a new key and value to a NamedMap . <markup lang=\"java\" > @Test void shouldPutNewKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); String oldValue = map.put(\"key-1\", \"value-1\"); assertNull(oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We call the put method to map the key \"key-1\" to the value \"value-1\" . As NamedMap implements java.util.Map , the put contract says that the put method returns the previous valued mapped to the key. In this case there was no previous value mapped to \"key-1\" , so the returned value must be null . To show that we do indeed get back the old value returned from a put , we can write a slightly different test method that puts a new key and value into a NamedMap then updates the mapping with a new value. <markup lang=\"java\" > @Test void shouldPutExistingKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-2\", \"value-1\"); String oldValue = map.put(\"key-2\", \"value-2\"); assertEquals(\"value-1\", oldValue); } ",
            "title": "The Put Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " We have seen how we can add data to a NamedMap using the put method, so the obvious next step is to get the data back out using the get method. <markup lang=\"java\" > @Test void shouldGet() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-3\", \"value-1\"); String value = map.get(\"key-3\"); assertEquals(\"value-1\", value); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the NamedMap mapping the key \"key-3\" to the value \"value-1\" ; We use the get method to get the value from the NamedMap that is mapped to the key \"key-3\" , which obviously must be \"value-1\" . ",
            "title": "The Get Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The Coherence NamedMap contains a getAll(java.util.Collection) method that takes a collection of keys as a parameter and returns a new Map that contains the requested mappings. <markup lang=\"java\" > @Test void shouldGetAll() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-5\", \"value-5\"); map.put(\"key-6\", \"value-6\"); map.put(\"key-7\", \"value-7\"); Map&lt;String, String&gt; results = map.getAll(Arrays.asList(\"key-5\", \"key-7\", \"key-8\")); assertEquals(2, results.size()); assertEquals(\"value-5\", results.get(\"key-5\")); assertEquals(\"value-7\", results.get(\"key-7\")); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. We call the getAll method requesting keys \"key-5\" , \"key-7\" and \"key-8\" . The result map returned should only contain two keys, because although we requested the mappings for three keys, \"key-8\" was not added to the NamedMap . The value mapped to \"key-5\" should be \"value-5\" . The value mapped to \"key-7\" should be \"value-7\" . ",
            "title": "Get Multiple Values"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " We&#8217;ve now seen adding data to and getting data from a NamedMap , we can also remove values mapped to a key with the remove method. <markup lang=\"java\" > @Test void shouldRemove() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-9\", \"value-9\"); String oldValue = map.remove(\"key-9\"); assertEquals(\"value-9\", oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-9\" . The contract of the remove method says that the value returned should be the value that was mapped to the key that was removed (or null if there was no mapping to the key). In this case the returned value must be \"value-9\" . ",
            "title": "The Remove Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " An alternate version of the remove method is the two argument remove method that removes a mapping to a key if the key is mapped to a specific value. <markup lang=\"java\" > @Test void shouldRemoveMapping() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-10\", \"value-10\"); boolean removed = map.remove(\"key-10\", \"Foo\"); assertFalse(removed); removed = map.remove(\"key-10\", \"value-10\"); assertTrue(removed); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-10\" with a value of \"Foo\" . This must return false as we mapped \"key-10\" to the value \"value-10\" , so nothing will be removed from the NamedMap . Call the remove method to remove the value mapped to key \"key-10\" with a value of \"value-10\" . This must return true as we mapped \"key-10\" to the value \"value-10\" , so the mapping will be removed from the NamedMap . ",
            "title": "The Remove Mapping Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " As already stated, a NamedCache is typically used to store transient data in caching use-cases. The NamedCache has an alternative put(K,V,long) method that takes a key, value, and an expiry value. The expiry value is the number of milli-seconds that the key and value should remain in the cache. When the expiry time has passed the key and value will be removed from the cache. <markup lang=\"java\" > @Test void shouldPutWithExpiry() throws Exception { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedCache&lt;String, String&gt; cache = session.getCache(\"test\"); cache.put(\"key-1\", \"value-1\", 2000); String value = cache.get(\"key-1\"); assertEquals(\"value-1\", value); Thread.sleep(3000); value = cache.get(\"key-1\"); assertNull(value); } In the same way that we obtained a NamedMap from the default Session , we can obtain a NamedCache using the getCache method, in this case the cache named test . Using the put with expiry method, we can add a key of \"key-1\" mapped to value \"value-1\" with an expiry of 2000 milli-seconds (or 2 seconds). If we now do a get for \"key-1\" we should get back \"value-1\" because two seconds has not yet passed (unless you are running this test on a terribly slow machine). Now we wait for three seconds to be sure the expiry time has passed. This time when we get \"key-1\" the value returned must be null because the value has expired, and been removed from the cache. ",
            "title": " NamedCache Transient Data"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " You have seen how simple it is to use simple CRUD methods on NamedMap and NamedCache instances, as well as the simplest way to bootstrap a default Coherence storage enabled server instance. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " This guide walks you through the basic CRUD operations on a Coherence NamedMap . What You Will Build The example code is written as a set of unit tests, as this is the simplest way to demonstrate something as basic as individual NamedMap operations. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Coherence NamedMap The Coherence NamedMap is an extension of Java&#8217;s java.util.Map interface and as such, it has all the Map methods that a Java developer is familiar with. Coherence also has a NamedCache which extends NamedMap and is form more transient data storage in caching use cases. The most basic operations on a NamedMap are the simple CRUD methods, put , get and remove , which this guide is all about. Create the Test Class The first step is to create the test class that will show and test the various NamedMap operations, we&#8217;ll call this class BasicCrudTest . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class BasicCrudTest { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. Obtain a NamedMap Instance All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. A Quick Word About Serialization In almost every case a NamedMap is backed by a distributed, clustered, Coherence resource. For this reason all Objects used as keys and values must be serializable so that they can be transferred between cluster members and clients during requests. Coherence Serialization support is a topic that deserves a guide of its own The Serializer implementation used by a NamedMap is configurable and Coherence comes with some out of the box Serializer implementations. The default is Java serialization, so all keys and values must be Java Serializable or implement Coherence ExternalizableLite interface for more control of serialization. Alternatively Coherence can also be configured to use Portable Object Format for serialization and additionally there is a JSON Coherence module that provides a JSON serializer that may be used. To keep this guide simple we are going to stick with the default serializer, so all NamedMap operations will use classes that are Serializable . The Put Method The obvious place to start is to add data to a NamedMap using the put method. We will create a simple test method that uses put to add a new key and value to a NamedMap . <markup lang=\"java\" > @Test void shouldPutNewKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); String oldValue = map.put(\"key-1\", \"value-1\"); assertNull(oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We call the put method to map the key \"key-1\" to the value \"value-1\" . As NamedMap implements java.util.Map , the put contract says that the put method returns the previous valued mapped to the key. In this case there was no previous value mapped to \"key-1\" , so the returned value must be null . To show that we do indeed get back the old value returned from a put , we can write a slightly different test method that puts a new key and value into a NamedMap then updates the mapping with a new value. <markup lang=\"java\" > @Test void shouldPutExistingKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-2\", \"value-1\"); String oldValue = map.put(\"key-2\", \"value-2\"); assertEquals(\"value-1\", oldValue); } The Get Method We have seen how we can add data to a NamedMap using the put method, so the obvious next step is to get the data back out using the get method. <markup lang=\"java\" > @Test void shouldGet() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-3\", \"value-1\"); String value = map.get(\"key-3\"); assertEquals(\"value-1\", value); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the NamedMap mapping the key \"key-3\" to the value \"value-1\" ; We use the get method to get the value from the NamedMap that is mapped to the key \"key-3\" , which obviously must be \"value-1\" . Get Multiple Values The Coherence NamedMap contains a getAll(java.util.Collection) method that takes a collection of keys as a parameter and returns a new Map that contains the requested mappings. <markup lang=\"java\" > @Test void shouldGetAll() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-5\", \"value-5\"); map.put(\"key-6\", \"value-6\"); map.put(\"key-7\", \"value-7\"); Map&lt;String, String&gt; results = map.getAll(Arrays.asList(\"key-5\", \"key-7\", \"key-8\")); assertEquals(2, results.size()); assertEquals(\"value-5\", results.get(\"key-5\")); assertEquals(\"value-7\", results.get(\"key-7\")); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. We call the getAll method requesting keys \"key-5\" , \"key-7\" and \"key-8\" . The result map returned should only contain two keys, because although we requested the mappings for three keys, \"key-8\" was not added to the NamedMap . The value mapped to \"key-5\" should be \"value-5\" . The value mapped to \"key-7\" should be \"value-7\" . The Remove Method We&#8217;ve now seen adding data to and getting data from a NamedMap , we can also remove values mapped to a key with the remove method. <markup lang=\"java\" > @Test void shouldRemove() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-9\", \"value-9\"); String oldValue = map.remove(\"key-9\"); assertEquals(\"value-9\", oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-9\" . The contract of the remove method says that the value returned should be the value that was mapped to the key that was removed (or null if there was no mapping to the key). In this case the returned value must be \"value-9\" . The Remove Mapping Method An alternate version of the remove method is the two argument remove method that removes a mapping to a key if the key is mapped to a specific value. <markup lang=\"java\" > @Test void shouldRemoveMapping() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-10\", \"value-10\"); boolean removed = map.remove(\"key-10\", \"Foo\"); assertFalse(removed); removed = map.remove(\"key-10\", \"value-10\"); assertTrue(removed); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-10\" with a value of \"Foo\" . This must return false as we mapped \"key-10\" to the value \"value-10\" , so nothing will be removed from the NamedMap . Call the remove method to remove the value mapped to key \"key-10\" with a value of \"value-10\" . This must return true as we mapped \"key-10\" to the value \"value-10\" , so the mapping will be removed from the NamedMap . NamedCache Transient Data As already stated, a NamedCache is typically used to store transient data in caching use-cases. The NamedCache has an alternative put(K,V,long) method that takes a key, value, and an expiry value. The expiry value is the number of milli-seconds that the key and value should remain in the cache. When the expiry time has passed the key and value will be removed from the cache. <markup lang=\"java\" > @Test void shouldPutWithExpiry() throws Exception { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedCache&lt;String, String&gt; cache = session.getCache(\"test\"); cache.put(\"key-1\", \"value-1\", 2000); String value = cache.get(\"key-1\"); assertEquals(\"value-1\", value); Thread.sleep(3000); value = cache.get(\"key-1\"); assertNull(value); } In the same way that we obtained a NamedMap from the default Session , we can obtain a NamedCache using the getCache method, in this case the cache named test . Using the put with expiry method, we can add a key of \"key-1\" mapped to value \"value-1\" with an expiry of 2000 milli-seconds (or 2 seconds). If we now do a get for \"key-1\" we should get back \"value-1\" because two seconds has not yet passed (unless you are running this test on a terribly slow machine). Now we wait for three seconds to be sure the expiry time has passed. This time when we get \"key-1\" the value returned must be null because the value has expired, and been removed from the cache. Summary You have seen how simple it is to use simple CRUD methods on NamedMap and NamedCache instances, as well as the simplest way to bootstrap a default Coherence storage enabled server instance. ",
            "title": "Put Get and Remove Operations"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " A common use case in Coherence is for caches to hold data that comes from other sources, typically a database. Often there is a requirement to preload data into caches when an application starts up. Using a CacheLoader to load data on demand may be suitable for many caching use cases, but other use cases such as querying and aggregating caches require all the data to be present in the cache. Over the years, there have been a number of patters to achieve preloading, this guide will cover some currently recommended approaches to preloading data. Whilst this guide reads data from a database and pushes it into caches, the same patterns can apply to any data source, for example, preloading from files, messaging systems, data streaming systems, etc. ",
            "title": "Bulk Loading Caches"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The example in this guide builds a simple application that uses different techniques to preload caches from a database. This includes examples of preloading from a database into a cache that uses a cachestore to write cache data to the same database. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The cache preloader is typically implemented as a process that is run after the storage enabled cluster members have started. This is typically a simple Java class with a main method that runs and controls loading the data. As this application will need to communicate with the Coherence storage members, it can obviously run in two modes, either as a storage disabled cluster member or as an Extend client. As the preload application&#8217;s only job is to push a large amount of data into Coherence caches as fast as possible, it is recommended to run the preloader as a storage disabled cluster member. Running as an Extend client would cause a bottleneck trying to push all the data over a single Extend connection to the proxy, where it is then distributed across the cluster. ",
            "title": "Extend Client or Cluster Member?"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " A simple preload task might look like the example below: <markup lang=\"java\" >public class CustomerPreloadTask implements Runnable { private final Connection connection; private final Session session; public PreloadTask(Connection connection, Session session) { this.connection = connection; this.session = session; } @Override public void run() { NamedMap&lt;Integer, Customer&gt; namedMap = session.getMap(\"customers\"); String query = \"SELECT id, name, address, creditLimit FROM customers\"; int batchSize = 10; try (PreparedStatement statement = connection.prepareStatement(query); ResultSet resultSet = statement.executeQuery()) { Map&lt;Integer, Customer&gt; batch = new HashMap&lt;&gt;(batchSize); while (resultSet.next()) { int key = resultSet.getInt(\"id\"); Customer value = new Customer(resultSet.getInt(\"id\"), resultSet.getString(\"name\"), resultSet.getString(\"address\"), resultSet.getInt(\"creditLimit\")); batch.put(key, value); if (batch.size() &gt;= batchSize) { namedMap.putAll(batch); batch.clear(); } } if (!batch.isEmpty()) { namedMap.putAll(batch); batch.clear(); } } catch (SQLException e) { throw Exceptions.ensureRuntimeException(e); } } } Obtain the cache to be loaded from the Coherence Session , in this case the \"customers\" cache. Perform the query on the database to get the data to load. There are many ways this could be done depending on the database. This is a very simple approach using a PreparedStatement . Create a Map to hold a batch of entries to load. This preload task will call NamedMap.putAll() to load the dat ain batches into the cache. This is more efficient that multiple single put calls. Iterate over all the rows of data returned by the ResultSet . Create the cache key from the current row in the ResultSet (in this case the key is just the \"id\" int). Create the cache value from the current row in the ResultSet . Add the key and new Customer to the batch map. If the batch map is &gt;= the batch size, put the batch into the cache using putAll() , then clear the batch map. After all the rows in the ResultSet have been iterated over, there may be entries in the batch map that need to be loaded, so put them in the cache, A different implementation of this class can be created to load different caches from different database tables. Ideally the common code would be extracted into an abstract base class. This is what the example code does in the AbstractJdbcPreloadTask , which is a base class for loading caches from a database. Concrete implementations are in the example test code in the CustomerJdbcLoader and OrderJdbcLoader classes. ",
            "title": "A Simple Preload Runnable"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " If the cache loaders are written as Runnable instances, as shown above, they can easily be run in parallel using a Java Executor . For example, if there were three preload tasks, written like the example above, to load Customers, Orders, and Products, they could be submitted to an ExecutorService as shown below. <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); ExecutorService executor = Executors.newCachedThreadPool(); executor.submit(new CustomerPreloadTask(session)); executor.submit(new OrdersPreloadTask(session)); executor.submit(new ProductsPreloadTask(session)); The loader application can wait for the executor to complete all the tasks, ideally with a timeout so that if there is an issue, it does not run forever. <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); ExecutorService executor = Executors.newCachedThreadPool(); executor.submit(new CustomerPreloadTask(session)); executor.submit(new OrdersPreloadTask(session)); executor.submit(new ProductsPreloadTask(session)); executor.shutdown(); boolean terminated = executor.awaitTermination(1, TimeUnit.HOURS); if (!terminated) { executor.shutdownNow(); } Stop the executor accepting any more requests. Wait a maximum of one hour for the executor to complete running the tasks. If the executor has not terminated in one hour, forcefully terminate. ",
            "title": "Running the Loaders"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The basic code to load a simple cache from a database (or other data source) is usually very simple, as shown above. There are many variations on this pattern to make loading scale better and execute faster. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The typical preloading use case is to pull data from a datasource and load it into caches as fast as possible. This means ideally scaling out the preloading to be batched and multi-threaded, to load multiple caches at once. The pre-loaders in this example will be written as simple Java Runnable implementations. This allows them to be easily scaled up by running them in a Java Executor or daemon thread pool. A separate Coherence example will show how to use Coherence concurrent Executor Service module to scale out preloading by distributing the preload runnables to execute on Coherence cluster members. Extend Client or Cluster Member? The cache preloader is typically implemented as a process that is run after the storage enabled cluster members have started. This is typically a simple Java class with a main method that runs and controls loading the data. As this application will need to communicate with the Coherence storage members, it can obviously run in two modes, either as a storage disabled cluster member or as an Extend client. As the preload application&#8217;s only job is to push a large amount of data into Coherence caches as fast as possible, it is recommended to run the preloader as a storage disabled cluster member. Running as an Extend client would cause a bottleneck trying to push all the data over a single Extend connection to the proxy, where it is then distributed across the cluster. A Simple Preload Runnable A simple preload task might look like the example below: <markup lang=\"java\" >public class CustomerPreloadTask implements Runnable { private final Connection connection; private final Session session; public PreloadTask(Connection connection, Session session) { this.connection = connection; this.session = session; } @Override public void run() { NamedMap&lt;Integer, Customer&gt; namedMap = session.getMap(\"customers\"); String query = \"SELECT id, name, address, creditLimit FROM customers\"; int batchSize = 10; try (PreparedStatement statement = connection.prepareStatement(query); ResultSet resultSet = statement.executeQuery()) { Map&lt;Integer, Customer&gt; batch = new HashMap&lt;&gt;(batchSize); while (resultSet.next()) { int key = resultSet.getInt(\"id\"); Customer value = new Customer(resultSet.getInt(\"id\"), resultSet.getString(\"name\"), resultSet.getString(\"address\"), resultSet.getInt(\"creditLimit\")); batch.put(key, value); if (batch.size() &gt;= batchSize) { namedMap.putAll(batch); batch.clear(); } } if (!batch.isEmpty()) { namedMap.putAll(batch); batch.clear(); } } catch (SQLException e) { throw Exceptions.ensureRuntimeException(e); } } } Obtain the cache to be loaded from the Coherence Session , in this case the \"customers\" cache. Perform the query on the database to get the data to load. There are many ways this could be done depending on the database. This is a very simple approach using a PreparedStatement . Create a Map to hold a batch of entries to load. This preload task will call NamedMap.putAll() to load the dat ain batches into the cache. This is more efficient that multiple single put calls. Iterate over all the rows of data returned by the ResultSet . Create the cache key from the current row in the ResultSet (in this case the key is just the \"id\" int). Create the cache value from the current row in the ResultSet . Add the key and new Customer to the batch map. If the batch map is &gt;= the batch size, put the batch into the cache using putAll() , then clear the batch map. After all the rows in the ResultSet have been iterated over, there may be entries in the batch map that need to be loaded, so put them in the cache, A different implementation of this class can be created to load different caches from different database tables. Ideally the common code would be extracted into an abstract base class. This is what the example code does in the AbstractJdbcPreloadTask , which is a base class for loading caches from a database. Concrete implementations are in the example test code in the CustomerJdbcLoader and OrderJdbcLoader classes. Running the Loaders If the cache loaders are written as Runnable instances, as shown above, they can easily be run in parallel using a Java Executor . For example, if there were three preload tasks, written like the example above, to load Customers, Orders, and Products, they could be submitted to an ExecutorService as shown below. <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); ExecutorService executor = Executors.newCachedThreadPool(); executor.submit(new CustomerPreloadTask(session)); executor.submit(new OrdersPreloadTask(session)); executor.submit(new ProductsPreloadTask(session)); The loader application can wait for the executor to complete all the tasks, ideally with a timeout so that if there is an issue, it does not run forever. <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); ExecutorService executor = Executors.newCachedThreadPool(); executor.submit(new CustomerPreloadTask(session)); executor.submit(new OrdersPreloadTask(session)); executor.submit(new ProductsPreloadTask(session)); executor.shutdown(); boolean terminated = executor.awaitTermination(1, TimeUnit.HOURS); if (!terminated) { executor.shutdownNow(); } Stop the executor accepting any more requests. Wait a maximum of one hour for the executor to complete running the tasks. If the executor has not terminated in one hour, forcefully terminate. Summary The basic code to load a simple cache from a database (or other data source) is usually very simple, as shown above. There are many variations on this pattern to make loading scale better and execute faster. ",
            "title": "Typical Preloading Use Case"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " In this example, a simple controller is used with a boolean field for enabling or disabling the CacheStore . The example source code contains the SimpleController class shown below: <markup lang=\"java\" title=\"SimpleController.java\" >public class SimpleController implements ControllableCacheStore.Controller { @Override public boolean isEnabled() { return enabled; } public void setEnabled(boolean enabled) { this.enabled = enabled; } } ",
            "title": "The Controller Implementation"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " For a cache to use a cache store, it needs to be configured in the cache configuration file to use a &lt;read-write-backing-map&gt; , which in turn configures the CacheStore implementation to use. There are a few ways to configure the CacheStore , either using the implementation class name directly, or using a factory class and static factory method. In this example we will use the second approach, this means determining the cache store to use will be done in a factory class rather than in configuration, but this fits our use case better. The &lt;distributed-scheme&gt; used in the example test code is shown below: <markup lang=\"xml\" title=\"controllable-cachestore-cache-config.xml\" > &lt;distributed-scheme&gt; &lt;scheme-name&gt;db-storage&lt;/scheme-name&gt; &lt;service-name&gt;StorageService&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-factory-name&gt; com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory &lt;/class-factory-name&gt; &lt;method-name&gt;createControllableCacheStore&lt;/method-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"jdbc.url\"&gt; jdbc:hsqldb:mem:test &lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; The CacheStore factory class is com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory The static factory method on the CacheStoreFactory class that will be called to create a CacheStore is createControllableCacheStore The createControllableCacheStore has two parameters, which are configured in the &lt;init-params&gt; , the first is the name of the cache. The Coherence configuration macro {cache-name} will pass the name of the cache being created to the factory method. The second parameter is the JDBC URL of the database to load data from, in the example this defaults to the HSQL in-memory test database. The CacheStoreFactory method createControllableCacheStore used in the example is shown below <markup lang=\"java\" title=\"filename.java\" >public static CacheStore createControllableCacheStore(String cacheName, String jdbcURL) throws Exception { CacheStore delegate; switch (cacheName.toLowerCase()) { case \"customers\": delegate = new CustomerCacheStore(jdbcURL); break; case \"orders\": delegate = new OrderCacheStore(jdbcURL); break; default: throw new IllegalArgumentException(\"Cannot create cache store for cache \" + cacheName); } return new ControllableCacheStore&lt;&gt;(new SimpleController(), delegate); } The code does a simple switch on the cache name to determine the actual CacheStore to create. If the cache name is \"customers\", then the CustomerCacheStore is created If the cache name is \"orders\", then the OrderCacheStore is created An exception is thrown for an unknown cache name Finally, a ControllableCacheStore is returned that uses a SimpleController and wraps the delegate CacheStore Now, when application code first requests either the \"customers\" cache or the \"orders\" cache, the cache Coherence will create the cache and call the CacheStoreFactory.createControllableCacheStore method to create the CacheStore . ",
            "title": "Configuring and Creating the CacheStore"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " Now we have caches that are configured to use the ControllableCacheStore with the SimpleController , the next step is to actually be able to enable or disable the controller so that the preload can run. In the SimpleController the setEnabled() method needs to be called to set the controlling boolean flag. For each cache configured to use the ControllableCacheStore there will be an instance of SimpleController in every storage enabled cluster member. A method is needed of calling the setEnabled() method on all these instances, and this can be done with an EntryProcessor . A section of the source for the SetEnabledProcessor is shown below (the methods for serialization have been omitted here, but are in the actual GitHub source). <markup lang=\"java\" title=\"SetEnabledProcessor.java\" >public static class SetEnabledProcessor&lt;K, V&gt; implements InvocableMap.EntryProcessor&lt;K, V, Void&gt;, PortableObject, ExternalizableLite { private boolean enabled; public SetEnabledProcessor() { } public SetEnabledProcessor(boolean enabled) { this.enabled = enabled; } @Override public Void process(InvocableMap.Entry&lt;K, V&gt; entry) { ObservableMap&lt;? extends K, ? extends V&gt; backingMap = entry.asBinaryEntry().getBackingMap(); if (backingMap instanceof ReadWriteBackingMap) { ReadWriteBackingMap.StoreWrapper wrapper = ((ReadWriteBackingMap) backingMap).getCacheStore(); Object o = wrapper.getStore(); if (o instanceof ControllableCacheStore) { ControllableCacheStore.Controller controller = ((ControllableCacheStore) o).getController(); if (controller instanceof SimpleController) { ((SimpleController) controller).setEnabled(enabled); } } } return null; } // PortableObject and ExternalizableLite method are omitted here... } In the process method, the backing map is obtained from the entry . The getBackingMap() is deprecated, mainly as a warning that this is quite a dangerous thing to do if you are not careful. The backing map is the internal structure used by Coherence to hold cache data, and directly manipulating it can have adverse effects. In this case we are not manipulating the backing map, so we are safe. If the cache is configured as with a &lt;read-write-backing-map&gt; then the implementation of the backing map here will be ReadWriteBackingMap . We can obtain the CacheStore being used from the ReadWriteBackingMap API, and check whether it is a ControllableCacheStore . If it is, we can get the Controller being used, and if it is a SimpleController set the flag. Now we have the SetEnabledProcessor we need to execute it so that we guarantee it runs on every storage enabled member. Using something like cache.invokeAll(Filters.always(), new SetEnabledProcessor()) will not work, because this will only execute on members where there are entries, and there are none as we are about to do a preload. One of the things to remember about methods like cache.invokeAll(keySet, new SetEnabledProcessor()) is that the keySet can contain keys that do not need to exist in the cache. As long as the entry processor does not call entry.setValue() the entry it executes against will never exist. Another feature of Coherence is the ability to influence the partition a key belongs to by writing a key class that implements the com.tangosol.net.partition.KeyPartitioningStrategy.PartitionAwareKey interface. Coherence has a built-in implementation of this class called com.tangosol.net.partition.SimplePartitionKey . We can make use of both these features to create a set of keys where we can guarantee we have one key for each partition in a cache. If we use this as the key set in an invokeAll method, we will guarantee to execute the EntryProcessor in every partition, and hence on every storage enable cluster member. The snippet of code below shows how to execute the SetEnabledProcessor to disable the cache stores for a cache. Changing the line new SetEnabledProcessor(false) to new SetEnabledProcessor(true) will instead enable the cache stores. <markup lang=\"java\" title=\"SimpleController.java\" >public static void disableCacheStores(NamedMap&lt;?, ?&gt; namedMap) { CacheService service = namedMap.getService(); int partitionCount = ((DistributedCacheService) service).getPartitionCount(); Set&lt;SimplePartitionKey&gt; keys = new HashSet&lt;&gt;(); for (int i = 0; i &lt; partitionCount; i++) { keys.add(SimplePartitionKey.getPartitionKey(i)); } namedMap.invokeAll(keys, new SetEnabledProcessor(false)); } Obtain the cache service for the NamedMap that has the ControllableCacheSTore to enable The cache service should actually be an instance of DistributedCacheService , from which we can get the partition count. The default is 257, but this could have been changed. Create a Set&lt;SimplePartitionKey&gt; that will hold the keys for the invokeAll In a for loop, create a SimplePartitionKey for every partition, and add it to the keys set The keys set can be used in the invokeAll call to invoke the SetEnabledProcessor on every partition Running the SetEnabledProcessor on every partition means it actually executes more times than it needs to, but this is not a problem, as repeated executions in the same JVM just set the same flag for the enabled value. Now we have a way to enable and disable the ControllableCacheStore , we can execute this code before running the preload, and then re-enable the cache stores after running the preload. ",
            "title": "Enabling and Disabling the ControllableCacheStore"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " A controllable cache store is reasonably simple, but it will really not work in cases where the cache is configured to use write-behind. With write-behind enabled, if the controllable cache store is turned back on too soon after loading (i.e. within the write delay time) then the data loaded to the cache that is still in the write-behind queue will be written to the database. A controllable cache store is also not going to work in situations where the application could be updating entries in the cache while the preload is still running. If there are a mixture of entries, some needing to be written and some not, the controllable cache store will not be suitable. Another caveat with the SimpleController above, is what happens during failure of a storage member. If a storage member in the cluster fails, that is not an issue, but in environments such as Kubernetes, where that failed member will be replaced automatically, that can be a problem. The new member will join the cluster, caches will be created on it, including the ControllableCacheStore for configured caches. The problem is that the boolean flag in the new member&#8217;s SimpleController will not be set to false , so the new member will start storing entries it receives to the database. Ideally, new members do not join during preload, but this may be out of the developers control. This could require a more complex controller, for example checking an entry in a cache for its initial state, etc. ",
            "title": "ControllableCacheStore Caveats"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " As the name suggests, a controllable CacheStore can be turned on or off. The CacheStore would be turned off before the preload tasks ran, then turned back on after the preload is complete. There have been a few patterns for controllable cache stores suggested in the past, including an example in the official Coherence documentation where the enable/disable flag is a boolean value stored in another cache. With a little more thought we can see that this is not really a good idea. Consider what a bulk preload is doing, it is loading a very large amount of data into caches, that will then call the CacheStore methods. If the CacheStore needed to look up a distributed cache value on every store call, that would be massively inefficient. Even accessing a cache from inside a CacheStore could be problematic due to making a remote call from a cache service worker thread, which may cause a deadlock. Even introducing near caching or a view cache would not necessarily help, as updates to the flag would be async. Checking the flag that controls the CacheStore needs to be as efficient as possible. For that reason, the example here just uses a simple boolean field in the controller itself. An EntryProcessor is then used to directly set the flag for the controller. How this works will be explained below. The code in this example has a ControllableCacheStore class that implements CacheStore and has a Controller that enables or disables operations. This allows the ControllableCacheStore to be controlled in different ways just by implementing different types of Controller . The ControllableCacheStore also just delegates operations to another CacheStore , it does not do anything itself. The ControllableCacheStore calls the delegate if the controller says it is enabled, otherwise it does nothing. This makes the ControllableCacheStore a simple class that can be reused to make any existing, or new, CacheStore implementation be controllable. A small section of the ControllableCacheStore class is shown below: <markup lang=\"java\" title=\"ControllableCacheStore.java\" >public class ControllableCacheStore&lt;K, V&gt; implements CacheStore&lt;K, V&gt; { private final Controller controller; private final CacheStore&lt;K, V&gt; delegate; public ControllableCacheStore(Controller controller, CacheStore&lt;K, V&gt; delegate) { this.controller = controller; this.delegate = delegate; } @Override public void store(K key, V value) { if (controller.isEnabled()) { delegate.store(key, value); } } @Override public void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) { if (controller.isEnabled()) { delegate.storeAll(mapEntries); } } // other methods omitted ... /** * Implementations of {@link Controller} can * control a {@link ControllableCacheStore}. */ public interface Controller { boolean isEnabled(); } } It should be obvious how the class works. The Controller is an inner interface, and an implementation of this is passed to the constructor, along with the delegate CacheStore . Each method call (only store and storeAll are shown above) calls the controller&#8217;s isEnabled() method to determine whether the delegate should be called. The Controller Implementation In this example, a simple controller is used with a boolean field for enabling or disabling the CacheStore . The example source code contains the SimpleController class shown below: <markup lang=\"java\" title=\"SimpleController.java\" >public class SimpleController implements ControllableCacheStore.Controller { @Override public boolean isEnabled() { return enabled; } public void setEnabled(boolean enabled) { this.enabled = enabled; } } Configuring and Creating the CacheStore For a cache to use a cache store, it needs to be configured in the cache configuration file to use a &lt;read-write-backing-map&gt; , which in turn configures the CacheStore implementation to use. There are a few ways to configure the CacheStore , either using the implementation class name directly, or using a factory class and static factory method. In this example we will use the second approach, this means determining the cache store to use will be done in a factory class rather than in configuration, but this fits our use case better. The &lt;distributed-scheme&gt; used in the example test code is shown below: <markup lang=\"xml\" title=\"controllable-cachestore-cache-config.xml\" > &lt;distributed-scheme&gt; &lt;scheme-name&gt;db-storage&lt;/scheme-name&gt; &lt;service-name&gt;StorageService&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-factory-name&gt; com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory &lt;/class-factory-name&gt; &lt;method-name&gt;createControllableCacheStore&lt;/method-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"jdbc.url\"&gt; jdbc:hsqldb:mem:test &lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; The CacheStore factory class is com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory The static factory method on the CacheStoreFactory class that will be called to create a CacheStore is createControllableCacheStore The createControllableCacheStore has two parameters, which are configured in the &lt;init-params&gt; , the first is the name of the cache. The Coherence configuration macro {cache-name} will pass the name of the cache being created to the factory method. The second parameter is the JDBC URL of the database to load data from, in the example this defaults to the HSQL in-memory test database. The CacheStoreFactory method createControllableCacheStore used in the example is shown below <markup lang=\"java\" title=\"filename.java\" >public static CacheStore createControllableCacheStore(String cacheName, String jdbcURL) throws Exception { CacheStore delegate; switch (cacheName.toLowerCase()) { case \"customers\": delegate = new CustomerCacheStore(jdbcURL); break; case \"orders\": delegate = new OrderCacheStore(jdbcURL); break; default: throw new IllegalArgumentException(\"Cannot create cache store for cache \" + cacheName); } return new ControllableCacheStore&lt;&gt;(new SimpleController(), delegate); } The code does a simple switch on the cache name to determine the actual CacheStore to create. If the cache name is \"customers\", then the CustomerCacheStore is created If the cache name is \"orders\", then the OrderCacheStore is created An exception is thrown for an unknown cache name Finally, a ControllableCacheStore is returned that uses a SimpleController and wraps the delegate CacheStore Now, when application code first requests either the \"customers\" cache or the \"orders\" cache, the cache Coherence will create the cache and call the CacheStoreFactory.createControllableCacheStore method to create the CacheStore . Enabling and Disabling the ControllableCacheStore Now we have caches that are configured to use the ControllableCacheStore with the SimpleController , the next step is to actually be able to enable or disable the controller so that the preload can run. In the SimpleController the setEnabled() method needs to be called to set the controlling boolean flag. For each cache configured to use the ControllableCacheStore there will be an instance of SimpleController in every storage enabled cluster member. A method is needed of calling the setEnabled() method on all these instances, and this can be done with an EntryProcessor . A section of the source for the SetEnabledProcessor is shown below (the methods for serialization have been omitted here, but are in the actual GitHub source). <markup lang=\"java\" title=\"SetEnabledProcessor.java\" >public static class SetEnabledProcessor&lt;K, V&gt; implements InvocableMap.EntryProcessor&lt;K, V, Void&gt;, PortableObject, ExternalizableLite { private boolean enabled; public SetEnabledProcessor() { } public SetEnabledProcessor(boolean enabled) { this.enabled = enabled; } @Override public Void process(InvocableMap.Entry&lt;K, V&gt; entry) { ObservableMap&lt;? extends K, ? extends V&gt; backingMap = entry.asBinaryEntry().getBackingMap(); if (backingMap instanceof ReadWriteBackingMap) { ReadWriteBackingMap.StoreWrapper wrapper = ((ReadWriteBackingMap) backingMap).getCacheStore(); Object o = wrapper.getStore(); if (o instanceof ControllableCacheStore) { ControllableCacheStore.Controller controller = ((ControllableCacheStore) o).getController(); if (controller instanceof SimpleController) { ((SimpleController) controller).setEnabled(enabled); } } } return null; } // PortableObject and ExternalizableLite method are omitted here... } In the process method, the backing map is obtained from the entry . The getBackingMap() is deprecated, mainly as a warning that this is quite a dangerous thing to do if you are not careful. The backing map is the internal structure used by Coherence to hold cache data, and directly manipulating it can have adverse effects. In this case we are not manipulating the backing map, so we are safe. If the cache is configured as with a &lt;read-write-backing-map&gt; then the implementation of the backing map here will be ReadWriteBackingMap . We can obtain the CacheStore being used from the ReadWriteBackingMap API, and check whether it is a ControllableCacheStore . If it is, we can get the Controller being used, and if it is a SimpleController set the flag. Now we have the SetEnabledProcessor we need to execute it so that we guarantee it runs on every storage enabled member. Using something like cache.invokeAll(Filters.always(), new SetEnabledProcessor()) will not work, because this will only execute on members where there are entries, and there are none as we are about to do a preload. One of the things to remember about methods like cache.invokeAll(keySet, new SetEnabledProcessor()) is that the keySet can contain keys that do not need to exist in the cache. As long as the entry processor does not call entry.setValue() the entry it executes against will never exist. Another feature of Coherence is the ability to influence the partition a key belongs to by writing a key class that implements the com.tangosol.net.partition.KeyPartitioningStrategy.PartitionAwareKey interface. Coherence has a built-in implementation of this class called com.tangosol.net.partition.SimplePartitionKey . We can make use of both these features to create a set of keys where we can guarantee we have one key for each partition in a cache. If we use this as the key set in an invokeAll method, we will guarantee to execute the EntryProcessor in every partition, and hence on every storage enable cluster member. The snippet of code below shows how to execute the SetEnabledProcessor to disable the cache stores for a cache. Changing the line new SetEnabledProcessor(false) to new SetEnabledProcessor(true) will instead enable the cache stores. <markup lang=\"java\" title=\"SimpleController.java\" >public static void disableCacheStores(NamedMap&lt;?, ?&gt; namedMap) { CacheService service = namedMap.getService(); int partitionCount = ((DistributedCacheService) service).getPartitionCount(); Set&lt;SimplePartitionKey&gt; keys = new HashSet&lt;&gt;(); for (int i = 0; i &lt; partitionCount; i++) { keys.add(SimplePartitionKey.getPartitionKey(i)); } namedMap.invokeAll(keys, new SetEnabledProcessor(false)); } Obtain the cache service for the NamedMap that has the ControllableCacheSTore to enable The cache service should actually be an instance of DistributedCacheService , from which we can get the partition count. The default is 257, but this could have been changed. Create a Set&lt;SimplePartitionKey&gt; that will hold the keys for the invokeAll In a for loop, create a SimplePartitionKey for every partition, and add it to the keys set The keys set can be used in the invokeAll call to invoke the SetEnabledProcessor on every partition Running the SetEnabledProcessor on every partition means it actually executes more times than it needs to, but this is not a problem, as repeated executions in the same JVM just set the same flag for the enabled value. Now we have a way to enable and disable the ControllableCacheStore , we can execute this code before running the preload, and then re-enable the cache stores after running the preload. ControllableCacheStore Caveats A controllable cache store is reasonably simple, but it will really not work in cases where the cache is configured to use write-behind. With write-behind enabled, if the controllable cache store is turned back on too soon after loading (i.e. within the write delay time) then the data loaded to the cache that is still in the write-behind queue will be written to the database. A controllable cache store is also not going to work in situations where the application could be updating entries in the cache while the preload is still running. If there are a mixture of entries, some needing to be written and some not, the controllable cache store will not be suitable. Another caveat with the SimpleController above, is what happens during failure of a storage member. If a storage member in the cluster fails, that is not an issue, but in environments such as Kubernetes, where that failed member will be replaced automatically, that can be a problem. The new member will join the cluster, caches will be created on it, including the ControllableCacheStore for configured caches. The problem is that the boolean flag in the new member&#8217;s SimpleController will not be set to false , so the new member will start storing entries it receives to the database. Ideally, new members do not join during preload, but this may be out of the developers control. This could require a more complex controller, for example checking an entry in a cache for its initial state, etc. ",
            "title": "Controllable CacheStore"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " A com.tangosol.util.Binary is usually used to hold a serialized binary value (occasionally it may also be a com.tangosol.io.ReadBuffer ). As well as the serialized data, that could have been serialized using any serializer configured in Coherence (Java, POF, json, etc) a Binary can be decorated with other information. Each decoration has an int identifier that is used to add, remove or obtain a specific decoration. Coherence itself uses decorations for a number of functions, so a number of decoration identifiers values are reserved. The identifiers are all stored in constants in com.tangosol.util.ExternalizableHelper class and all have the prefix DECO_ , for example DECO_EXPIRY . There are three identifiers reserved for use by application code, DECO_APP_1 , DECO_APP_2 and DECO_APP_3 which Coherence will not use, so for this example we can use ExternalizableHelper.DECO_APP_1 for the SmartCacheStore decoration. The method used to decorate a Binary is ExternalizableHelper method: <markup lang=\"java\" >public static Binary decorate(Binary bin, int nId, Binary binDecoration) A Binary is decorated with another Binary value. For the SmartCacheStore we do not care what the value of the decoration is, we only check whether it is present or not. Obviously we do not want to use a large Binary decoration, as this will add to the serialized size of the value, the smallest possible Binary is the constant value Binary.NO_BINARY , which is actually zero bytes, but still a Binary . We can therefore decorate a Binary for use in the SmartCacheStore like this: <markup lang=\"java\" >Binary binary = // create the serialized binary value Binary decorated = ExternalizableHelper.decorate(binary, ExternalizableHelper.DECO_APP_1, Binary.NO_BINARY); ",
            "title": "Decorating a Binary"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The example below shows part of the SmartCacheStore implementation. The store and storeAll methods are shown, but the other BinaryEntryStore are omitted here to make things clearer. <markup lang=\"java\" title=\"SmartCacheStore.java\" >public class SmartCacheStore&lt;K, V&gt; implements BinaryEntryStore&lt;K, V&gt; { private final BinaryEntryStore&lt;K, V&gt; delegate; public SmartCacheStore(CacheStore&lt;K, V&gt; delegate) { this.delegate = new WrapperBinaryEntryStore&lt;&gt;(Objects.requireNonNull(delegate)); } @Override public void store(BinaryEntry&lt;K, V&gt; entry) { if (shouldStore(entry)) { delegate.store(entry); } } @Override public void storeAll(Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entries) { Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entriesToStore = entries.stream() .filter(this::shouldStore) .collect(Collectors.toSet()); if (entriesToStore.size() &gt; 0) { delegate.storeAll(entriesToStore); } } private boolean shouldStore(BinaryEntry&lt;K, V&gt; entry) { return !ExternalizableHelper.isDecorated(entry.getBinaryValue(), ExternalizableHelper.DECO_APP_1); } // other BinaryEntryStore omitted... } The delegate CacheStore passed to the constructor is wrapped in a WrapperBinaryEntryStore to make it look like a BinaryEntryStore that the SmartCacheStore can delegate to. The store method is passed the BinaryEntry to be stored. To check whether the delegate should be called, it calls the shouldStore method. If shouldStore returns true the delegate is called to store the entry. The storeAll method is similar to the store method, but is passed a Set of entries to store. The set is filtered to create a new Set containing only entries that should be stored. If there are any entries to store the delegate is called. The shouldStore method checks to see whether an entry should be stored. The Binary value is obtained from the BinaryEntry and checked to see whether the ExternalizableHelper.DECO_APP_1 decoration is present. ",
            "title": "The SmartCacheStore Implementation"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " Now we have a SmartCacheStore that only stores non-decorated values, how do we load decorated Binary values into a cache? This example only works on a cluster member, because it requires access to the server side cache service that the cache being loaded is using. This will allow the preloader to create serialized Binary keys and values using the correct serilzation method for the cache. Obtain a Binary NamedMap or NamedCache The usual way to obtain a NamedMap (or NamedCache ) from a Coherence Session is to just call session.getMap(cacheName); but the getMap() and getCache() method allow options to be passed in to control the cache returned. One of these is the WithClassLoader option, that takes a ClassLoader . Coherence has a special class loader obtained from com.tangosol.util.NullImplementation.getClassLoader() . If this is used, the cache returned is a binary cache, this means that values passed to methods such as get , put , putAll` etc., must be instances of Binary , i.e. a binary cache gives access to the actual serialized data in the cache. The code below gets the normal \"customers\" cache, with Integer keys and Customer values. <markup lang=\"java\" >NamedMap&lt;Integer, Customer&gt; namedMap = session.getMap(\"customers\"); Whereas the code below gets the same \"customers\" cache but with Binary keys (serializer Integer instances in this example) and Binary values (serialized Customer instances in this example). <markup lang=\"java\" >NamedMap&lt;Binary, Binary&gt; namedMap = session.getMap(\"customers\", WithClassLoader.using(NullImplementation.getClassLoader())); Load Data to a Binary NamedMap or NamedCache Now we can obtain a binary cache to use to load decorated binary values, we can put everything together to load data from the data source, convert it to decorated binaty values, and call putAll . The method below from the AbstractBinaryJdbcPreloadTask class in the example source loads a batch of decorated values into a cache. The method is passed a batch of values to load (in a Map ), and a binary NamedMap to load the batch of data into, <markup lang=\"java\" title=\"AbstractBinaryJdbcPreloadTask.java\" >private void load(Map&lt;K, V&gt; batch, NamedMap&lt;Binary, Binary&gt; namedMap) { BackingMapManagerContext context = namedMap.getService().getBackingMapManager().getContext(); Converter&lt;K, Binary&gt; keyConverter = context.getKeyToInternalConverter(); Converter&lt;V, Binary&gt; valueConverter = context.getValueToInternalConverter(); Map&lt;Binary, Binary&gt; decoratedBatch = new HashMap&lt;&gt;(); for (Map.Entry&lt;K, V&gt; entry : batch.entrySet()) { Binary binary = valueConverter.convert(entry.getValue()); Binary decorated = ExternalizableHelper.decorate(binary, decorationId, Binary.NO_BINARY); decoratedBatch.put(keyConverter.convert(entry.getKey()), decorated); } namedMap.putAll(decoratedBatch); batch.clear(); } First the BackingMapManagerContext is obtained from the NamedMap . This will allow access to the converters to use to serialize the keys and values into Binary values. Obtain the key converter to serialize keys to Binary . Coherence uses different converters to serialize the key and value, because different logic is used internally to decorate a serialized key. If a key is converted to a Binary incorrectly it will not be possible to get the value back out again with something like a get() call. Obtain the converter to use to serialize values to Binary Create a Map to hold the Binary keys and value we will put into the cache, them iterate over the values in the batch. Convert the value to a Binary and add the decoration to it. Put the serialized key and decorated value into the decoratedBatch map After converting all th keys and value to Binary keys and decorated Binary values the map of binaries can be passed to the namedMap.putAll method. As all the data is already serialized, Coherence will send it unchanged to the storage enabled cluster members that own those entries. The SmartCacheStore works around the caveats of a ControllableCacheStore As the decoration on the value controls whether it is written, this method will work with write-behind. There is no need to turn on or off the cache stores If application code updates caches during the preload, those updated values will not be decorated and will be stored by the cache store If a new cluster member joins and becomes the owner of a number of entries, those entries will still have the decoration present and will not be written by the cache store. ",
            "title": "Loading Decorated Binary Values"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " A smart cache store is a solution to the caveats that a controllable cache store has. The code for a smart cache store and for preloading is slightly more complex, but it can be applied to more use cases. Basically a smart cache store can use some sort of flag on the cache value to determine whether an entry should be stored in the data store. This could be a boolean field in the cache value itself, but this means corrupting the application data model with Coherence control data. This is not ideal, and in fact some applications may not actually own the class files that are being used in the cache and cannot add fields. A better way that leaves the cache value unchanged is to use a feature in Coherence that allows decorations to be added to the serialized binary values in a cache. Coherence uses decorations itself for a number of reasons (for example marking an entry as being successfully stored). In this case we can add a simple decoration to indicate whether an entry should be stored or not, the actual value of the decoration does not matter, we can just use the presence of the decoration to indicate that the entry should not be written. The preloader would then load the cache with decorated values, which would not be stored by the smart cache store. Any other entries updated by the application would be stored, even if they were updated while the preload was running. A normal CacheStore does not have access to the binary values in the cache. To be able to do this, the cache store needs to be an implementation of com.tangosol.net.cache.BinaryEntryStore . The SmartCacheStore class in the example source code is an implementation of a BinaryEntryStore . Like the ControllableCacheStore in the example above, the SmartCacheStore wraps a delagate CacheStore so it can be used to make any CacheStore implementation smart. Decorating a Binary A com.tangosol.util.Binary is usually used to hold a serialized binary value (occasionally it may also be a com.tangosol.io.ReadBuffer ). As well as the serialized data, that could have been serialized using any serializer configured in Coherence (Java, POF, json, etc) a Binary can be decorated with other information. Each decoration has an int identifier that is used to add, remove or obtain a specific decoration. Coherence itself uses decorations for a number of functions, so a number of decoration identifiers values are reserved. The identifiers are all stored in constants in com.tangosol.util.ExternalizableHelper class and all have the prefix DECO_ , for example DECO_EXPIRY . There are three identifiers reserved for use by application code, DECO_APP_1 , DECO_APP_2 and DECO_APP_3 which Coherence will not use, so for this example we can use ExternalizableHelper.DECO_APP_1 for the SmartCacheStore decoration. The method used to decorate a Binary is ExternalizableHelper method: <markup lang=\"java\" >public static Binary decorate(Binary bin, int nId, Binary binDecoration) A Binary is decorated with another Binary value. For the SmartCacheStore we do not care what the value of the decoration is, we only check whether it is present or not. Obviously we do not want to use a large Binary decoration, as this will add to the serialized size of the value, the smallest possible Binary is the constant value Binary.NO_BINARY , which is actually zero bytes, but still a Binary . We can therefore decorate a Binary for use in the SmartCacheStore like this: <markup lang=\"java\" >Binary binary = // create the serialized binary value Binary decorated = ExternalizableHelper.decorate(binary, ExternalizableHelper.DECO_APP_1, Binary.NO_BINARY); The SmartCacheStore Implementation The example below shows part of the SmartCacheStore implementation. The store and storeAll methods are shown, but the other BinaryEntryStore are omitted here to make things clearer. <markup lang=\"java\" title=\"SmartCacheStore.java\" >public class SmartCacheStore&lt;K, V&gt; implements BinaryEntryStore&lt;K, V&gt; { private final BinaryEntryStore&lt;K, V&gt; delegate; public SmartCacheStore(CacheStore&lt;K, V&gt; delegate) { this.delegate = new WrapperBinaryEntryStore&lt;&gt;(Objects.requireNonNull(delegate)); } @Override public void store(BinaryEntry&lt;K, V&gt; entry) { if (shouldStore(entry)) { delegate.store(entry); } } @Override public void storeAll(Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entries) { Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entriesToStore = entries.stream() .filter(this::shouldStore) .collect(Collectors.toSet()); if (entriesToStore.size() &gt; 0) { delegate.storeAll(entriesToStore); } } private boolean shouldStore(BinaryEntry&lt;K, V&gt; entry) { return !ExternalizableHelper.isDecorated(entry.getBinaryValue(), ExternalizableHelper.DECO_APP_1); } // other BinaryEntryStore omitted... } The delegate CacheStore passed to the constructor is wrapped in a WrapperBinaryEntryStore to make it look like a BinaryEntryStore that the SmartCacheStore can delegate to. The store method is passed the BinaryEntry to be stored. To check whether the delegate should be called, it calls the shouldStore method. If shouldStore returns true the delegate is called to store the entry. The storeAll method is similar to the store method, but is passed a Set of entries to store. The set is filtered to create a new Set containing only entries that should be stored. If there are any entries to store the delegate is called. The shouldStore method checks to see whether an entry should be stored. The Binary value is obtained from the BinaryEntry and checked to see whether the ExternalizableHelper.DECO_APP_1 decoration is present. Loading Decorated Binary Values Now we have a SmartCacheStore that only stores non-decorated values, how do we load decorated Binary values into a cache? This example only works on a cluster member, because it requires access to the server side cache service that the cache being loaded is using. This will allow the preloader to create serialized Binary keys and values using the correct serilzation method for the cache. Obtain a Binary NamedMap or NamedCache The usual way to obtain a NamedMap (or NamedCache ) from a Coherence Session is to just call session.getMap(cacheName); but the getMap() and getCache() method allow options to be passed in to control the cache returned. One of these is the WithClassLoader option, that takes a ClassLoader . Coherence has a special class loader obtained from com.tangosol.util.NullImplementation.getClassLoader() . If this is used, the cache returned is a binary cache, this means that values passed to methods such as get , put , putAll` etc., must be instances of Binary , i.e. a binary cache gives access to the actual serialized data in the cache. The code below gets the normal \"customers\" cache, with Integer keys and Customer values. <markup lang=\"java\" >NamedMap&lt;Integer, Customer&gt; namedMap = session.getMap(\"customers\"); Whereas the code below gets the same \"customers\" cache but with Binary keys (serializer Integer instances in this example) and Binary values (serialized Customer instances in this example). <markup lang=\"java\" >NamedMap&lt;Binary, Binary&gt; namedMap = session.getMap(\"customers\", WithClassLoader.using(NullImplementation.getClassLoader())); Load Data to a Binary NamedMap or NamedCache Now we can obtain a binary cache to use to load decorated binary values, we can put everything together to load data from the data source, convert it to decorated binaty values, and call putAll . The method below from the AbstractBinaryJdbcPreloadTask class in the example source loads a batch of decorated values into a cache. The method is passed a batch of values to load (in a Map ), and a binary NamedMap to load the batch of data into, <markup lang=\"java\" title=\"AbstractBinaryJdbcPreloadTask.java\" >private void load(Map&lt;K, V&gt; batch, NamedMap&lt;Binary, Binary&gt; namedMap) { BackingMapManagerContext context = namedMap.getService().getBackingMapManager().getContext(); Converter&lt;K, Binary&gt; keyConverter = context.getKeyToInternalConverter(); Converter&lt;V, Binary&gt; valueConverter = context.getValueToInternalConverter(); Map&lt;Binary, Binary&gt; decoratedBatch = new HashMap&lt;&gt;(); for (Map.Entry&lt;K, V&gt; entry : batch.entrySet()) { Binary binary = valueConverter.convert(entry.getValue()); Binary decorated = ExternalizableHelper.decorate(binary, decorationId, Binary.NO_BINARY); decoratedBatch.put(keyConverter.convert(entry.getKey()), decorated); } namedMap.putAll(decoratedBatch); batch.clear(); } First the BackingMapManagerContext is obtained from the NamedMap . This will allow access to the converters to use to serialize the keys and values into Binary values. Obtain the key converter to serialize keys to Binary . Coherence uses different converters to serialize the key and value, because different logic is used internally to decorate a serialized key. If a key is converted to a Binary incorrectly it will not be possible to get the value back out again with something like a get() call. Obtain the converter to use to serialize values to Binary Create a Map to hold the Binary keys and value we will put into the cache, them iterate over the values in the batch. Convert the value to a Binary and add the decoration to it. Put the serialized key and decorated value into the decoratedBatch map After converting all th keys and value to Binary keys and decorated Binary values the map of binaries can be passed to the namedMap.putAll method. As all the data is already serialized, Coherence will send it unchanged to the storage enabled cluster members that own those entries. The SmartCacheStore works around the caveats of a ControllableCacheStore As the decoration on the value controls whether it is written, this method will work with write-behind. There is no need to turn on or off the cache stores If application code updates caches during the preload, those updated values will not be decorated and will be stored by the cache store If a new cluster member joins and becomes the owner of a number of entries, those entries will still have the decoration present and will not be written by the cache store. ",
            "title": "Smart CacheStore"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " This guide has shown a few solutions to the preload use case. Which one is suitable depends on your applications requirements. The example code has been built in such a way that it can be taken as a starting framework for a preloader and controllable/smart cache stores and expanded to suit application use cases. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " As already shown, a basic cache loader can be very simple. Where it gets complicated is when the cache being loaded has a CacheStore configured that writes data to the same data source that the loader is loading from. It should be obvious what the issue is, the data is read from the database, put into the cache, and the re-written back to the database - this is not desirable. The solution to this problem is to have a CacheStore implementation that can determine whether an entry should be written to the database or not. There are a few ways to do this, a controllable CacheStore that can be turned on or off, or a CacheStore that can check a flag on a value to determine whether that value should be stored or not. In this example we will cover both of these options. Controllable CacheStore As the name suggests, a controllable CacheStore can be turned on or off. The CacheStore would be turned off before the preload tasks ran, then turned back on after the preload is complete. There have been a few patterns for controllable cache stores suggested in the past, including an example in the official Coherence documentation where the enable/disable flag is a boolean value stored in another cache. With a little more thought we can see that this is not really a good idea. Consider what a bulk preload is doing, it is loading a very large amount of data into caches, that will then call the CacheStore methods. If the CacheStore needed to look up a distributed cache value on every store call, that would be massively inefficient. Even accessing a cache from inside a CacheStore could be problematic due to making a remote call from a cache service worker thread, which may cause a deadlock. Even introducing near caching or a view cache would not necessarily help, as updates to the flag would be async. Checking the flag that controls the CacheStore needs to be as efficient as possible. For that reason, the example here just uses a simple boolean field in the controller itself. An EntryProcessor is then used to directly set the flag for the controller. How this works will be explained below. The code in this example has a ControllableCacheStore class that implements CacheStore and has a Controller that enables or disables operations. This allows the ControllableCacheStore to be controlled in different ways just by implementing different types of Controller . The ControllableCacheStore also just delegates operations to another CacheStore , it does not do anything itself. The ControllableCacheStore calls the delegate if the controller says it is enabled, otherwise it does nothing. This makes the ControllableCacheStore a simple class that can be reused to make any existing, or new, CacheStore implementation be controllable. A small section of the ControllableCacheStore class is shown below: <markup lang=\"java\" title=\"ControllableCacheStore.java\" >public class ControllableCacheStore&lt;K, V&gt; implements CacheStore&lt;K, V&gt; { private final Controller controller; private final CacheStore&lt;K, V&gt; delegate; public ControllableCacheStore(Controller controller, CacheStore&lt;K, V&gt; delegate) { this.controller = controller; this.delegate = delegate; } @Override public void store(K key, V value) { if (controller.isEnabled()) { delegate.store(key, value); } } @Override public void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) { if (controller.isEnabled()) { delegate.storeAll(mapEntries); } } // other methods omitted ... /** * Implementations of {@link Controller} can * control a {@link ControllableCacheStore}. */ public interface Controller { boolean isEnabled(); } } It should be obvious how the class works. The Controller is an inner interface, and an implementation of this is passed to the constructor, along with the delegate CacheStore . Each method call (only store and storeAll are shown above) calls the controller&#8217;s isEnabled() method to determine whether the delegate should be called. The Controller Implementation In this example, a simple controller is used with a boolean field for enabling or disabling the CacheStore . The example source code contains the SimpleController class shown below: <markup lang=\"java\" title=\"SimpleController.java\" >public class SimpleController implements ControllableCacheStore.Controller { @Override public boolean isEnabled() { return enabled; } public void setEnabled(boolean enabled) { this.enabled = enabled; } } Configuring and Creating the CacheStore For a cache to use a cache store, it needs to be configured in the cache configuration file to use a &lt;read-write-backing-map&gt; , which in turn configures the CacheStore implementation to use. There are a few ways to configure the CacheStore , either using the implementation class name directly, or using a factory class and static factory method. In this example we will use the second approach, this means determining the cache store to use will be done in a factory class rather than in configuration, but this fits our use case better. The &lt;distributed-scheme&gt; used in the example test code is shown below: <markup lang=\"xml\" title=\"controllable-cachestore-cache-config.xml\" > &lt;distributed-scheme&gt; &lt;scheme-name&gt;db-storage&lt;/scheme-name&gt; &lt;service-name&gt;StorageService&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-factory-name&gt; com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory &lt;/class-factory-name&gt; &lt;method-name&gt;createControllableCacheStore&lt;/method-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"jdbc.url\"&gt; jdbc:hsqldb:mem:test &lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; The CacheStore factory class is com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory The static factory method on the CacheStoreFactory class that will be called to create a CacheStore is createControllableCacheStore The createControllableCacheStore has two parameters, which are configured in the &lt;init-params&gt; , the first is the name of the cache. The Coherence configuration macro {cache-name} will pass the name of the cache being created to the factory method. The second parameter is the JDBC URL of the database to load data from, in the example this defaults to the HSQL in-memory test database. The CacheStoreFactory method createControllableCacheStore used in the example is shown below <markup lang=\"java\" title=\"filename.java\" >public static CacheStore createControllableCacheStore(String cacheName, String jdbcURL) throws Exception { CacheStore delegate; switch (cacheName.toLowerCase()) { case \"customers\": delegate = new CustomerCacheStore(jdbcURL); break; case \"orders\": delegate = new OrderCacheStore(jdbcURL); break; default: throw new IllegalArgumentException(\"Cannot create cache store for cache \" + cacheName); } return new ControllableCacheStore&lt;&gt;(new SimpleController(), delegate); } The code does a simple switch on the cache name to determine the actual CacheStore to create. If the cache name is \"customers\", then the CustomerCacheStore is created If the cache name is \"orders\", then the OrderCacheStore is created An exception is thrown for an unknown cache name Finally, a ControllableCacheStore is returned that uses a SimpleController and wraps the delegate CacheStore Now, when application code first requests either the \"customers\" cache or the \"orders\" cache, the cache Coherence will create the cache and call the CacheStoreFactory.createControllableCacheStore method to create the CacheStore . Enabling and Disabling the ControllableCacheStore Now we have caches that are configured to use the ControllableCacheStore with the SimpleController , the next step is to actually be able to enable or disable the controller so that the preload can run. In the SimpleController the setEnabled() method needs to be called to set the controlling boolean flag. For each cache configured to use the ControllableCacheStore there will be an instance of SimpleController in every storage enabled cluster member. A method is needed of calling the setEnabled() method on all these instances, and this can be done with an EntryProcessor . A section of the source for the SetEnabledProcessor is shown below (the methods for serialization have been omitted here, but are in the actual GitHub source). <markup lang=\"java\" title=\"SetEnabledProcessor.java\" >public static class SetEnabledProcessor&lt;K, V&gt; implements InvocableMap.EntryProcessor&lt;K, V, Void&gt;, PortableObject, ExternalizableLite { private boolean enabled; public SetEnabledProcessor() { } public SetEnabledProcessor(boolean enabled) { this.enabled = enabled; } @Override public Void process(InvocableMap.Entry&lt;K, V&gt; entry) { ObservableMap&lt;? extends K, ? extends V&gt; backingMap = entry.asBinaryEntry().getBackingMap(); if (backingMap instanceof ReadWriteBackingMap) { ReadWriteBackingMap.StoreWrapper wrapper = ((ReadWriteBackingMap) backingMap).getCacheStore(); Object o = wrapper.getStore(); if (o instanceof ControllableCacheStore) { ControllableCacheStore.Controller controller = ((ControllableCacheStore) o).getController(); if (controller instanceof SimpleController) { ((SimpleController) controller).setEnabled(enabled); } } } return null; } // PortableObject and ExternalizableLite method are omitted here... } In the process method, the backing map is obtained from the entry . The getBackingMap() is deprecated, mainly as a warning that this is quite a dangerous thing to do if you are not careful. The backing map is the internal structure used by Coherence to hold cache data, and directly manipulating it can have adverse effects. In this case we are not manipulating the backing map, so we are safe. If the cache is configured as with a &lt;read-write-backing-map&gt; then the implementation of the backing map here will be ReadWriteBackingMap . We can obtain the CacheStore being used from the ReadWriteBackingMap API, and check whether it is a ControllableCacheStore . If it is, we can get the Controller being used, and if it is a SimpleController set the flag. Now we have the SetEnabledProcessor we need to execute it so that we guarantee it runs on every storage enabled member. Using something like cache.invokeAll(Filters.always(), new SetEnabledProcessor()) will not work, because this will only execute on members where there are entries, and there are none as we are about to do a preload. One of the things to remember about methods like cache.invokeAll(keySet, new SetEnabledProcessor()) is that the keySet can contain keys that do not need to exist in the cache. As long as the entry processor does not call entry.setValue() the entry it executes against will never exist. Another feature of Coherence is the ability to influence the partition a key belongs to by writing a key class that implements the com.tangosol.net.partition.KeyPartitioningStrategy.PartitionAwareKey interface. Coherence has a built-in implementation of this class called com.tangosol.net.partition.SimplePartitionKey . We can make use of both these features to create a set of keys where we can guarantee we have one key for each partition in a cache. If we use this as the key set in an invokeAll method, we will guarantee to execute the EntryProcessor in every partition, and hence on every storage enable cluster member. The snippet of code below shows how to execute the SetEnabledProcessor to disable the cache stores for a cache. Changing the line new SetEnabledProcessor(false) to new SetEnabledProcessor(true) will instead enable the cache stores. <markup lang=\"java\" title=\"SimpleController.java\" >public static void disableCacheStores(NamedMap&lt;?, ?&gt; namedMap) { CacheService service = namedMap.getService(); int partitionCount = ((DistributedCacheService) service).getPartitionCount(); Set&lt;SimplePartitionKey&gt; keys = new HashSet&lt;&gt;(); for (int i = 0; i &lt; partitionCount; i++) { keys.add(SimplePartitionKey.getPartitionKey(i)); } namedMap.invokeAll(keys, new SetEnabledProcessor(false)); } Obtain the cache service for the NamedMap that has the ControllableCacheSTore to enable The cache service should actually be an instance of DistributedCacheService , from which we can get the partition count. The default is 257, but this could have been changed. Create a Set&lt;SimplePartitionKey&gt; that will hold the keys for the invokeAll In a for loop, create a SimplePartitionKey for every partition, and add it to the keys set The keys set can be used in the invokeAll call to invoke the SetEnabledProcessor on every partition Running the SetEnabledProcessor on every partition means it actually executes more times than it needs to, but this is not a problem, as repeated executions in the same JVM just set the same flag for the enabled value. Now we have a way to enable and disable the ControllableCacheStore , we can execute this code before running the preload, and then re-enable the cache stores after running the preload. ControllableCacheStore Caveats A controllable cache store is reasonably simple, but it will really not work in cases where the cache is configured to use write-behind. With write-behind enabled, if the controllable cache store is turned back on too soon after loading (i.e. within the write delay time) then the data loaded to the cache that is still in the write-behind queue will be written to the database. A controllable cache store is also not going to work in situations where the application could be updating entries in the cache while the preload is still running. If there are a mixture of entries, some needing to be written and some not, the controllable cache store will not be suitable. Another caveat with the SimpleController above, is what happens during failure of a storage member. If a storage member in the cluster fails, that is not an issue, but in environments such as Kubernetes, where that failed member will be replaced automatically, that can be a problem. The new member will join the cluster, caches will be created on it, including the ControllableCacheStore for configured caches. The problem is that the boolean flag in the new member&#8217;s SimpleController will not be set to false , so the new member will start storing entries it receives to the database. Ideally, new members do not join during preload, but this may be out of the developers control. This could require a more complex controller, for example checking an entry in a cache for its initial state, etc. Smart CacheStore A smart cache store is a solution to the caveats that a controllable cache store has. The code for a smart cache store and for preloading is slightly more complex, but it can be applied to more use cases. Basically a smart cache store can use some sort of flag on the cache value to determine whether an entry should be stored in the data store. This could be a boolean field in the cache value itself, but this means corrupting the application data model with Coherence control data. This is not ideal, and in fact some applications may not actually own the class files that are being used in the cache and cannot add fields. A better way that leaves the cache value unchanged is to use a feature in Coherence that allows decorations to be added to the serialized binary values in a cache. Coherence uses decorations itself for a number of reasons (for example marking an entry as being successfully stored). In this case we can add a simple decoration to indicate whether an entry should be stored or not, the actual value of the decoration does not matter, we can just use the presence of the decoration to indicate that the entry should not be written. The preloader would then load the cache with decorated values, which would not be stored by the smart cache store. Any other entries updated by the application would be stored, even if they were updated while the preload was running. A normal CacheStore does not have access to the binary values in the cache. To be able to do this, the cache store needs to be an implementation of com.tangosol.net.cache.BinaryEntryStore . The SmartCacheStore class in the example source code is an implementation of a BinaryEntryStore . Like the ControllableCacheStore in the example above, the SmartCacheStore wraps a delagate CacheStore so it can be used to make any CacheStore implementation smart. Decorating a Binary A com.tangosol.util.Binary is usually used to hold a serialized binary value (occasionally it may also be a com.tangosol.io.ReadBuffer ). As well as the serialized data, that could have been serialized using any serializer configured in Coherence (Java, POF, json, etc) a Binary can be decorated with other information. Each decoration has an int identifier that is used to add, remove or obtain a specific decoration. Coherence itself uses decorations for a number of functions, so a number of decoration identifiers values are reserved. The identifiers are all stored in constants in com.tangosol.util.ExternalizableHelper class and all have the prefix DECO_ , for example DECO_EXPIRY . There are three identifiers reserved for use by application code, DECO_APP_1 , DECO_APP_2 and DECO_APP_3 which Coherence will not use, so for this example we can use ExternalizableHelper.DECO_APP_1 for the SmartCacheStore decoration. The method used to decorate a Binary is ExternalizableHelper method: <markup lang=\"java\" >public static Binary decorate(Binary bin, int nId, Binary binDecoration) A Binary is decorated with another Binary value. For the SmartCacheStore we do not care what the value of the decoration is, we only check whether it is present or not. Obviously we do not want to use a large Binary decoration, as this will add to the serialized size of the value, the smallest possible Binary is the constant value Binary.NO_BINARY , which is actually zero bytes, but still a Binary . We can therefore decorate a Binary for use in the SmartCacheStore like this: <markup lang=\"java\" >Binary binary = // create the serialized binary value Binary decorated = ExternalizableHelper.decorate(binary, ExternalizableHelper.DECO_APP_1, Binary.NO_BINARY); The SmartCacheStore Implementation The example below shows part of the SmartCacheStore implementation. The store and storeAll methods are shown, but the other BinaryEntryStore are omitted here to make things clearer. <markup lang=\"java\" title=\"SmartCacheStore.java\" >public class SmartCacheStore&lt;K, V&gt; implements BinaryEntryStore&lt;K, V&gt; { private final BinaryEntryStore&lt;K, V&gt; delegate; public SmartCacheStore(CacheStore&lt;K, V&gt; delegate) { this.delegate = new WrapperBinaryEntryStore&lt;&gt;(Objects.requireNonNull(delegate)); } @Override public void store(BinaryEntry&lt;K, V&gt; entry) { if (shouldStore(entry)) { delegate.store(entry); } } @Override public void storeAll(Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entries) { Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entriesToStore = entries.stream() .filter(this::shouldStore) .collect(Collectors.toSet()); if (entriesToStore.size() &gt; 0) { delegate.storeAll(entriesToStore); } } private boolean shouldStore(BinaryEntry&lt;K, V&gt; entry) { return !ExternalizableHelper.isDecorated(entry.getBinaryValue(), ExternalizableHelper.DECO_APP_1); } // other BinaryEntryStore omitted... } The delegate CacheStore passed to the constructor is wrapped in a WrapperBinaryEntryStore to make it look like a BinaryEntryStore that the SmartCacheStore can delegate to. The store method is passed the BinaryEntry to be stored. To check whether the delegate should be called, it calls the shouldStore method. If shouldStore returns true the delegate is called to store the entry. The storeAll method is similar to the store method, but is passed a Set of entries to store. The set is filtered to create a new Set containing only entries that should be stored. If there are any entries to store the delegate is called. The shouldStore method checks to see whether an entry should be stored. The Binary value is obtained from the BinaryEntry and checked to see whether the ExternalizableHelper.DECO_APP_1 decoration is present. Loading Decorated Binary Values Now we have a SmartCacheStore that only stores non-decorated values, how do we load decorated Binary values into a cache? This example only works on a cluster member, because it requires access to the server side cache service that the cache being loaded is using. This will allow the preloader to create serialized Binary keys and values using the correct serilzation method for the cache. Obtain a Binary NamedMap or NamedCache The usual way to obtain a NamedMap (or NamedCache ) from a Coherence Session is to just call session.getMap(cacheName); but the getMap() and getCache() method allow options to be passed in to control the cache returned. One of these is the WithClassLoader option, that takes a ClassLoader . Coherence has a special class loader obtained from com.tangosol.util.NullImplementation.getClassLoader() . If this is used, the cache returned is a binary cache, this means that values passed to methods such as get , put , putAll` etc., must be instances of Binary , i.e. a binary cache gives access to the actual serialized data in the cache. The code below gets the normal \"customers\" cache, with Integer keys and Customer values. <markup lang=\"java\" >NamedMap&lt;Integer, Customer&gt; namedMap = session.getMap(\"customers\"); Whereas the code below gets the same \"customers\" cache but with Binary keys (serializer Integer instances in this example) and Binary values (serialized Customer instances in this example). <markup lang=\"java\" >NamedMap&lt;Binary, Binary&gt; namedMap = session.getMap(\"customers\", WithClassLoader.using(NullImplementation.getClassLoader())); Load Data to a Binary NamedMap or NamedCache Now we can obtain a binary cache to use to load decorated binary values, we can put everything together to load data from the data source, convert it to decorated binaty values, and call putAll . The method below from the AbstractBinaryJdbcPreloadTask class in the example source loads a batch of decorated values into a cache. The method is passed a batch of values to load (in a Map ), and a binary NamedMap to load the batch of data into, <markup lang=\"java\" title=\"AbstractBinaryJdbcPreloadTask.java\" >private void load(Map&lt;K, V&gt; batch, NamedMap&lt;Binary, Binary&gt; namedMap) { BackingMapManagerContext context = namedMap.getService().getBackingMapManager().getContext(); Converter&lt;K, Binary&gt; keyConverter = context.getKeyToInternalConverter(); Converter&lt;V, Binary&gt; valueConverter = context.getValueToInternalConverter(); Map&lt;Binary, Binary&gt; decoratedBatch = new HashMap&lt;&gt;(); for (Map.Entry&lt;K, V&gt; entry : batch.entrySet()) { Binary binary = valueConverter.convert(entry.getValue()); Binary decorated = ExternalizableHelper.decorate(binary, decorationId, Binary.NO_BINARY); decoratedBatch.put(keyConverter.convert(entry.getKey()), decorated); } namedMap.putAll(decoratedBatch); batch.clear(); } First the BackingMapManagerContext is obtained from the NamedMap . This will allow access to the converters to use to serialize the keys and values into Binary values. Obtain the key converter to serialize keys to Binary . Coherence uses different converters to serialize the key and value, because different logic is used internally to decorate a serialized key. If a key is converted to a Binary incorrectly it will not be possible to get the value back out again with something like a get() call. Obtain the converter to use to serialize values to Binary Create a Map to hold the Binary keys and value we will put into the cache, them iterate over the values in the batch. Convert the value to a Binary and add the decoration to it. Put the serialized key and decorated value into the decoratedBatch map After converting all th keys and value to Binary keys and decorated Binary values the map of binaries can be passed to the namedMap.putAll method. As all the data is already serialized, Coherence will send it unchanged to the storage enabled cluster members that own those entries. The SmartCacheStore works around the caveats of a ControllableCacheStore As the decoration on the value controls whether it is written, this method will work with write-behind. There is no need to turn on or off the cache stores If application code updates caches during the preload, those updated values will not be decorated and will be stored by the cache store If a new cluster member joins and becomes the owner of a number of entries, those entries will still have the decoration present and will not be written by the cache store. Summary This guide has shown a few solutions to the preload use case. Which one is suitable depends on your applications requirements. The example code has been built in such a way that it can be taken as a starting framework for a preloader and controllable/smart cache stores and expanded to suit application use cases. ",
            "title": "CacheStore Complications"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " What You Will Build What You Need Building The Example Code Example Data Model Why use Coherence*Extend? Connect via the Name Service Using Proxy Load Balancing Setting Host and Port Explicitly Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " The example code is written as a set of JUnit integration tests, showing how you can use Coherence*Extend. For our test cases we will also use Oracle Bedrock to start server instances of Oracle Coherence for testing purposes. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " Although recommended, it may not always be possible that your application can be directly part of a Coherence cluster using the Tangosol Cluster Management Protocol (TCMP). For example, your application may be located in a different network, you need to access Oracle Coherence from desktop applications, or you need to use languages other than Java, e.g. C++ or .NET. Another alternative is to use the gRPC integration . ",
            "title": "Why use Coherence*Extend?"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " Running the test should be fairly uneventful. If successful, you will see Bedrock starting up the Coherence server with 1 instance followed by the client starting up and connecting. Let&#8217;s do a quick test of the request-timeout and see what happens when the Coherence Server is not available. Comment out the setup() method, and re-run the test. After the specified request-timeout of 5 seconds, you should get a stacktrace with an exception similar to the following: <markup lang=\"bash\" >com.tangosol.net.messaging.ConnectionException: Unable to locate cluster 'myCluster' while looking for its ProxyService 'MyCountryExtendService' In the next section we will see how we can use multiple Coherence servers, and thus taking advantage of proxy load-balancing. ",
            "title": "Run the Test"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " When connecting to a Coherence Cluster via Coherence*Extend, we recommend the use of the Name Service. The use of the name service simplifies port management as the name service will look up the actual Coherence*Extend ports. That way Coherence*Extend ports can be ephemeral. For this example, let&#8217;s start with the Server Cache Configuration file at src/main/resources/name-service/server-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; In the &lt;cache-mapping&gt; element, we state that the countries cache maps to the country-scheme The country-scheme then declares the &lt;proxy-scheme&gt; with the name MyCountryExtendService The MyCountryExtendService will start automatically The MyCountryExtendService will be registered with the default name service. If you wanted to customize that behavior, you would need to provide an &lt;acceptor-config&gt; element. See the load-balancing use-case below for details. We will also create a corresponding Client Cache Configuration file at src/main/resources/name-service/client-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The cache mapping for the client will look similar to the server one, but we name the scheme remote-country-scheme The client specifies a &lt;remote-cache-scheme&gt; element The service name MyCountryExtendService must match the name we use in the server cache configuration file We also define a request-timeout of 5 seconds. This means that if a connection cannot be established within that time, an exception is raised The client will be using the default name service port of 7574 to lookup the proxy endpoint for the MyCountryExtendService . You could customize that configuration by providing an &lt;initiator-config&gt; element. See the firewall example below for details. In the test case itself, we will use Oracle Bedrock to boostrap the Coherence server using the Server Cache Configuration file: <markup lang=\"java\" >static CoherenceClusterMember server; @BeforeAll static void setup() { final LocalPlatform platform = LocalPlatform.get(); // Start the Coherence server server = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"name-service/server-coherence-cache-config.xml\"), IPv4Preferred.yes(), SystemProperty.of(\"coherence.wka\", \"127.0.0.1\"), ClusterName.of(\"myCluster\"), DisplayName.of(\"server\")); // Wait for Coherence to start Eventually.assertDeferred(() -&gt; server.invoke( new IsServiceRunning(\"MyCountryExtendService\")), is(true)); } Specify the server cache configuration file Give the Server Cluster an explicit name myCluster Make sure that we wait until the MyCountryExtendService proxy service is available Then we configure and start the Coherence client. <markup lang=\"java\" >@Test void testNameServiceUseCase() { System.setProperty(\"coherence.tcmp.enabled\", \"false\"); System.setProperty(\"coherence.cluster\", \"myCluster\"); System.setProperty(\"coherence.wka\", \"127.0.0.1\"); CoherenceHelper.startCoherenceClient( CoherenceHelper.NAME_SERVICE_INSTANCE_NAME, \"name-service/client-coherence-cache-config.xml\"); NamedCache&lt;String, Country&gt; countries = CoherenceHelper.getMap(CoherenceHelper.NAME_SERVICE_INSTANCE_NAME,\"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); } Disable TCMP to ensure that we only connect via Coherence*Extend Set the cluster name of the client to the same name as the server Specify the client cache configuration file Get the NamedCache and add a new country Important When configuring your Coherence*Extend client, it is important that your client&#8217;s Cluster Name match the name of the Coherence Server Cluster. Tip Java-based clients located on the same network as the Coherence server should disable TCMP communication in order to ensure that the client connect to clustered services exclusively using extend proxies. This can be achieved by setting System property coherence.tcmp.enabled to false . Please see the reference documentation for more detailed information. Run the Test Running the test should be fairly uneventful. If successful, you will see Bedrock starting up the Coherence server with 1 instance followed by the client starting up and connecting. Let&#8217;s do a quick test of the request-timeout and see what happens when the Coherence Server is not available. Comment out the setup() method, and re-run the test. After the specified request-timeout of 5 seconds, you should get a stacktrace with an exception similar to the following: <markup lang=\"bash\" >com.tangosol.net.messaging.ConnectionException: Unable to locate cluster 'myCluster' while looking for its ProxyService 'MyCountryExtendService' In the next section we will see how we can use multiple Coherence servers, and thus taking advantage of proxy load-balancing. ",
            "title": "Connect via the Name Service"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " When running the test you should notice the logging to the console from our CustomProxyServiceLoadBalancer : <markup lang=\"bash\" >[server-1:out:44488] 2: Local Member Id: 1 (Total # of Members: 4) - Connection Count: 1 [server-3:out:44488] 2: Local Member Id: 4 (Total # of Members: 4) - Connection Count: 1 [server-2:out:44488] 2: Local Member Id: 2 (Total # of Members: 4) - Connection Count: 1 [server-4:out:44488] 2: Local Member Id: 3 (Total # of Members: 4) - Connection Count: 0 As we have 4 Cluster Servers but only 3 clients, 1 Cluster Server will have 0 client connections, while each other server has 1 client connection each. ",
            "title": "Run the Test"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " When you have multiple Coherence servers that you are connecting to via Coherence*Extend, connection load-balancing is automatically applied. The default load-balancing behavior is based on the load of each Coherence server member and client connections are evenly spread across the Coherence cluster. The default load balance algorithm is called ‘proxy’, which if you were to explicitly configure that setting, your Server Cache Configuration file would add the following &lt;proxy-scheme&gt; : <markup lang=\"xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;load-balancer&gt;proxy&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; Under the covers, this configuration will use the class DefaultProxyServiceLoadBalancer . Note The other build-in load-balancing option is client for client-based load-balancing. We will use that option in the firewall use-case below. You can, however, customize the load-balancing logic depending on your needs by providing an implementation of the ProxyServiceLoadBalancer interface. As mentioned above, Coherence&#8217;s default implementation is the DefaultProxyServiceLoadBalancer . For our test-case, lets simply customize it by adding some more logging: <markup lang=\"java\" >public class CustomProxyServiceLoadBalancer extends DefaultProxyServiceLoadBalancer { @Override public int compare(ProxyServiceLoad load1, ProxyServiceLoad load2) { int result = super.compare(load1, load2); System.out.println(String.format(\"Local Member Id: %s (Total # of Members: %s) - Connection Count: %s\", super.getLocalMember().getId(), super.getMemberList(null).size(), load1.getConnectionCount())); return result; } } The Server Cache Configuration file at src/main/resources/load-balancing/server-coherence-cache-config.xml is almost the same compared to the name-service example, but we add a &lt;load-balancer&gt; element. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;load-balancer&gt; &lt;instance&gt; &lt;class-name&gt;com.oracle.coherence.guides.extend.loadbalancer.CustomProxyServiceLoadBalancer&lt;/class-name&gt; &lt;/instance&gt; &lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The MyCountryExtendService also specifies a &lt;load-balancer&gt; element The load-balancer uses the customized CustomProxyServiceLoadBalancer The corresponding Client Cache Configuration file at src/main/resources/load-balancing/client-coherence-cache-config.xml will be identical to the Client Cache Configuration files used for the name-service example. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; In this example, we will beef up the usage of Oracle Bedrock quite a bit. In order to demonstrate the load-balancer, we will create a Coherence Cluster with 4 nodes (members) and 3 Coherence*Extend clients that connect to those members. <markup lang=\"java\" >@Test void testCoherenceExtendConnection() throws InterruptedException { LocalPlatform platform = LocalPlatform.get(); int numberOfServers = 4; int numberOfClients = 3; List&lt;CoherenceClusterMember&gt; servers = new ArrayList&lt;&gt;(numberOfServers); List&lt;CoherenceClusterMember&gt; clients = new ArrayList&lt;&gt;(numberOfClients); try { for (int i = 1; i &lt;= numberOfServers; i++) { CoherenceClusterMember server = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"load-balancing/server-coherence-cache-config.xml\"), ClassName.of(Coherence.class), LocalHost.only(), Logging.atInfo(), IPv4Preferred.yes(), ClusterName.of(\"myCluster\"), RoleName.of(\"server\"), SystemProperty.of(\"coherence.log.level\", \"5\"), DisplayName.of(\"server-\" + i)); servers.add(server); } for (CoherenceClusterMember server : servers) { Eventually.assertDeferred(() -&gt; server.invoke( new IsServiceRunning(\"MyCountryExtendService\")), is(true)); assertThat(server.getExtendConnectionCount(\"MyCountryExtendService\"), is(0)); } for (int i = 1; i &lt;= numberOfClients; i++) { CoherenceClusterMember client = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"load-balancing/client-coherence-cache-config.xml\"), ClassName.of(Coherence.class), LocalHost.only(), Logging.atInfo(), IPv4Preferred.yes(), SystemProperty.of(\"coherence.client\", \"remote\"), SystemProperty.of(\"coherence.tcmp.enabled\", \"false\"), SystemProperty.of(\"coherence.log.level\", \"5\"), ClusterName.of(\"myCluster\"), RoleName.of(\"client\"), DisplayName.of(\"client-\" + i)); clients.add(client); } for (CoherenceClusterMember client : clients) { Eventually.assertDeferred(client::isCoherenceRunning, is(true)); client.invoke(new Connect()); } int clientCount = servers.stream() .map(server -&gt; server.getExtendConnectionCount(\"MyCountryExtendService\")) .reduce(0, Integer::sum); assertThat(clientCount, is(3)); TimeUnit.MILLISECONDS.sleep(20000); } finally { for (CoherenceClusterMember client : clients) { client.close(); } for (CoherenceClusterMember server : servers) { server.close(); } } } First we specify the desired number of Coherence servers, 4 in this case We also need 3 Coherence*Extend clients In a loop we create the Coherence servers using the server cache configuration file For each server we make sure it is running None of the servers should have a client connected to them, yet Next we start all the clients using the client cache configuration file We also make sure that all clients are running Once running, we invoke a task on the client that establishes the Coherence*Extend connection. See the source code snippet below Let&#8217;s introspect all the started servers. For each of them, we get the Coherence*Extend connection count for the MyCountryExtendService and sum the result The client connection count should be 3 Let&#8217;s wait for 20 seconds, so you can observe the logging activity of our custom load-balancer As mentioned above, we execute a task for each Coherence*Extend client, to establish the actual connection. In Bedrock, we can submit a RemoteCallable to achieve this: <markup lang=\"java\" >public static class Connect implements RemoteCallable&lt;UUID&gt; { @Override public UUID call() { Session session = Coherence.getInstance().getSession(); NamedCache&lt;Object, Object&gt; cache = session.getCache(\"countries\"); Member member = cache.getCacheService().getInfo().getServiceMember(0); return member.getUuid(); } } Our class implements Bedrock&#8217;s RemoteCallable interface Get a Coherence session Retrieve the countries cache from the session Run the Test When running the test you should notice the logging to the console from our CustomProxyServiceLoadBalancer : <markup lang=\"bash\" >[server-1:out:44488] 2: Local Member Id: 1 (Total # of Members: 4) - Connection Count: 1 [server-3:out:44488] 2: Local Member Id: 4 (Total # of Members: 4) - Connection Count: 1 [server-2:out:44488] 2: Local Member Id: 2 (Total # of Members: 4) - Connection Count: 1 [server-4:out:44488] 2: Local Member Id: 3 (Total # of Members: 4) - Connection Count: 0 As we have 4 Cluster Servers but only 3 clients, 1 Cluster Server will have 0 client connections, while each other server has 1 client connection each. ",
            "title": "Using Proxy Load Balancing"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " The client should be able to connect the server on the explicitly defined host and port. ",
            "title": "Run the Test"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " Generally we recommend using the name service to connect to Coherence, but you may have specific firewall constraints. In that case, you can configure the Coherence server to listen to a specific address and port instead. The Server Cache Configuration file at src/main/resources/firewall/server-coherence-cache-config.xml will look almost identical to the example using the name service. However, here we add an &lt;acceptor-config&gt; XML element. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;address-provider&gt; &lt;local-address&gt; &lt;address&gt;127.0.0.1&lt;/address&gt; &lt;port&gt;7077&lt;/port&gt; &lt;/local-address&gt; &lt;/address-provider&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;load-balancer&gt;client&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Add a &lt;acceptor-config&gt; element Define an explicit Coherence*Extend host address, in this case 127.0.0.1 Define the port 7077 on which we will be listening for Coherence*Extend clients We need to set load-balancing to client We will also create a corresponding Client Cache Configuration file at src/main/resources/firewall/client-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;127.0.0.1&lt;/address&gt; &lt;port&gt;7077&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Define the &lt;initiator-config&gt; element Specify the Coherence*Extend server address and port under the &lt;remote-addresses&gt; element The test case itself is identical to the name service test above: <markup lang=\"java\" >@Test void testFirewallUseCase() { System.setProperty(\"coherence.tcmp.enabled\", \"false\"); System.setProperty(\"coherence.cluster\", \"myCluster\"); System.setProperty(\"coherence.wka\", \"127.0.0.1\"); CoherenceHelper.startCoherenceClient( CoherenceHelper.FIREWALL_INSTANCE_NAME, \"firewall/client-coherence-cache-config.xml\"); NamedCache&lt;String, Country&gt; countries = CoherenceHelper.getMap(CoherenceHelper.FIREWALL_INSTANCE_NAME, \"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); } Run the Test The client should be able to connect the server on the explicitly defined host and port. ",
            "title": "Setting Specific Host and Port"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " In this guide, we gave a few deeper examples of how to set up Coherence*Extend to connect clients to a Coherence Cluster. As part of the Coherence reference documentation, we provide an entire guide on Developing Remote Clients for Oracle Coherence . Part I of that guide provides not only an introduction to Coherence*Extend but also covers advanced topics as well as best practices. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " Bootstrap Coherence Run Coherence as an Extend Client Configure an Extend Client Introduction to Coherence*Extend Configuring Extend Clients Advanced Extend Configuration Best Practices for Coherence*Extend ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " In the previous guide Bootstrap Coherence we briefly talked about connecting to a Coherence Cluster via a Coherence*Extend client using the default cache configuration file. This guide will go a bit deeper in regard to using Coherence*Extend and cover the following use-cases: Connect using the name service using a custom cache configuration file Demonstrate Proxy load balancing Setting specific host &amp; port (Firewall use-case) In all 3 use-cases we will use custom cache configuration files. Table of Contents What You Will Build What You Need Building The Example Code Example Data Model Why use Coherence*Extend? Connect via the Name Service Using Proxy Load Balancing Setting Host and Port Explicitly Summary See Also What You Will Build The example code is written as a set of JUnit integration tests, showing how you can use Coherence*Extend. For our test cases we will also use Oracle Bedrock to start server instances of Oracle Coherence for testing purposes. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Example Data Model The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . Why use Coherence*Extend? Although recommended, it may not always be possible that your application can be directly part of a Coherence cluster using the Tangosol Cluster Management Protocol (TCMP). For example, your application may be located in a different network, you need to access Oracle Coherence from desktop applications, or you need to use languages other than Java, e.g. C++ or .NET. Another alternative is to use the gRPC integration . Connect via the Name Service When connecting to a Coherence Cluster via Coherence*Extend, we recommend the use of the Name Service. The use of the name service simplifies port management as the name service will look up the actual Coherence*Extend ports. That way Coherence*Extend ports can be ephemeral. For this example, let&#8217;s start with the Server Cache Configuration file at src/main/resources/name-service/server-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; In the &lt;cache-mapping&gt; element, we state that the countries cache maps to the country-scheme The country-scheme then declares the &lt;proxy-scheme&gt; with the name MyCountryExtendService The MyCountryExtendService will start automatically The MyCountryExtendService will be registered with the default name service. If you wanted to customize that behavior, you would need to provide an &lt;acceptor-config&gt; element. See the load-balancing use-case below for details. We will also create a corresponding Client Cache Configuration file at src/main/resources/name-service/client-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The cache mapping for the client will look similar to the server one, but we name the scheme remote-country-scheme The client specifies a &lt;remote-cache-scheme&gt; element The service name MyCountryExtendService must match the name we use in the server cache configuration file We also define a request-timeout of 5 seconds. This means that if a connection cannot be established within that time, an exception is raised The client will be using the default name service port of 7574 to lookup the proxy endpoint for the MyCountryExtendService . You could customize that configuration by providing an &lt;initiator-config&gt; element. See the firewall example below for details. In the test case itself, we will use Oracle Bedrock to boostrap the Coherence server using the Server Cache Configuration file: <markup lang=\"java\" >static CoherenceClusterMember server; @BeforeAll static void setup() { final LocalPlatform platform = LocalPlatform.get(); // Start the Coherence server server = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"name-service/server-coherence-cache-config.xml\"), IPv4Preferred.yes(), SystemProperty.of(\"coherence.wka\", \"127.0.0.1\"), ClusterName.of(\"myCluster\"), DisplayName.of(\"server\")); // Wait for Coherence to start Eventually.assertDeferred(() -&gt; server.invoke( new IsServiceRunning(\"MyCountryExtendService\")), is(true)); } Specify the server cache configuration file Give the Server Cluster an explicit name myCluster Make sure that we wait until the MyCountryExtendService proxy service is available Then we configure and start the Coherence client. <markup lang=\"java\" >@Test void testNameServiceUseCase() { System.setProperty(\"coherence.tcmp.enabled\", \"false\"); System.setProperty(\"coherence.cluster\", \"myCluster\"); System.setProperty(\"coherence.wka\", \"127.0.0.1\"); CoherenceHelper.startCoherenceClient( CoherenceHelper.NAME_SERVICE_INSTANCE_NAME, \"name-service/client-coherence-cache-config.xml\"); NamedCache&lt;String, Country&gt; countries = CoherenceHelper.getMap(CoherenceHelper.NAME_SERVICE_INSTANCE_NAME,\"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); } Disable TCMP to ensure that we only connect via Coherence*Extend Set the cluster name of the client to the same name as the server Specify the client cache configuration file Get the NamedCache and add a new country Important When configuring your Coherence*Extend client, it is important that your client&#8217;s Cluster Name match the name of the Coherence Server Cluster. Tip Java-based clients located on the same network as the Coherence server should disable TCMP communication in order to ensure that the client connect to clustered services exclusively using extend proxies. This can be achieved by setting System property coherence.tcmp.enabled to false . Please see the reference documentation for more detailed information. Run the Test Running the test should be fairly uneventful. If successful, you will see Bedrock starting up the Coherence server with 1 instance followed by the client starting up and connecting. Let&#8217;s do a quick test of the request-timeout and see what happens when the Coherence Server is not available. Comment out the setup() method, and re-run the test. After the specified request-timeout of 5 seconds, you should get a stacktrace with an exception similar to the following: <markup lang=\"bash\" >com.tangosol.net.messaging.ConnectionException: Unable to locate cluster 'myCluster' while looking for its ProxyService 'MyCountryExtendService' In the next section we will see how we can use multiple Coherence servers, and thus taking advantage of proxy load-balancing. Using Proxy Load Balancing When you have multiple Coherence servers that you are connecting to via Coherence*Extend, connection load-balancing is automatically applied. The default load-balancing behavior is based on the load of each Coherence server member and client connections are evenly spread across the Coherence cluster. The default load balance algorithm is called ‘proxy’, which if you were to explicitly configure that setting, your Server Cache Configuration file would add the following &lt;proxy-scheme&gt; : <markup lang=\"xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;load-balancer&gt;proxy&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; Under the covers, this configuration will use the class DefaultProxyServiceLoadBalancer . Note The other build-in load-balancing option is client for client-based load-balancing. We will use that option in the firewall use-case below. You can, however, customize the load-balancing logic depending on your needs by providing an implementation of the ProxyServiceLoadBalancer interface. As mentioned above, Coherence&#8217;s default implementation is the DefaultProxyServiceLoadBalancer . For our test-case, lets simply customize it by adding some more logging: <markup lang=\"java\" >public class CustomProxyServiceLoadBalancer extends DefaultProxyServiceLoadBalancer { @Override public int compare(ProxyServiceLoad load1, ProxyServiceLoad load2) { int result = super.compare(load1, load2); System.out.println(String.format(\"Local Member Id: %s (Total # of Members: %s) - Connection Count: %s\", super.getLocalMember().getId(), super.getMemberList(null).size(), load1.getConnectionCount())); return result; } } The Server Cache Configuration file at src/main/resources/load-balancing/server-coherence-cache-config.xml is almost the same compared to the name-service example, but we add a &lt;load-balancer&gt; element. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;load-balancer&gt; &lt;instance&gt; &lt;class-name&gt;com.oracle.coherence.guides.extend.loadbalancer.CustomProxyServiceLoadBalancer&lt;/class-name&gt; &lt;/instance&gt; &lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The MyCountryExtendService also specifies a &lt;load-balancer&gt; element The load-balancer uses the customized CustomProxyServiceLoadBalancer The corresponding Client Cache Configuration file at src/main/resources/load-balancing/client-coherence-cache-config.xml will be identical to the Client Cache Configuration files used for the name-service example. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; In this example, we will beef up the usage of Oracle Bedrock quite a bit. In order to demonstrate the load-balancer, we will create a Coherence Cluster with 4 nodes (members) and 3 Coherence*Extend clients that connect to those members. <markup lang=\"java\" >@Test void testCoherenceExtendConnection() throws InterruptedException { LocalPlatform platform = LocalPlatform.get(); int numberOfServers = 4; int numberOfClients = 3; List&lt;CoherenceClusterMember&gt; servers = new ArrayList&lt;&gt;(numberOfServers); List&lt;CoherenceClusterMember&gt; clients = new ArrayList&lt;&gt;(numberOfClients); try { for (int i = 1; i &lt;= numberOfServers; i++) { CoherenceClusterMember server = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"load-balancing/server-coherence-cache-config.xml\"), ClassName.of(Coherence.class), LocalHost.only(), Logging.atInfo(), IPv4Preferred.yes(), ClusterName.of(\"myCluster\"), RoleName.of(\"server\"), SystemProperty.of(\"coherence.log.level\", \"5\"), DisplayName.of(\"server-\" + i)); servers.add(server); } for (CoherenceClusterMember server : servers) { Eventually.assertDeferred(() -&gt; server.invoke( new IsServiceRunning(\"MyCountryExtendService\")), is(true)); assertThat(server.getExtendConnectionCount(\"MyCountryExtendService\"), is(0)); } for (int i = 1; i &lt;= numberOfClients; i++) { CoherenceClusterMember client = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"load-balancing/client-coherence-cache-config.xml\"), ClassName.of(Coherence.class), LocalHost.only(), Logging.atInfo(), IPv4Preferred.yes(), SystemProperty.of(\"coherence.client\", \"remote\"), SystemProperty.of(\"coherence.tcmp.enabled\", \"false\"), SystemProperty.of(\"coherence.log.level\", \"5\"), ClusterName.of(\"myCluster\"), RoleName.of(\"client\"), DisplayName.of(\"client-\" + i)); clients.add(client); } for (CoherenceClusterMember client : clients) { Eventually.assertDeferred(client::isCoherenceRunning, is(true)); client.invoke(new Connect()); } int clientCount = servers.stream() .map(server -&gt; server.getExtendConnectionCount(\"MyCountryExtendService\")) .reduce(0, Integer::sum); assertThat(clientCount, is(3)); TimeUnit.MILLISECONDS.sleep(20000); } finally { for (CoherenceClusterMember client : clients) { client.close(); } for (CoherenceClusterMember server : servers) { server.close(); } } } First we specify the desired number of Coherence servers, 4 in this case We also need 3 Coherence*Extend clients In a loop we create the Coherence servers using the server cache configuration file For each server we make sure it is running None of the servers should have a client connected to them, yet Next we start all the clients using the client cache configuration file We also make sure that all clients are running Once running, we invoke a task on the client that establishes the Coherence*Extend connection. See the source code snippet below Let&#8217;s introspect all the started servers. For each of them, we get the Coherence*Extend connection count for the MyCountryExtendService and sum the result The client connection count should be 3 Let&#8217;s wait for 20 seconds, so you can observe the logging activity of our custom load-balancer As mentioned above, we execute a task for each Coherence*Extend client, to establish the actual connection. In Bedrock, we can submit a RemoteCallable to achieve this: <markup lang=\"java\" >public static class Connect implements RemoteCallable&lt;UUID&gt; { @Override public UUID call() { Session session = Coherence.getInstance().getSession(); NamedCache&lt;Object, Object&gt; cache = session.getCache(\"countries\"); Member member = cache.getCacheService().getInfo().getServiceMember(0); return member.getUuid(); } } Our class implements Bedrock&#8217;s RemoteCallable interface Get a Coherence session Retrieve the countries cache from the session Run the Test When running the test you should notice the logging to the console from our CustomProxyServiceLoadBalancer : <markup lang=\"bash\" >[server-1:out:44488] 2: Local Member Id: 1 (Total # of Members: 4) - Connection Count: 1 [server-3:out:44488] 2: Local Member Id: 4 (Total # of Members: 4) - Connection Count: 1 [server-2:out:44488] 2: Local Member Id: 2 (Total # of Members: 4) - Connection Count: 1 [server-4:out:44488] 2: Local Member Id: 3 (Total # of Members: 4) - Connection Count: 0 As we have 4 Cluster Servers but only 3 clients, 1 Cluster Server will have 0 client connections, while each other server has 1 client connection each. Setting Specific Host and Port Generally we recommend using the name service to connect to Coherence, but you may have specific firewall constraints. In that case, you can configure the Coherence server to listen to a specific address and port instead. The Server Cache Configuration file at src/main/resources/firewall/server-coherence-cache-config.xml will look almost identical to the example using the name service. However, here we add an &lt;acceptor-config&gt; XML element. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;address-provider&gt; &lt;local-address&gt; &lt;address&gt;127.0.0.1&lt;/address&gt; &lt;port&gt;7077&lt;/port&gt; &lt;/local-address&gt; &lt;/address-provider&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;load-balancer&gt;client&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Add a &lt;acceptor-config&gt; element Define an explicit Coherence*Extend host address, in this case 127.0.0.1 Define the port 7077 on which we will be listening for Coherence*Extend clients We need to set load-balancing to client We will also create a corresponding Client Cache Configuration file at src/main/resources/firewall/client-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;127.0.0.1&lt;/address&gt; &lt;port&gt;7077&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Define the &lt;initiator-config&gt; element Specify the Coherence*Extend server address and port under the &lt;remote-addresses&gt; element The test case itself is identical to the name service test above: <markup lang=\"java\" >@Test void testFirewallUseCase() { System.setProperty(\"coherence.tcmp.enabled\", \"false\"); System.setProperty(\"coherence.cluster\", \"myCluster\"); System.setProperty(\"coherence.wka\", \"127.0.0.1\"); CoherenceHelper.startCoherenceClient( CoherenceHelper.FIREWALL_INSTANCE_NAME, \"firewall/client-coherence-cache-config.xml\"); NamedCache&lt;String, Country&gt; countries = CoherenceHelper.getMap(CoherenceHelper.FIREWALL_INSTANCE_NAME, \"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); } Run the Test The client should be able to connect the server on the explicitly defined host and port. Summary In this guide, we gave a few deeper examples of how to set up Coherence*Extend to connect clients to a Coherence Cluster. As part of the Coherence reference documentation, we provide an entire guide on Developing Remote Clients for Oracle Coherence . Part I of that guide provides not only an introduction to Coherence*Extend but also covers advanced topics as well as best practices. See Also Bootstrap Coherence Run Coherence as an Extend Client Configure an Extend Client Introduction to Coherence*Extend Configuring Extend Clients Advanced Extend Configuration Best Practices for Coherence*Extend ",
            "title": "Coherence*Extend"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " What You Will Build What You Need Building The Example Code Example Data Model Why use Entry Processors? Creating an Entry Processor Using Lambda Expressions Process Single Map Keys Using Lambda Expressions Update all Map Entries Specify Cache Entry Expiration Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " The example code is written as a set of unit tests, showing how you can use Entry Processors with Coherence. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " In our example, we do have several countries loaded into the cache. Let&#8217;s assume we want to increase the population of several countries by a million each. More specifically, we only want to increase the population for those countries that have a population of 60 million or more. The obvious choice would be to query the cache using a GreaterEqualsFilter as we have done in the previous example on Views , iterate over the results and update the respective countries. <markup lang=\"java\" >@Test void testIncreasePopulationWithoutEntryProcessor() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Set&lt;String&gt; filteredKeys = map.keySet(filter); assertThat(filteredKeys).hasSize(2); for (String key : filteredKeys) { map.lock(key, 0); try { Country country = map.get(key); country.setPopulation(country.getPopulation() + 1); map.put(key, country); } finally { map.unlock(key); } } assertThat(map).hasSize(5); Country germany = map.get(\"de\"); Country france = map.get(\"fr\"); assertThat(germany.getPopulation()).isEqualTo(84.2d); assertThat(france.getPopulation()).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Apply the Filter by invoking keySet(filter) on the NamedCache that will return a Set of keys Assert that the Set of filtered keys only contains 2 keys Loop over the keys Make sure we lock the cache entry Increment the population by 1 million Update the map This is an example of how NOT to do this! While this works, it will be inefficient in use-cases where you have to update high number of cache entries.This approach would cause a lot of data to be moved over the wire, first for the retrieval of countries and then when pushing the updated countries back into the cluster. This is where Entry Processors come into play. Entry Processors allow us to perform data grid processing inside the Coherence cluster. You can either apply Entry Processors for single cache keys or you can perform parallel processing against a collection of cache entries (map-reduce functionality). For a more in-depth introduction to Entry Processors, please refer to the respective chapter Processing Data In a Cache in the Oracle Coherence reference guide. ",
            "title": "Why use Entry Processors?"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " Let&#8217;s rewrite the inefficient example above to use an Entry Processor. First, we will create a class called IncrementingEntryProcessor that implements InvocableMap.EntryProcessor : <markup lang=\"java\" >import com.oracle.coherence.guides.entryprocessors.model.Country; import com.tangosol.util.InvocableMap; /** * @author Gunnar Hillert 2022.02.25 */ public class IncrementingEntryProcessor implements InvocableMap.EntryProcessor&lt;String, Country, Double&gt; { @Override public Double process(InvocableMap.Entry&lt;String, Country&gt; entry) { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); } } The Entry Processor implements Coherence&#8217;s InvocableMap.EntryProcessor class. The type parameters represent the key, the value and the return type of the Entry Processor. The process() method gives us access to the value of the countries Map Increment the population by 1 million Return the incremented population The IncrementingEntryProcessor contains one method process() that provides us with access to the Country via the InvocableMap.Entry argument. We will increase the population and the return the population. Now it is time to use the IncrementingEntryProcessor . <markup lang=\"java\" >@Test void testIncreasePopulationWithCustomEntryProcessor() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Map&lt;String, Double&gt; results = map.invokeAll(filter, new IncrementingEntryProcessor()); assertThat(results).hasSize(2); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Call invokeAll on the countries Map, passing in both the filter and the IncrementingEntryProcessor The result should be the Map containing the key and the new population value for the 2 affected countries In this example we are processing multiple map entries at once. You can of course apply Entry Processors to single map keys as well by using: <markup lang=\"java\" >@Test void testIncreasePopulationForSingleEntry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Double result = map.invoke(\"de\", new IncrementingEntryProcessor()); assertThat(result).isEqualTo(84.2d); } Get the countries Map Call invoke on the countries Map, passing in the key (instead of the filter) and the IncrementingEntryProcessor The result should be the double value representing the new population value of Germany Tip Before writing your own Entry Processor from scratch, please also check if one of the built-in Entry Processors may already solve your requirement. In the next section we will see how we can simplify the example even further using lambda expressions. ",
            "title": "Creating an Entry Processor"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " Instead of creating dedicated Entry Processor classes, it may be advisable to pass in lambda expressions instead. Especially in use-cases such as our very simple contrived example, lambda expressions simplify the code noticeably. <markup lang=\"java\" >@Test void testIncreasePopulationUsingLambdaExpression() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(2); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Call invokeAll on the countries Map passing in the filter and the function that increments the population The result should be the Map containing the key and the new population value for the 2 affected countries ",
            "title": "Using Lambda Expressions"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " When using lambda expressions for single map keys, you can use Coherence&#8217;s invoke() as well as Java&#8217;s Map.compute() method. Let&#8217;s see the code for Coherence&#8217;s invoke() method first: <markup lang=\"java\" >@Test void testIncreasePopulationUsingInvokeForSingleCountry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Double results = map.invoke(\"de\", entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); entry.setValue(country); return country.getPopulation(); }); assertThat(results).isEqualTo(84.2d); assertThat(map.get(\"de\").getPopulation()).isEqualTo(84.2d); } Get the countries Map Call invoke on the NamedCache , passing the key for Germany and the lambda expression It is important to explicitly call setValue on the cache entry If using compute() , the code will look like this: <markup lang=\"java\" >@Test void testIncreasePopulationUsingComputeForSingleCountry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Country results = map.compute(\"de\", (key, country) -&gt; { country.setPopulation(country.getPopulation() + 1); return country; }); assertThat(results.getPopulation()).isEqualTo(84.2d); assertThat(map.get(\"de\").getPopulation()).isEqualTo(84.2d); } Get the countries Map Call compute on the NamedCache , passing the key for Germany and the lambda expression. Set the new population but as you can see that there is no need to set the country explicitly on the cache entry. The code when using compute looks a little simpler, as compute implicitly updates the value to whatever you return. When using invoke , you have to explicitly call entry.setValue(country) . On the other hand, compute will return the entire country, whereas with invoke you can return any data object. This is advantageous in situations where you need to minimize the amount of data passed over the wire. ",
            "title": "Process Single Map Keys Using Lambda Expressions"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " Sometimes we may need to update all entries in a Coherence Map. In that use-case we simply change the passed-in Filter. The lambda expression on the other hand remains the same. All we need to do is to pass in an instance of the AlwaysFilter : <markup lang=\"java\" >@Test void testIncreasePopulationForAllCountries() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = AlwaysFilter.INSTANCE(); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(5); assertThat(results.get(\"ua\")).isEqualTo(42.2d); assertThat(results.get(\"co\")).isEqualTo(51.4d); assertThat(results.get(\"au\")).isEqualTo(27d); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map Get an instance of the AlwaysFilter that will select all entries in the countries Map Call invokeAll on the countries Map passing in the AlwaysFilter and the function that increments the population The result should be the Map containing the key and the new population value for all 5 countries in the Map ",
            "title": "Update all Map Entries"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " When adding values to a NamedCache , without using an Entry Processor, you have the optional ability to also set an expiration value for the cache value. For example, in the following code snippet, we add a new country with an expiration value of 5 seconds (Values are specified in milliseconds). <markup lang=\"java\" >NamedCache&lt;String, Country&gt; countries = getMap(\"countries\"); Country country = new Country(\"Germany\", \"Berlin\", 83.2); countries.put(\"de\", country , 5000); Note Cache expiration can also be configured per-cache using the &lt;expiry-delay&gt; cache configuration element in the coherence-cache-config.xml file. See the reference documentation for details. You can do the same when mutating cache entries via Entry Processors. Let&#8217;s update the previous example so that all countries we are updating will expire within 2 seconds. As the code listed below is very similar to the previous example, we shall highlight the changes only: <markup lang=\"java\" >@Test void testIncreasePopulationForAllCountriesWithExpiration() throws InterruptedException { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = AlwaysFilter.INSTANCE(); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { BinaryEntry&lt;String, Double&gt; binEntry = (BinaryEntry) entry; binEntry.expire(2000); Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(5); assertThat(map).hasSize(5); assertThat(results.get(\"ua\")).isEqualTo(42.2d); assertThat(results.get(\"co\")).isEqualTo(51.4d); assertThat(results.get(\"au\")).isEqualTo(27d); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); Thread.sleep(4000); assertThat(results).hasSize(5); assertThat(map).hasSize(0); } We need to cast the InvocableMap.Entry to a BinaryEntry . BinaryEntry has a method expire that specifies when the cache entry will expire. We set it to 2 seconds. The parameter is an integer value representing milliseconds. The returned Map of invokeAll will have 5 countries . This Map will not \"expire\". There should be 5 entries in the NamedCache map. The NamedCache map will expire. The Thread will sleep for 4 seconds. Assert that the returned Map of invokeAll still has 5 countries Assert that the NamedCache map is empty. The values have expired. The important piece of information to remember is that the underlying BinaryEntry has the relevant property to specify the expiration of the cache entry, and we need to cast InvocableMap.Entry to a BinaryEntry . Important The expiration property is defined as an integer and is expressed in milliseconds. Therefore, the maximum amount of time cannot exceed approximately 24 days. ",
            "title": "Specify Cache Entry Expiration"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " In this guide we showed how you can easily create Entry Processors to perform data grid processing across a cluster. Please see the Coherence reference guide, specifically the chapter Processing Data In a Cache for more details. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " Processing Data In a Cache Querying Caches Views ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " This guide walks you through the concepts of creating Entry Processors . Entry Processors allow you to perform data grid processing across a cluster. That means without moving cache entries across the wire, you can process one or more cache entries locally on the storage node. Table of Contents What You Will Build What You Need Building The Example Code Example Data Model Why use Entry Processors? Creating an Entry Processor Using Lambda Expressions Process Single Map Keys Using Lambda Expressions Update all Map Entries Specify Cache Entry Expiration Summary See Also What You Will Build The example code is written as a set of unit tests, showing how you can use Entry Processors with Coherence. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Example Data Model The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . Why use Entry Processors? In our example, we do have several countries loaded into the cache. Let&#8217;s assume we want to increase the population of several countries by a million each. More specifically, we only want to increase the population for those countries that have a population of 60 million or more. The obvious choice would be to query the cache using a GreaterEqualsFilter as we have done in the previous example on Views , iterate over the results and update the respective countries. <markup lang=\"java\" >@Test void testIncreasePopulationWithoutEntryProcessor() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Set&lt;String&gt; filteredKeys = map.keySet(filter); assertThat(filteredKeys).hasSize(2); for (String key : filteredKeys) { map.lock(key, 0); try { Country country = map.get(key); country.setPopulation(country.getPopulation() + 1); map.put(key, country); } finally { map.unlock(key); } } assertThat(map).hasSize(5); Country germany = map.get(\"de\"); Country france = map.get(\"fr\"); assertThat(germany.getPopulation()).isEqualTo(84.2d); assertThat(france.getPopulation()).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Apply the Filter by invoking keySet(filter) on the NamedCache that will return a Set of keys Assert that the Set of filtered keys only contains 2 keys Loop over the keys Make sure we lock the cache entry Increment the population by 1 million Update the map This is an example of how NOT to do this! While this works, it will be inefficient in use-cases where you have to update high number of cache entries.This approach would cause a lot of data to be moved over the wire, first for the retrieval of countries and then when pushing the updated countries back into the cluster. This is where Entry Processors come into play. Entry Processors allow us to perform data grid processing inside the Coherence cluster. You can either apply Entry Processors for single cache keys or you can perform parallel processing against a collection of cache entries (map-reduce functionality). For a more in-depth introduction to Entry Processors, please refer to the respective chapter Processing Data In a Cache in the Oracle Coherence reference guide. Creating an Entry Processor Let&#8217;s rewrite the inefficient example above to use an Entry Processor. First, we will create a class called IncrementingEntryProcessor that implements InvocableMap.EntryProcessor : <markup lang=\"java\" >import com.oracle.coherence.guides.entryprocessors.model.Country; import com.tangosol.util.InvocableMap; /** * @author Gunnar Hillert 2022.02.25 */ public class IncrementingEntryProcessor implements InvocableMap.EntryProcessor&lt;String, Country, Double&gt; { @Override public Double process(InvocableMap.Entry&lt;String, Country&gt; entry) { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); } } The Entry Processor implements Coherence&#8217;s InvocableMap.EntryProcessor class. The type parameters represent the key, the value and the return type of the Entry Processor. The process() method gives us access to the value of the countries Map Increment the population by 1 million Return the incremented population The IncrementingEntryProcessor contains one method process() that provides us with access to the Country via the InvocableMap.Entry argument. We will increase the population and the return the population. Now it is time to use the IncrementingEntryProcessor . <markup lang=\"java\" >@Test void testIncreasePopulationWithCustomEntryProcessor() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Map&lt;String, Double&gt; results = map.invokeAll(filter, new IncrementingEntryProcessor()); assertThat(results).hasSize(2); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Call invokeAll on the countries Map, passing in both the filter and the IncrementingEntryProcessor The result should be the Map containing the key and the new population value for the 2 affected countries In this example we are processing multiple map entries at once. You can of course apply Entry Processors to single map keys as well by using: <markup lang=\"java\" >@Test void testIncreasePopulationForSingleEntry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Double result = map.invoke(\"de\", new IncrementingEntryProcessor()); assertThat(result).isEqualTo(84.2d); } Get the countries Map Call invoke on the countries Map, passing in the key (instead of the filter) and the IncrementingEntryProcessor The result should be the double value representing the new population value of Germany Tip Before writing your own Entry Processor from scratch, please also check if one of the built-in Entry Processors may already solve your requirement. In the next section we will see how we can simplify the example even further using lambda expressions. Using Lambda Expressions Instead of creating dedicated Entry Processor classes, it may be advisable to pass in lambda expressions instead. Especially in use-cases such as our very simple contrived example, lambda expressions simplify the code noticeably. <markup lang=\"java\" >@Test void testIncreasePopulationUsingLambdaExpression() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(2); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Call invokeAll on the countries Map passing in the filter and the function that increments the population The result should be the Map containing the key and the new population value for the 2 affected countries Process Single Map Keys Using Lambda Expressions When using lambda expressions for single map keys, you can use Coherence&#8217;s invoke() as well as Java&#8217;s Map.compute() method. Let&#8217;s see the code for Coherence&#8217;s invoke() method first: <markup lang=\"java\" >@Test void testIncreasePopulationUsingInvokeForSingleCountry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Double results = map.invoke(\"de\", entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); entry.setValue(country); return country.getPopulation(); }); assertThat(results).isEqualTo(84.2d); assertThat(map.get(\"de\").getPopulation()).isEqualTo(84.2d); } Get the countries Map Call invoke on the NamedCache , passing the key for Germany and the lambda expression It is important to explicitly call setValue on the cache entry If using compute() , the code will look like this: <markup lang=\"java\" >@Test void testIncreasePopulationUsingComputeForSingleCountry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Country results = map.compute(\"de\", (key, country) -&gt; { country.setPopulation(country.getPopulation() + 1); return country; }); assertThat(results.getPopulation()).isEqualTo(84.2d); assertThat(map.get(\"de\").getPopulation()).isEqualTo(84.2d); } Get the countries Map Call compute on the NamedCache , passing the key for Germany and the lambda expression. Set the new population but as you can see that there is no need to set the country explicitly on the cache entry. The code when using compute looks a little simpler, as compute implicitly updates the value to whatever you return. When using invoke , you have to explicitly call entry.setValue(country) . On the other hand, compute will return the entire country, whereas with invoke you can return any data object. This is advantageous in situations where you need to minimize the amount of data passed over the wire. Update all Map Entries Sometimes we may need to update all entries in a Coherence Map. In that use-case we simply change the passed-in Filter. The lambda expression on the other hand remains the same. All we need to do is to pass in an instance of the AlwaysFilter : <markup lang=\"java\" >@Test void testIncreasePopulationForAllCountries() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = AlwaysFilter.INSTANCE(); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(5); assertThat(results.get(\"ua\")).isEqualTo(42.2d); assertThat(results.get(\"co\")).isEqualTo(51.4d); assertThat(results.get(\"au\")).isEqualTo(27d); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map Get an instance of the AlwaysFilter that will select all entries in the countries Map Call invokeAll on the countries Map passing in the AlwaysFilter and the function that increments the population The result should be the Map containing the key and the new population value for all 5 countries in the Map Specify Cache Entry Expiration When adding values to a NamedCache , without using an Entry Processor, you have the optional ability to also set an expiration value for the cache value. For example, in the following code snippet, we add a new country with an expiration value of 5 seconds (Values are specified in milliseconds). <markup lang=\"java\" >NamedCache&lt;String, Country&gt; countries = getMap(\"countries\"); Country country = new Country(\"Germany\", \"Berlin\", 83.2); countries.put(\"de\", country , 5000); Note Cache expiration can also be configured per-cache using the &lt;expiry-delay&gt; cache configuration element in the coherence-cache-config.xml file. See the reference documentation for details. You can do the same when mutating cache entries via Entry Processors. Let&#8217;s update the previous example so that all countries we are updating will expire within 2 seconds. As the code listed below is very similar to the previous example, we shall highlight the changes only: <markup lang=\"java\" >@Test void testIncreasePopulationForAllCountriesWithExpiration() throws InterruptedException { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = AlwaysFilter.INSTANCE(); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { BinaryEntry&lt;String, Double&gt; binEntry = (BinaryEntry) entry; binEntry.expire(2000); Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(5); assertThat(map).hasSize(5); assertThat(results.get(\"ua\")).isEqualTo(42.2d); assertThat(results.get(\"co\")).isEqualTo(51.4d); assertThat(results.get(\"au\")).isEqualTo(27d); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); Thread.sleep(4000); assertThat(results).hasSize(5); assertThat(map).hasSize(0); } We need to cast the InvocableMap.Entry to a BinaryEntry . BinaryEntry has a method expire that specifies when the cache entry will expire. We set it to 2 seconds. The parameter is an integer value representing milliseconds. The returned Map of invokeAll will have 5 countries . This Map will not \"expire\". There should be 5 entries in the NamedCache map. The NamedCache map will expire. The Thread will sleep for 4 seconds. Assert that the returned Map of invokeAll still has 5 countries Assert that the NamedCache map is empty. The values have expired. The important piece of information to remember is that the underlying BinaryEntry has the relevant property to specify the expiration of the cache entry, and we need to cast InvocableMap.Entry to a BinaryEntry . Important The expiration property is defined as an integer and is expressed in milliseconds. Therefore, the maximum amount of time cannot exceed approximately 24 days. Summary In this guide we showed how you can easily create Entry Processors to perform data grid processing across a cluster. Please see the Coherence reference guide, specifically the chapter Processing Data In a Cache for more details. See Also Processing Data In a Cache Querying Caches Views ",
            "title": "Entry Processors"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " Note The documentation on this site covers new features and improvements that are currently only available in the open source Coherence Community Edition (CE). For complete documentation covering all the features that are available both in the latest commercial editions (Enterprise and Grid Edition) and the Community Edition, please refer to the Official Documentation . Coherence is scalable, fault-tolerant, cloud-ready, distributed platform for building grid-based applications and reliably storing data. The product is used at scale, for both compute and raw storage, in a vast array of industries such as critical financial trading systems, high performance telecommunication products, and eCommerce applications. Typically, these deployments do not tolerate any downtime and Coherence is chosen due its novel features in death detection, application data evolvability, and the robust, battle-hardened core of the product that enables it to be seamlessly deployed and adapted within any ecosystem. At a high level, Coherence provides an implementation of the familiar Map&lt;K,V&gt; interface but rather than storing the associated data in the local process, it is partitioned (or sharded) across a number of designated remote nodes. This partitioning enables applications to not only distribute (and therefore scale) their storage across multiple processes, machines, racks, and data centers, but also to perform grid-based processing to truly harness the CPU resources of the machines. The Coherence interface NamedMap&lt;K,V&gt; (an extension of Map&lt;K,V&gt; provides methods to query, aggregate (map/reduce style), and compute (send functions to storage nodes for locally executed mutations) the data set. These capabilities, in addition to numerous other features, enable Coherence to be used as a framework to write robust, distributed applications. See Core Improvements to learn about new and improved functionality since release 22.06. Please see here for the latest release notes on GitHub. ",
            "title": "Overview"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " assistant Coherence What is Oracle Coherence? fa-exclamation-circle Important Changes Important changes in this release of Coherence. fa-rocket Quick Start A quick-start guide to using Coherence. fa-graduation-cap Guides & Tutorials Guides, examples and tutorial about Coherence features and best practice. import_contacts Docs Oracle Coherence commercial edition product documentation. library_books API Docs Browse the Coherence CE API Docs. fa-th Container Images Example Coherence OCI container (Docker) images. ",
            "title": "Get Going"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " In this example you will utilize the built-in aggregators such as count , sum , min , average and top on orders and customers maps. You will also use the Aggregators class and its helpers to simplify aggregator usage. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " The data model consists of the following classes in two maps, customers and orders Customer - Represents a customer Order - Represents and order for a customer and contains order lines OrderLine - Represents an individual order line for an order Address - Represents an address for a customer ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " In this example we will show a number of the built-in aggregation functions in action. The full list is: count distinct average max min top sum All the above aggregators can be implemented using static helpers in the Aggregators class, for example Aggregators.count() . The helpers create the right aggregator type based on the type of method reference/extractor that is passed as an argument. They all return the EntryAggregator implementations which allows them to be passed as argument to the aggregate methods below. public default &lt;R&gt; R aggregate(EntryAggregator&lt;? super K, ? super V, R&gt; aggregator) - Aggregate across all entries in a cache public &lt;R&gt; R aggregate(Collection&lt;? extends K&gt; collKeys, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defined by the keys public &lt;R&gt; R aggregate(Filter filter, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defines by the filter The SimpleAggregationExample runs various aggregations using a number of the above functions. Example Details The runExample() method contains the code that exercises the above aggregators. Refer to the inline code comments for explanations of what each aggregator is doing. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { NamedMap&lt;Integer, Customer&gt; customers = getCustomers(); NamedMap&lt;Integer, Order&gt; orders = getOrders(); // count the customers using the Aggregators helper int customerCount = customers.aggregate(Aggregators.count()); Logger.info(\"Customer Count = \" + customerCount); // count the orders int orderCount = orders.aggregate(Aggregators.count()); Logger.info(\"Order Count = \" + orderCount); // get the total value of all orders - requires index on Order::getOrderTotal to be efficient Double totalOrders = orders.aggregate(Aggregators.sum(Order::getOrderTotal)); Logger.info(\"Total Order Value \" + formatMoney(totalOrders)); // get the average order value across all orders - requires index to be efficient Double averageOrderValue = orders.aggregate(Aggregators.average(Order::getOrderTotal)); Logger.info(\"Average Order Value \" + formatMoney(averageOrderValue)); // get the minimum order value where then is only 1 order line - requires index on Order::getOrderLineCount to be efficient Double minOrderValue1Line = orders.aggregate(Filters.equal(Order::getOrderLineCount, 1), Aggregators.min(Order::getOrderTotal)); Logger.info(\"Min Order Value for orders with 1 line \" + formatMoney(minOrderValue1Line)); // get the outstanding balances by state - requires index on the full ValueExtractor to be efficient ValueExtractor&lt;Customer, String&gt; officeState = ValueExtractor.of(Customer::getOfficeAddress).andThen(Address::getState); Map&lt;String, BigDecimal&gt; mapOutstandingByState = customers.aggregate( GroupAggregator.createInstance(officeState, Aggregators.sum(Customer::getOutstandingBalance))); mapOutstandingByState.forEach((k, v) -&gt; Logger.info(\"State: \" + k + \", outstanding total is \" + formatMoney(v))); // get the top 5 order totals by value Logger.info(\"Top 5 orders by value\"); Object[] topOrderValues = orders.aggregate(Aggregators.topN(Order::getOrderTotal, 5)); for (Object value : topOrderValues) { Logger.info(formatMoney((Double) value)); } } ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Run the following to load the data and run the example. E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Creating 10000 customers &lt;Info&gt; (thread=main, member=1): Creating orders for customers &lt;Info&gt; (thread=main, member=1): Orders created &lt;Info&gt; (thread=main, member=1): Customer Count = 10000 &lt;Info&gt; (thread=main, member=1): Order Count = 29848 &lt;Info&gt; (thread=main, member=1): Total Order Value $89,689,872.00 &lt;Info&gt; (thread=main, member=1): Average Order Value $3,004.89 &lt;Info&gt; (thread=main, member=1): Min Order Value for orders with 1 line $500.08 &lt;Info&gt; (thread=main, member=1): State: QLD, outstanding total is $567,600.00 &lt;Info&gt; (thread=main, member=1): State: WA, outstanding total is $585,800.00 &lt;Info&gt; (thread=main, member=1): State: SA, outstanding total is $561,900.00 &lt;Info&gt; (thread=main, member=1): State: VIC, outstanding total is $556,500.00 &lt;Info&gt; (thread=main, member=1): State: NT, outstanding total is $528,700.00 &lt;Info&gt; (thread=main, member=1): State: ACT, outstanding total is $566,800.00 &lt;Info&gt; (thread=main, member=1): State: TAS, outstanding total is $563,900.00 &lt;Info&gt; (thread=main, member=1): State: NSW, outstanding total is $530,900.00 &lt;Info&gt; (thread=main, member=1): Top 5 orders by value &lt;Info&gt; (thread=main, member=1): $8,304.27 &lt;Info&gt; (thread=main, member=1): $8,273.82 &lt;Info&gt; (thread=main, member=1): $8,229.51 &lt;Info&gt; (thread=main, member=1): $8,197.35 &lt;Info&gt; (thread=main, member=1): $8,194.63 ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " You have seen how to use built-in aggregators which include count , sum , min , average and top on orders and customers maps. You also used the Aggregators class and its helpers to simplify aggregator usage. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " Performing Data Grid Operations Streams ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " This guide walks you through how to use built-in aggregators such as including count, sum, min, average and top which allow you to process data stored in Coherence in parallel. Coherence supports entry aggregators that perform operations against all, or a subset of entries to obtain a single result. This aggregation is carried out in parallel across the cluster and is a map-reduce type of operation which can be performed efficiently across large amounts of data. See the Coherence Documentation for detailed information on Aggregations. Table of Contents What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also What You Will Build In this example you will utilize the built-in aggregators such as count , sum , min , average and top on orders and customers maps. You will also use the Aggregators class and its helpers to simplify aggregator usage. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Example Data Model The data model consists of the following classes in two maps, customers and orders Customer - Represents a customer Order - Represents and order for a customer and contains order lines OrderLine - Represents an individual order line for an order Address - Represents an address for a customer Review the Example Code In this example we will show a number of the built-in aggregation functions in action. The full list is: count distinct average max min top sum All the above aggregators can be implemented using static helpers in the Aggregators class, for example Aggregators.count() . The helpers create the right aggregator type based on the type of method reference/extractor that is passed as an argument. They all return the EntryAggregator implementations which allows them to be passed as argument to the aggregate methods below. public default &lt;R&gt; R aggregate(EntryAggregator&lt;? super K, ? super V, R&gt; aggregator) - Aggregate across all entries in a cache public &lt;R&gt; R aggregate(Collection&lt;? extends K&gt; collKeys, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defined by the keys public &lt;R&gt; R aggregate(Filter filter, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defines by the filter The SimpleAggregationExample runs various aggregations using a number of the above functions. Example Details The runExample() method contains the code that exercises the above aggregators. Refer to the inline code comments for explanations of what each aggregator is doing. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { NamedMap&lt;Integer, Customer&gt; customers = getCustomers(); NamedMap&lt;Integer, Order&gt; orders = getOrders(); // count the customers using the Aggregators helper int customerCount = customers.aggregate(Aggregators.count()); Logger.info(\"Customer Count = \" + customerCount); // count the orders int orderCount = orders.aggregate(Aggregators.count()); Logger.info(\"Order Count = \" + orderCount); // get the total value of all orders - requires index on Order::getOrderTotal to be efficient Double totalOrders = orders.aggregate(Aggregators.sum(Order::getOrderTotal)); Logger.info(\"Total Order Value \" + formatMoney(totalOrders)); // get the average order value across all orders - requires index to be efficient Double averageOrderValue = orders.aggregate(Aggregators.average(Order::getOrderTotal)); Logger.info(\"Average Order Value \" + formatMoney(averageOrderValue)); // get the minimum order value where then is only 1 order line - requires index on Order::getOrderLineCount to be efficient Double minOrderValue1Line = orders.aggregate(Filters.equal(Order::getOrderLineCount, 1), Aggregators.min(Order::getOrderTotal)); Logger.info(\"Min Order Value for orders with 1 line \" + formatMoney(minOrderValue1Line)); // get the outstanding balances by state - requires index on the full ValueExtractor to be efficient ValueExtractor&lt;Customer, String&gt; officeState = ValueExtractor.of(Customer::getOfficeAddress).andThen(Address::getState); Map&lt;String, BigDecimal&gt; mapOutstandingByState = customers.aggregate( GroupAggregator.createInstance(officeState, Aggregators.sum(Customer::getOutstandingBalance))); mapOutstandingByState.forEach((k, v) -&gt; Logger.info(\"State: \" + k + \", outstanding total is \" + formatMoney(v))); // get the top 5 order totals by value Logger.info(\"Top 5 orders by value\"); Object[] topOrderValues = orders.aggregate(Aggregators.topN(Order::getOrderTotal, 5)); for (Object value : topOrderValues) { Logger.info(formatMoney((Double) value)); } } Run the Example Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Run the following to load the data and run the example. E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Creating 10000 customers &lt;Info&gt; (thread=main, member=1): Creating orders for customers &lt;Info&gt; (thread=main, member=1): Orders created &lt;Info&gt; (thread=main, member=1): Customer Count = 10000 &lt;Info&gt; (thread=main, member=1): Order Count = 29848 &lt;Info&gt; (thread=main, member=1): Total Order Value $89,689,872.00 &lt;Info&gt; (thread=main, member=1): Average Order Value $3,004.89 &lt;Info&gt; (thread=main, member=1): Min Order Value for orders with 1 line $500.08 &lt;Info&gt; (thread=main, member=1): State: QLD, outstanding total is $567,600.00 &lt;Info&gt; (thread=main, member=1): State: WA, outstanding total is $585,800.00 &lt;Info&gt; (thread=main, member=1): State: SA, outstanding total is $561,900.00 &lt;Info&gt; (thread=main, member=1): State: VIC, outstanding total is $556,500.00 &lt;Info&gt; (thread=main, member=1): State: NT, outstanding total is $528,700.00 &lt;Info&gt; (thread=main, member=1): State: ACT, outstanding total is $566,800.00 &lt;Info&gt; (thread=main, member=1): State: TAS, outstanding total is $563,900.00 &lt;Info&gt; (thread=main, member=1): State: NSW, outstanding total is $530,900.00 &lt;Info&gt; (thread=main, member=1): Top 5 orders by value &lt;Info&gt; (thread=main, member=1): $8,304.27 &lt;Info&gt; (thread=main, member=1): $8,273.82 &lt;Info&gt; (thread=main, member=1): $8,229.51 &lt;Info&gt; (thread=main, member=1): $8,197.35 &lt;Info&gt; (thread=main, member=1): $8,194.63 Summary You have seen how to use built-in aggregators which include count , sum , min , average and top on orders and customers maps. You also used the Aggregators class and its helpers to simplify aggregator usage. See Also Performing Data Grid Operations Streams ",
            "title": "Built-In Aggregators"
        },
        {
            "location": "/examples/README",
            "text": " These guides and tutorials are designed to help you be productive as quickly as possible in whatever use-case you are building with Coherence. Coherence has a long history and having been around for twenty years its APIs have evolved over that time. Occasionally there are multiple ways to implement a specific use-case, typically because to remain backwards compatible with older releases, features cannot be removed from the product. For that reason these guides use the latest Coherence versions and best practice and approaches recommended by the Coherence team for that version. explore Simple Guides fa-graduation-cap Tutorials ",
            "title": "Overview"
        },
        {
            "location": "/examples/README",
            "text": " These simple guides are designed to be a quick hands-on introduction to a specific feature of Coherence. In most cases they require nothing more than a Coherence jar and an IDE (or a text editor if you&#8217;re really old-school). Guides are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. Bootstrap Coherence This guide walks you through various methods to configure and bootstrap a Coherence instance. Coherence*Extend Provides a guide for clients to connect to a Coherence Cluster via Coherence*Extend. Put Get and Remove This guide walks you through basic CRUD put , get , and remove operations on a NamedMap . Querying Caches This guide walks you through the basic concepts of querying Coherence caches. Built-in Aggregators This guide walks you through how to use built-in aggregators within Coherence. Custom Aggregators This guide walks you through how to create custom aggregators within Coherence. Views Learn about the basic concepts of working with views using the ContinuousQueryCache . Streams This guide walks you through how to use the Streams API with Coherence. Entry Processors This guide walks you through how to use Entry Processors with Coherence. Federation This guide walks you through how to use Federation within Coherence. Topics This guide walks you through how to use Topics within Coherence. Near Caching This guide walks you through how to use near caching within Coherence. Client Events This guide walks you through how to use client events within Coherence. Server-Side Events This guide walks you through how to use server-side events within Coherence. Durable Events This guide walks you through how to use durable events within Coherence. Cache Stores This guide walks you through how to use and configure Cache Stores. Bulk Loading Caches This guide shows approaches to bulk load data into caches, typically this would be loading data into caches from a DB when applications start. Securing with SSL This guide walks you through how to secure Coherence using SSL/TLS. Performance over Consistency & Availability This guide walks you through how to tweak Coherence to provide more performance at the expense of data consistency and availability. Executor Service This guide explains how to use the Coherence Executor Service. CDI Response Caching This guide walks you through how to configure CDI Response Caching to cache the results of method invocations. Key Association This guide walks you through a use case for key association in Coherence. Partition Level Transactions This guide explains how to atomically access and update multiple related entries using an EntryProcessor in a partition level transaction. Multi-Cluster Client An example of how to connect an Extend or gRPC client to multiple Coherence clusters. ",
            "title": "Guides"
        },
        {
            "location": "/examples/README",
            "text": " These tutorials provide a deeper understanding of larger Coherence features and concepts that cannot be usually be explained with a few simple code snippets. They might, for example, require a running Coherence cluster to properly show a feature. Tutorials are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. GraphQL This tutorial shows you how to access Coherence Data using GraphQL. Persistence This tutorial shows you how to use Persistence from CohQL and how to monitor Persistence events. ",
            "title": "Tutorials"
        },
        {
            "location": "/docs/core/02_topics",
            "text": " The PagedTopic MBean provides statistics for Topic services running in a cluster. A cluster contains zero or more instances of this MBean, each instance representing an instance of a Topic on a member. The object name of the MBean is: type=PagedTopic,service=ServiceName,name=TopicName,nodeId=node Attributes PagedTopic MBean attributes Attribute Type Access Description AllowUnownedCommits Boolean read-only Allow Unowned Commits. ChannelCount Integer read-only The number of channels in the topic. Channels TabularData read-only Channel statistics. ElementCalculator String read-only Element Calculator. PageCapacity Integer read-only The capacity of a page. PublishedCount Long read-only The number of published messages. PublishedFifteenMinuteRate Double read-only The published messages fifteen-minute rate. PublishedFiveMinuteRate Double read-only The published messages five-minute rate. PublishedMeanRate Double read-only The published messages mean rate. PublishedOneMinuteRate Double read-only The published messages one-minute rate. ReconnectRetry Integer read-only Reconnect Retry. ReconnectTimeout Integer read-only Reconnect Timeout. ReconnectWait Integer read-only Reconnect Wait. RetainConsumed Boolean read-only Retain consumed values. SubscriberTimeout Integer read-only Subscriber Timeout. Operations PagedTopic MBean operations Operation Parameters Return Type Description disconnectAll Not applicable Void Force this topic to disconnect all subscribers. ",
            "title": "PagedTopic MBean"
        },
        {
            "location": "/docs/core/02_topics",
            "text": " The PagedTopicSubscriber MBean provides statistics for Topic Subscribers running in a cluster. A cluster contains zero or more instances of this MBean, each instance representing an instance of a Topic Subscriber on a member. The object name of the MBean is: type=PagedTopicSubscriber,service=ServiceName,topic=TopicName,subtype=SubType,group=SubscriberGroup,id=SubscriberId,nodeId=node PagedTopicSubscriber MBean attributes Attribute Type Access Description Backlog Integer read-only The number of outstanding receive requests. ChannelAllocations String read-only The subscriber&#8217;s allocated channels. ChannelCount Integer read-only The number of channels in the topic. Channels TabularData read-only Channel statistics. CompleteOnEmpty Boolean read-only A flag indicating whether the subscriber completes receive requests with a null message when the topic is empty. Converter String read-only The optional converter being used to transform messages. Disconnections Long read-only The number of times this subscriber has disconnected. Filter String read-only The optional filter being used to filter messages. Id Long read-only The subscriber&#8217;s identifier. IdentifyingName String read-only An optional name to help identify this subscriber. MaxBacklog Long read-only The maximum number of outstanding receive requests allowed before flow control blocks receive calls. Member String read-only The cluster member owning this subscriber. NotificationId Long read-only The subscriber&#8217;s notification identifier. Notifications Long read-only The number of channel notifications received. Polls Long read-only The total number of polls for messages. ReceiveCompletionsCount Long read-only The number completed receive requests. ReceiveCompletionsFifteenMinuteRate Double read-only The completed receive requests, fifteen-minute rate. ReceiveCompletionsFiveMinuteRate Double read-only The completed receive requests, five-minute rate. ReceiveCompletionsMeanRate Double read-only The completed receive requests, mean rate. ReceiveCompletionsOneMinuteRate Double read-only The completed receive requests, one-minute rate. ReceiveEmpty Long read-only The number empty receive requests. ReceiveErrors Long read-only The number exceptionally completed receive requests. ReceivedCount Long read-only The number of elements received. Serializer String read-only The serializer used to deserialize messages. State Integer read-only The state of the subscriber. Valid values are: 0 – Initial, 1 – Connected, 2 – Disconnected, 3 – Closing, 4 - Closed StateName String read-only The state of the subscriber as a string. SubTypeCode Integer read-only Indicates if the subscriber is Durable (1) or Anonymous (0). SubscriberGroup String read-only The subscriber group the subscriber belongs to. Type String read-only The type of this subscriber. Waits Long read-only The number of waits on an empty channel. Operations PagedTopicSubscriber MBean operations Operation Parameters Return Type Description connect Not applicable Void Ensure this subscriber is connected. disconnect Not applicable Void Force this subscriber to disconnect and reset itself. heads Not applicable TabularData Retrieve the current head positions for each channel. notifyPopulated Integer nChannel Void Send a channel populated notification to this subscriber. remainingMessages Not applicable TabularData Retrieve the count of remaining messages for each channel. ",
            "title": "PagedTopicSubscriber MBean"
        },
        {
            "location": "/docs/core/02_topics",
            "text": " The PagedTopicSubscriberGroup MBean provides statistics for Topic Subscriber Groups running in a cluster. A cluster contains zero or more instances of this MBean, each instance representing an instance of a Topic Subscriber Group on a member. The object name of the MBean is: type=PagedTopicSubscriberGroup,service=ServiceName,topic=TopicName,subtype=SubType,name=SubscriberGroup,nodeId=node PagedTopicSubscriberGroup MBean attributes Attribute Type Access Description ChannelCount Integer read-only The number of channels in the topic. Channels TabularData read-only Channel statistics. Filter String read-only The filter. PolledCount Long read-only The number of polled messages. PolledFifteenMinuteRate Double read-only The polled messages fifteen-minute rate. PolledFiveMinuteRate Double read-only The polled messages five-minute rate. PolledMeanRate Double read-only The polled messages mean rate. PolledOneMinuteRate Double read-only The polled messages one-minute rate. Transformer String read-only The transformer. Operations PagedPagedTopicSubscriberGroupTopic MBean operations Operation Parameters Return Type Description disconnectAll Not applicable Void Force this subscriber group to disconnect all subscribers. ",
            "title": "PagedTopicSubscriberGroup MBean"
        },
        {
            "location": "/docs/core/02_topics",
            "text": " Three new Topics MBeans are now available, PagedTopic , PagedTopicSubscriber and PagedTopicSubscriberGroup MBean. These are described in more detail below: PagedTopic MBean The PagedTopic MBean provides statistics for Topic services running in a cluster. A cluster contains zero or more instances of this MBean, each instance representing an instance of a Topic on a member. The object name of the MBean is: type=PagedTopic,service=ServiceName,name=TopicName,nodeId=node Attributes PagedTopic MBean attributes Attribute Type Access Description AllowUnownedCommits Boolean read-only Allow Unowned Commits. ChannelCount Integer read-only The number of channels in the topic. Channels TabularData read-only Channel statistics. ElementCalculator String read-only Element Calculator. PageCapacity Integer read-only The capacity of a page. PublishedCount Long read-only The number of published messages. PublishedFifteenMinuteRate Double read-only The published messages fifteen-minute rate. PublishedFiveMinuteRate Double read-only The published messages five-minute rate. PublishedMeanRate Double read-only The published messages mean rate. PublishedOneMinuteRate Double read-only The published messages one-minute rate. ReconnectRetry Integer read-only Reconnect Retry. ReconnectTimeout Integer read-only Reconnect Timeout. ReconnectWait Integer read-only Reconnect Wait. RetainConsumed Boolean read-only Retain consumed values. SubscriberTimeout Integer read-only Subscriber Timeout. Operations PagedTopic MBean operations Operation Parameters Return Type Description disconnectAll Not applicable Void Force this topic to disconnect all subscribers. PagedTopicSubscriber MBean The PagedTopicSubscriber MBean provides statistics for Topic Subscribers running in a cluster. A cluster contains zero or more instances of this MBean, each instance representing an instance of a Topic Subscriber on a member. The object name of the MBean is: type=PagedTopicSubscriber,service=ServiceName,topic=TopicName,subtype=SubType,group=SubscriberGroup,id=SubscriberId,nodeId=node PagedTopicSubscriber MBean attributes Attribute Type Access Description Backlog Integer read-only The number of outstanding receive requests. ChannelAllocations String read-only The subscriber&#8217;s allocated channels. ChannelCount Integer read-only The number of channels in the topic. Channels TabularData read-only Channel statistics. CompleteOnEmpty Boolean read-only A flag indicating whether the subscriber completes receive requests with a null message when the topic is empty. Converter String read-only The optional converter being used to transform messages. Disconnections Long read-only The number of times this subscriber has disconnected. Filter String read-only The optional filter being used to filter messages. Id Long read-only The subscriber&#8217;s identifier. IdentifyingName String read-only An optional name to help identify this subscriber. MaxBacklog Long read-only The maximum number of outstanding receive requests allowed before flow control blocks receive calls. Member String read-only The cluster member owning this subscriber. NotificationId Long read-only The subscriber&#8217;s notification identifier. Notifications Long read-only The number of channel notifications received. Polls Long read-only The total number of polls for messages. ReceiveCompletionsCount Long read-only The number completed receive requests. ReceiveCompletionsFifteenMinuteRate Double read-only The completed receive requests, fifteen-minute rate. ReceiveCompletionsFiveMinuteRate Double read-only The completed receive requests, five-minute rate. ReceiveCompletionsMeanRate Double read-only The completed receive requests, mean rate. ReceiveCompletionsOneMinuteRate Double read-only The completed receive requests, one-minute rate. ReceiveEmpty Long read-only The number empty receive requests. ReceiveErrors Long read-only The number exceptionally completed receive requests. ReceivedCount Long read-only The number of elements received. Serializer String read-only The serializer used to deserialize messages. State Integer read-only The state of the subscriber. Valid values are: 0 – Initial, 1 – Connected, 2 – Disconnected, 3 – Closing, 4 - Closed StateName String read-only The state of the subscriber as a string. SubTypeCode Integer read-only Indicates if the subscriber is Durable (1) or Anonymous (0). SubscriberGroup String read-only The subscriber group the subscriber belongs to. Type String read-only The type of this subscriber. Waits Long read-only The number of waits on an empty channel. Operations PagedTopicSubscriber MBean operations Operation Parameters Return Type Description connect Not applicable Void Ensure this subscriber is connected. disconnect Not applicable Void Force this subscriber to disconnect and reset itself. heads Not applicable TabularData Retrieve the current head positions for each channel. notifyPopulated Integer nChannel Void Send a channel populated notification to this subscriber. remainingMessages Not applicable TabularData Retrieve the count of remaining messages for each channel. PagedTopicSubscriberGroup MBean The PagedTopicSubscriberGroup MBean provides statistics for Topic Subscriber Groups running in a cluster. A cluster contains zero or more instances of this MBean, each instance representing an instance of a Topic Subscriber Group on a member. The object name of the MBean is: type=PagedTopicSubscriberGroup,service=ServiceName,topic=TopicName,subtype=SubType,name=SubscriberGroup,nodeId=node PagedTopicSubscriberGroup MBean attributes Attribute Type Access Description ChannelCount Integer read-only The number of channels in the topic. Channels TabularData read-only Channel statistics. Filter String read-only The filter. PolledCount Long read-only The number of polled messages. PolledFifteenMinuteRate Double read-only The polled messages fifteen-minute rate. PolledFiveMinuteRate Double read-only The polled messages five-minute rate. PolledMeanRate Double read-only The polled messages mean rate. PolledOneMinuteRate Double read-only The polled messages one-minute rate. Transformer String read-only The transformer. Operations PagedPagedTopicSubscriberGroupTopic MBean operations Operation Parameters Return Type Description disconnectAll Not applicable Void Force this subscriber group to disconnect all subscribers. ",
            "title": "Topics MBeans"
        },
        {
            "location": "/docs/core/02_topics",
            "text": " The following reports have been added in this release. Topic Report The topic report provides detailed metrics for topics defined within a cluster. The name of the topic report is timestamp-topic.txt where the timestamp is in YYYYMMDDHH format. For example, a file named 2009013101-topics.txt represents a topics report for January 31, 2009 at 1:00 a.m. This report is not included in report-group.xml but is available by running report-all.xml . Topic Report Attribute Type Description Batch Counter Long A sequential counter to help integrate information between related files. This value resets when the reporter restarts and is not consistent across members. However, it is helpful when trying to integrate files. Report Date Date A timestamp for each report refresh. Service String The service name. Name String The topic name. NodeId String The numeric member identifier. ChannelCount Integer The number of channels in the topic. PublishedCount Long The number of published messages since the last report refresh. PublishedFifteenMinuteRate Double The published messages fifteen-minute rate. PublishedFiveMinuteRate Double The published messages five-minute rate. PublishedMeanRate Double The published messages mean rate. PublishedOneMinuteRate Double The published messages one-minute rate. Topic Subscribers Report The topic subscriber report provides detailed metrics for topic subscribers defined within a cluster. The name of the topic subscribers report is timestamp-topic-subscribers.txt where the timestamp is in YYYYMMDDHH format. For example, a file named 2009013101-topic-subscribers.txt represents a topic subscriber report for January 31, 2009 at 1:00 a.m. This report is not included in report-group.xml but is available by running report-all.xml . Topic Subscribers Report Attribute Type Description Batch Counter Long A sequential counter to help integrate information between related files. This value resets when the reporter restarts and is not consistent across members. However, it is helpful when trying to integrate files. Report Date Date A timestamp for each report refresh. Service String The service name. Name String The topic name. SubscriberGroup String The subscriber group the subscriber belongs to. Id Long The Id of the subscriber. NodeId String The numeric member identifier. Backlog Long The number of outstanding receive requests. ChannelAllocations String The subscriber&#8217;s allocated channels. ChannelCount Integer The number of channels in the topic. Disconnections Long The number of times this subscriber has disconnected since the last report refresh. Notifications Long The number of channel notifications received since the last report refresh. Polls Long The total number of polls for messages since the last report refresh. ReceiveCompletionsCount Long The number completed receive requests since the last report refresh . ReceiveCompletionsFifteenMinuteRate Double The completed receive requests, fifteen-minute rate. ReceiveCompletionsFiveMinuteRate Double The completed receive requests, five-minute rate. ReceiveCompletionsMeanRate Double The completed receive requests, mean rate. ReceiveCompletionsOneMinuteRate Double The completed receive requests, one-minute rate. ReceiveEmpty Long The number empty receive requests since the last report refresh. ReceiveErrors Long The number exceptionally completed receive requests since the last report refresh. ReceivedCount Long The number of elements received since the last report refresh. State Integer The state of the subscriber. Valid values are: 0 – Initial, 1 – Connected, 2 – Disconnected, 3 – Closing, 4 - Closed. StateName String The state of the subscriber as a string. Waits Long The number of elements received since the last report refresh. Topic Subscriber Groups Report The topic subscriber groups report provides detailed metrics for topic subscriber groups defined within a cluster. The name of the topic subscriber groups report is timestamp-topic-subscriber-groups.txt where the timestamp is in YYYYMMDDHH format. For example, a file named 2009013101-topic-subscriber-groups.txt represents a topic subscriber report for January 31, 2009 at 1:00 a.m. This report is not included in report-group.xml but is available by running report-all.xml. Topic Subscriber Groups Report Attribute Type Description Batch Counter Long A sequential counter to help integrate information between related files. This value resets when the reporter restarts and is not consistent across members. However, it is helpful when trying to integrate files. Report Date Date A timestamp for each report refresh. Service String The service name. Topic String The topic name. Name String The subscriber group the subscriber belongs to. NodeId String The numeric member identifier. ChannelCount Integer The number of channels in the topic. PolledCount Long The total number of polls for messages since the last report refresh. PolledFifteenMinuteRate Double The polled messages fifteen-minute rate PolledFiveMinuteRate Double The polled messages five-minute rate PolledOneMinuteRate Double The polled messages one-minute rate PolledMeanRate Double The polled messages mean rate ",
            "title": "Topics Reports"
        },
        {
            "location": "/docs/core/02_topics",
            "text": " You are now able to view and manage Topics, Subscribers and Subscriber Groups using Management over REST API. For example, to retrieve the topics for a service you can use the following curl command replacing serviceName with your Topics service name. <markup lang=\"bash\" >curl http://host:port/management/coherence/cluster/services/serviceName/topics See REST API for Managing Oracle Coherence for full details of the available REST end points. ",
            "title": "Management over REST"
        },
        {
            "location": "/docs/core/02_topics",
            "text": " The Coherence VisualVM Plugin and Coherence CLI have been updated to provide management and monitoring of Topics within a Coherence cluster. See the following links for more information on each of the tools. Coherence CLI - See Coherence CLI on GitHub and CLI Command Reference . Coherence VisualVM Plugin - See Coherence VisualVM on GitHub ",
            "title": "Topics Management via CLI and VisualVM"
        },
        {
            "location": "/docs/core/02_topics",
            "text": " There are four new Grafana Dashboards available to show Topics related information: Topics Summary Topic Details Topic Subscriber Details Topic Subscriber Group Details The above dashboards are available from the Coherence Operator GitHub repository. See the Oracle Metrics Documentation for more information on configuring metrics. ",
            "title": "Topics Grafana Dashboards"
        },
        {
            "location": "/docs/core/02_topics",
            "text": " This version of Coherence introduces additional features and functionality to help you manage and monitor topics within a Coherence cluster. New Topics MBeans are at the core with three new reports allowing for analysis of these MBeans overtime as well as additional Management over REST endpoints to interact with the MBeans. The Coherence CLI and VisualVM plugin now support viewing and managing Topics and new Grafana Dashboards are available to view Topics metrics over time. Topics MBeans Topics Reports Management over REST Topics Management via CLI and VisualVM Topics Grafana Dashboards Topics MBeans Three new Topics MBeans are now available, PagedTopic , PagedTopicSubscriber and PagedTopicSubscriberGroup MBean. These are described in more detail below: PagedTopic MBean The PagedTopic MBean provides statistics for Topic services running in a cluster. A cluster contains zero or more instances of this MBean, each instance representing an instance of a Topic on a member. The object name of the MBean is: type=PagedTopic,service=ServiceName,name=TopicName,nodeId=node Attributes PagedTopic MBean attributes Attribute Type Access Description AllowUnownedCommits Boolean read-only Allow Unowned Commits. ChannelCount Integer read-only The number of channels in the topic. Channels TabularData read-only Channel statistics. ElementCalculator String read-only Element Calculator. PageCapacity Integer read-only The capacity of a page. PublishedCount Long read-only The number of published messages. PublishedFifteenMinuteRate Double read-only The published messages fifteen-minute rate. PublishedFiveMinuteRate Double read-only The published messages five-minute rate. PublishedMeanRate Double read-only The published messages mean rate. PublishedOneMinuteRate Double read-only The published messages one-minute rate. ReconnectRetry Integer read-only Reconnect Retry. ReconnectTimeout Integer read-only Reconnect Timeout. ReconnectWait Integer read-only Reconnect Wait. RetainConsumed Boolean read-only Retain consumed values. SubscriberTimeout Integer read-only Subscriber Timeout. Operations PagedTopic MBean operations Operation Parameters Return Type Description disconnectAll Not applicable Void Force this topic to disconnect all subscribers. PagedTopicSubscriber MBean The PagedTopicSubscriber MBean provides statistics for Topic Subscribers running in a cluster. A cluster contains zero or more instances of this MBean, each instance representing an instance of a Topic Subscriber on a member. The object name of the MBean is: type=PagedTopicSubscriber,service=ServiceName,topic=TopicName,subtype=SubType,group=SubscriberGroup,id=SubscriberId,nodeId=node PagedTopicSubscriber MBean attributes Attribute Type Access Description Backlog Integer read-only The number of outstanding receive requests. ChannelAllocations String read-only The subscriber&#8217;s allocated channels. ChannelCount Integer read-only The number of channels in the topic. Channels TabularData read-only Channel statistics. CompleteOnEmpty Boolean read-only A flag indicating whether the subscriber completes receive requests with a null message when the topic is empty. Converter String read-only The optional converter being used to transform messages. Disconnections Long read-only The number of times this subscriber has disconnected. Filter String read-only The optional filter being used to filter messages. Id Long read-only The subscriber&#8217;s identifier. IdentifyingName String read-only An optional name to help identify this subscriber. MaxBacklog Long read-only The maximum number of outstanding receive requests allowed before flow control blocks receive calls. Member String read-only The cluster member owning this subscriber. NotificationId Long read-only The subscriber&#8217;s notification identifier. Notifications Long read-only The number of channel notifications received. Polls Long read-only The total number of polls for messages. ReceiveCompletionsCount Long read-only The number completed receive requests. ReceiveCompletionsFifteenMinuteRate Double read-only The completed receive requests, fifteen-minute rate. ReceiveCompletionsFiveMinuteRate Double read-only The completed receive requests, five-minute rate. ReceiveCompletionsMeanRate Double read-only The completed receive requests, mean rate. ReceiveCompletionsOneMinuteRate Double read-only The completed receive requests, one-minute rate. ReceiveEmpty Long read-only The number empty receive requests. ReceiveErrors Long read-only The number exceptionally completed receive requests. ReceivedCount Long read-only The number of elements received. Serializer String read-only The serializer used to deserialize messages. State Integer read-only The state of the subscriber. Valid values are: 0 – Initial, 1 – Connected, 2 – Disconnected, 3 – Closing, 4 - Closed StateName String read-only The state of the subscriber as a string. SubTypeCode Integer read-only Indicates if the subscriber is Durable (1) or Anonymous (0). SubscriberGroup String read-only The subscriber group the subscriber belongs to. Type String read-only The type of this subscriber. Waits Long read-only The number of waits on an empty channel. Operations PagedTopicSubscriber MBean operations Operation Parameters Return Type Description connect Not applicable Void Ensure this subscriber is connected. disconnect Not applicable Void Force this subscriber to disconnect and reset itself. heads Not applicable TabularData Retrieve the current head positions for each channel. notifyPopulated Integer nChannel Void Send a channel populated notification to this subscriber. remainingMessages Not applicable TabularData Retrieve the count of remaining messages for each channel. PagedTopicSubscriberGroup MBean The PagedTopicSubscriberGroup MBean provides statistics for Topic Subscriber Groups running in a cluster. A cluster contains zero or more instances of this MBean, each instance representing an instance of a Topic Subscriber Group on a member. The object name of the MBean is: type=PagedTopicSubscriberGroup,service=ServiceName,topic=TopicName,subtype=SubType,name=SubscriberGroup,nodeId=node PagedTopicSubscriberGroup MBean attributes Attribute Type Access Description ChannelCount Integer read-only The number of channels in the topic. Channels TabularData read-only Channel statistics. Filter String read-only The filter. PolledCount Long read-only The number of polled messages. PolledFifteenMinuteRate Double read-only The polled messages fifteen-minute rate. PolledFiveMinuteRate Double read-only The polled messages five-minute rate. PolledMeanRate Double read-only The polled messages mean rate. PolledOneMinuteRate Double read-only The polled messages one-minute rate. Transformer String read-only The transformer. Operations PagedPagedTopicSubscriberGroupTopic MBean operations Operation Parameters Return Type Description disconnectAll Not applicable Void Force this subscriber group to disconnect all subscribers. Topics Reports The following reports have been added in this release. Topic Report The topic report provides detailed metrics for topics defined within a cluster. The name of the topic report is timestamp-topic.txt where the timestamp is in YYYYMMDDHH format. For example, a file named 2009013101-topics.txt represents a topics report for January 31, 2009 at 1:00 a.m. This report is not included in report-group.xml but is available by running report-all.xml . Topic Report Attribute Type Description Batch Counter Long A sequential counter to help integrate information between related files. This value resets when the reporter restarts and is not consistent across members. However, it is helpful when trying to integrate files. Report Date Date A timestamp for each report refresh. Service String The service name. Name String The topic name. NodeId String The numeric member identifier. ChannelCount Integer The number of channels in the topic. PublishedCount Long The number of published messages since the last report refresh. PublishedFifteenMinuteRate Double The published messages fifteen-minute rate. PublishedFiveMinuteRate Double The published messages five-minute rate. PublishedMeanRate Double The published messages mean rate. PublishedOneMinuteRate Double The published messages one-minute rate. Topic Subscribers Report The topic subscriber report provides detailed metrics for topic subscribers defined within a cluster. The name of the topic subscribers report is timestamp-topic-subscribers.txt where the timestamp is in YYYYMMDDHH format. For example, a file named 2009013101-topic-subscribers.txt represents a topic subscriber report for January 31, 2009 at 1:00 a.m. This report is not included in report-group.xml but is available by running report-all.xml . Topic Subscribers Report Attribute Type Description Batch Counter Long A sequential counter to help integrate information between related files. This value resets when the reporter restarts and is not consistent across members. However, it is helpful when trying to integrate files. Report Date Date A timestamp for each report refresh. Service String The service name. Name String The topic name. SubscriberGroup String The subscriber group the subscriber belongs to. Id Long The Id of the subscriber. NodeId String The numeric member identifier. Backlog Long The number of outstanding receive requests. ChannelAllocations String The subscriber&#8217;s allocated channels. ChannelCount Integer The number of channels in the topic. Disconnections Long The number of times this subscriber has disconnected since the last report refresh. Notifications Long The number of channel notifications received since the last report refresh. Polls Long The total number of polls for messages since the last report refresh. ReceiveCompletionsCount Long The number completed receive requests since the last report refresh . ReceiveCompletionsFifteenMinuteRate Double The completed receive requests, fifteen-minute rate. ReceiveCompletionsFiveMinuteRate Double The completed receive requests, five-minute rate. ReceiveCompletionsMeanRate Double The completed receive requests, mean rate. ReceiveCompletionsOneMinuteRate Double The completed receive requests, one-minute rate. ReceiveEmpty Long The number empty receive requests since the last report refresh. ReceiveErrors Long The number exceptionally completed receive requests since the last report refresh. ReceivedCount Long The number of elements received since the last report refresh. State Integer The state of the subscriber. Valid values are: 0 – Initial, 1 – Connected, 2 – Disconnected, 3 – Closing, 4 - Closed. StateName String The state of the subscriber as a string. Waits Long The number of elements received since the last report refresh. Topic Subscriber Groups Report The topic subscriber groups report provides detailed metrics for topic subscriber groups defined within a cluster. The name of the topic subscriber groups report is timestamp-topic-subscriber-groups.txt where the timestamp is in YYYYMMDDHH format. For example, a file named 2009013101-topic-subscriber-groups.txt represents a topic subscriber report for January 31, 2009 at 1:00 a.m. This report is not included in report-group.xml but is available by running report-all.xml. Topic Subscriber Groups Report Attribute Type Description Batch Counter Long A sequential counter to help integrate information between related files. This value resets when the reporter restarts and is not consistent across members. However, it is helpful when trying to integrate files. Report Date Date A timestamp for each report refresh. Service String The service name. Topic String The topic name. Name String The subscriber group the subscriber belongs to. NodeId String The numeric member identifier. ChannelCount Integer The number of channels in the topic. PolledCount Long The total number of polls for messages since the last report refresh. PolledFifteenMinuteRate Double The polled messages fifteen-minute rate PolledFiveMinuteRate Double The polled messages five-minute rate PolledOneMinuteRate Double The polled messages one-minute rate PolledMeanRate Double The polled messages mean rate Management over REST You are now able to view and manage Topics, Subscribers and Subscriber Groups using Management over REST API. For example, to retrieve the topics for a service you can use the following curl command replacing serviceName with your Topics service name. <markup lang=\"bash\" >curl http://host:port/management/coherence/cluster/services/serviceName/topics See REST API for Managing Oracle Coherence for full details of the available REST end points. Topics Management via CLI and VisualVM The Coherence VisualVM Plugin and Coherence CLI have been updated to provide management and monitoring of Topics within a Coherence cluster. See the following links for more information on each of the tools. Coherence CLI - See Coherence CLI on GitHub and CLI Command Reference . Coherence VisualVM Plugin - See Coherence VisualVM on GitHub Topics Grafana Dashboards There are four new Grafana Dashboards available to show Topics related information: Topics Summary Topic Details Topic Subscriber Details Topic Subscriber Group Details The above dashboards are available from the Coherence Operator GitHub repository. See the Oracle Metrics Documentation for more information on configuring metrics. ",
            "title": "Topics Management"
        },
        {
            "location": "/docs/core/05_response_caching",
            "text": " To use CDI Response Caching, you should first declare a coherence-cdi as a dependency in the project&#8217;s pom.xml file. <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Usage"
        },
        {
            "location": "/docs/core/05_response_caching",
            "text": " Method marked with @CacheAdd is always invoked, and its execution result stored in the cache. Key is made of the values of all parameters (in this case just the string parameter name ). <markup lang=\"java\" > @Path(\"{name}\") @POST @CacheAdd @CacheName(\"messages\") public Message addMessage(@PathParam(\"name\") String name) { return new Message(\"Hello \" + name); } ",
            "title": "@CacheAdd"
        },
        {
            "location": "/docs/core/05_response_caching",
            "text": " If the return value is present in the cache, it is fetched and returned. Otherwise, the target method is invoked, and the invocation result is stored in the cache and returned to the caller. <markup lang=\"java\" > @Path(\"{name}\") @GET @CacheGet @CacheName(\"messages\") public Message getMessage(@PathParam(\"name\") String name) { return new Message(\"Hello \" + name); } ",
            "title": "@CacheGet"
        },
        {
            "location": "/docs/core/05_response_caching",
            "text": " The value of the @CacheValue annotated parameter is stored in the cache, the target method is invoked, and the invocation result is returned to the caller. In this example, the passed message will be stored in the cache for the key whose value was passed as the name parameter. <markup lang=\"java\" > @Path(\"{name}\") @POST @Consumes(MediaType.APPLICATION_JSON) @CachePut @CacheName(\"messages\") public Response putMessage(@CacheKey @PathParam(\"name\") String name, @CacheValue Message message) { return Response.status(Response.Status.CREATED).build(); } ",
            "title": "@CachePut"
        },
        {
            "location": "/docs/core/05_response_caching",
            "text": " Removes the key from the cache and returns the result of the method invocation. In this example, the key whose value was passed as the name parameter will be removed from the cache. <markup lang=\"java\" > @Path(\"{name}\") @DELETE @CacheRemove public Response removeMessage(@PathParam(\"name\") String name) { return Response.ok().build(); } ",
            "title": "@CacheRemove"
        },
        {
            "location": "/docs/core/05_response_caching",
            "text": " The cache key is assembled from the values of all parameters not explicitly annotated with the @CacheValue annotation. If one or more parameters are annotated with the @CacheKey annotation, only those parameters will be used to create the key. In this example, only the values of the lastName and firstName parameters will be used to create the cache key. <markup lang=\"java\" > @Path(\"{lastName}/{firstName}\") @GET @CacheGet public Message getMessage(@PathParam(\"lastName\") @CacheKey String lastName, @PathParam(\"firstName\") @CacheKey String firstName, @HeaderParam(\"Accept-Language\") String acceptLanguage) { return new Message(\"Hello \" + firstName + \" \" + lastName); } ",
            "title": "@CacheKey"
        },
        {
            "location": "/docs/core/05_response_caching",
            "text": " The following response caching annotations are supported: The specific cache to be used for response caching can be declared by the @CacheName and @SessionName annotations on a class or method. @CacheAdd Method marked with @CacheAdd is always invoked, and its execution result stored in the cache. Key is made of the values of all parameters (in this case just the string parameter name ). <markup lang=\"java\" > @Path(\"{name}\") @POST @CacheAdd @CacheName(\"messages\") public Message addMessage(@PathParam(\"name\") String name) { return new Message(\"Hello \" + name); } @CacheGet If the return value is present in the cache, it is fetched and returned. Otherwise, the target method is invoked, and the invocation result is stored in the cache and returned to the caller. <markup lang=\"java\" > @Path(\"{name}\") @GET @CacheGet @CacheName(\"messages\") public Message getMessage(@PathParam(\"name\") String name) { return new Message(\"Hello \" + name); } @CachePut The value of the @CacheValue annotated parameter is stored in the cache, the target method is invoked, and the invocation result is returned to the caller. In this example, the passed message will be stored in the cache for the key whose value was passed as the name parameter. <markup lang=\"java\" > @Path(\"{name}\") @POST @Consumes(MediaType.APPLICATION_JSON) @CachePut @CacheName(\"messages\") public Response putMessage(@CacheKey @PathParam(\"name\") String name, @CacheValue Message message) { return Response.status(Response.Status.CREATED).build(); } @CacheRemove Removes the key from the cache and returns the result of the method invocation. In this example, the key whose value was passed as the name parameter will be removed from the cache. <markup lang=\"java\" > @Path(\"{name}\") @DELETE @CacheRemove public Response removeMessage(@PathParam(\"name\") String name) { return Response.ok().build(); } @CacheKey The cache key is assembled from the values of all parameters not explicitly annotated with the @CacheValue annotation. If one or more parameters are annotated with the @CacheKey annotation, only those parameters will be used to create the key. In this example, only the values of the lastName and firstName parameters will be used to create the cache key. <markup lang=\"java\" > @Path(\"{lastName}/{firstName}\") @GET @CacheGet public Message getMessage(@PathParam(\"lastName\") @CacheKey String lastName, @PathParam(\"firstName\") @CacheKey String firstName, @HeaderParam(\"Accept-Language\") String acceptLanguage) { return new Message(\"Hello \" + firstName + \" \" + lastName); } ",
            "title": "Response Caching Annotations"
        },
        {
            "location": "/docs/core/05_response_caching",
            "text": " CDI Response Caching allows you to apply caching to Java methods transparently. CDI Response Caching will be enabled once coherence-cdi dependency is added. Usage To use CDI Response Caching, you should first declare a coherence-cdi as a dependency in the project&#8217;s pom.xml file. <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;/dependency&gt; Response Caching Annotations The following response caching annotations are supported: The specific cache to be used for response caching can be declared by the @CacheName and @SessionName annotations on a class or method. @CacheAdd Method marked with @CacheAdd is always invoked, and its execution result stored in the cache. Key is made of the values of all parameters (in this case just the string parameter name ). <markup lang=\"java\" > @Path(\"{name}\") @POST @CacheAdd @CacheName(\"messages\") public Message addMessage(@PathParam(\"name\") String name) { return new Message(\"Hello \" + name); } @CacheGet If the return value is present in the cache, it is fetched and returned. Otherwise, the target method is invoked, and the invocation result is stored in the cache and returned to the caller. <markup lang=\"java\" > @Path(\"{name}\") @GET @CacheGet @CacheName(\"messages\") public Message getMessage(@PathParam(\"name\") String name) { return new Message(\"Hello \" + name); } @CachePut The value of the @CacheValue annotated parameter is stored in the cache, the target method is invoked, and the invocation result is returned to the caller. In this example, the passed message will be stored in the cache for the key whose value was passed as the name parameter. <markup lang=\"java\" > @Path(\"{name}\") @POST @Consumes(MediaType.APPLICATION_JSON) @CachePut @CacheName(\"messages\") public Response putMessage(@CacheKey @PathParam(\"name\") String name, @CacheValue Message message) { return Response.status(Response.Status.CREATED).build(); } @CacheRemove Removes the key from the cache and returns the result of the method invocation. In this example, the key whose value was passed as the name parameter will be removed from the cache. <markup lang=\"java\" > @Path(\"{name}\") @DELETE @CacheRemove public Response removeMessage(@PathParam(\"name\") String name) { return Response.ok().build(); } @CacheKey The cache key is assembled from the values of all parameters not explicitly annotated with the @CacheValue annotation. If one or more parameters are annotated with the @CacheKey annotation, only those parameters will be used to create the key. In this example, only the values of the lastName and firstName parameters will be used to create the cache key. <markup lang=\"java\" > @Path(\"{lastName}/{firstName}\") @GET @CacheGet public Message getMessage(@PathParam(\"lastName\") @CacheKey String lastName, @PathParam(\"firstName\") @CacheKey String firstName, @HeaderParam(\"Accept-Language\") String acceptLanguage) { return new Message(\"Hello \" + firstName + \" \" + lastName); } ",
            "title": "CDI Response Caching"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " What You Will Build What You Need Getting Started Follow the Tutorial Review the Initial Project Configure MicroProfile GraphQL Create Queries to Show Customer and Orders Inject Related Objects Add Mutations Add a Dynamic Where Clause Access Metrics Run the Completed Tutorial Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " You will build on an existing mock sample Coherence data model and create an application that will expose a GraphQL endpoint to perform various queries and mutations against the data model. If you wish to read more about GraphQL or Helidon&#8217;s support in GraphQL, please see this Medium post . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " About 30-45 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " This tutorial contains both the completed codebase as well as the initial state from which you build the complete the tutorial on. If you would like to run the completed example, please follow the instructions here otherwise continue below for the tutorial. ",
            "title": "Getting Started"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi , coherence-cdi-server and helidon-microprofile-metrics are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.metrics&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-metrics&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;io.smallrye&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@starwars.com', address='Address 3', balance=0.0} ===ORDERS=== .... ",
            "title": "Review the Initial Project"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. ",
            "title": "Configure MicroProfile GraphQL"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Ensure you have the project in tutorials/500-graphql/initial imported into your IDE. Review the Initial Project Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi , coherence-cdi-server and helidon-microprofile-metrics are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.metrics&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-metrics&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;io.smallrye&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@starwars.com', address='Address 3', balance=0.0} ===ORDERS=== .... Configure MicroProfile GraphQL Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. ",
            "title": "Follow the Tutorial"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Create the CustomerApi Class Firstly we need to create a class to expose our GraphQL endpoint. Create a new Class called CustomerApi in the package com.oracle.coherence.tutorials.graphql.api . Add the GraphQLApi annotation to mark this class as a GraphQL Endpoint and make it application scoped. <markup lang=\"java\" >@ApplicationScoped @GraphQLApi public class CustomerApi { Inject the Coherence `NamedMap`s for customers and orders <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; Add a Query to return all customers Add the following code to CustomerApi to create a query to return all customers: <markup lang=\"java\" >/** * Returns all of the {@link Customer}s. * * @return all of the {@link Customer}s. */ @Query @Description(\"Displays customers\") @Counted public Collection&lt;Customer&gt; getCustomers() { return customers.values(); } Include the @Counted microprofile metrics annotation to count the number of invocations Ensure you import the Query and Description annotations from org.eclipse.microprofile.graphql Build and run the project. Issue the following to display the automatically generated schema: <markup lang=\"bash\" >curl http://localhost:7001/graphql/schema.graphql type Customer { address: String balance: String! customerId: Int! email: String name: String orders: [Order] } type Query { \"Displays customers\" customers: [Customer] } Open the URL http://localhost:7001/ui . You should see the GraphiQL UI. Notice the Documentation Explorer on the right, which will allow you to explore the generated schema. Enter the following in the left-hand pane and click the Play button. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance } } This will result in the following JSON output: <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": 0 }, { \"customerId\": 4, \"name\": \"Tom Jones\", \"address\": \"Address 4\", \"email\": \"tom@jones.com\", \"balance\": 0 }, { \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": 100 }, { \"customerId\": 3, \"name\": \"John Williams\", \"address\": \"Address 3\", \"email\": \"john@starwars.com\", \"balance\": 0 } ] } } Add a Query to return all Orders Add the following code to CustomerApi to create a query to return all orders: <markup lang=\"java\" >@Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders() { return orders.values(); } Include the @Timed microprofile metrics annotation to time the query In this case we are overriding the default name for the query, which would be orders , with displayOrders . Stop the running project, rebuild and re-run. Refresh GraphiQL and enter the following in the left-hand pane and click the Play button and choose orders . <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } This will result in the following JSON output. The output below has been shortened. Notice that because we included the orderLines field and it is an object, then we must specify the individual fields to return. <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": 12163.024674447412, \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"Samsung TU8000 55 inch Crystal UHD 4K Smart TV [2020]\", \"itemCount\": 1, \"costPerItem\": 1695.3084188228172, \"orderLineTotal\": 1695.3084188228172 }, { \"lineNumber\": 4, \"productDescription\": \"Sony X7000G 49 inch 4k Ultra HD HDR Smart TV\", \"itemCount\": 2, \"costPerItem\": 2003.1246529714456, \"orderLineTotal\": 4006.249305942891 }, { \"lineNumber\": 3, \"productDescription\": \"TCL S615 40 inch Full HD Android TV\", \"itemCount\": 2, \"costPerItem\": 1171.4274805289924, \"orderLineTotal\": 2342.854961057985 }, { \"lineNumber\": 2, \"productDescription\": \"Samsung Q80T 85 inch QLED Ultra HD 4K Smart TV [2020]\", \"itemCount\": 2, \"costPerItem\": 2059.305994311859, \"orderLineTotal\": 4118.611988623718 } ] }, { \"orderId\": 102, \"customerId\": 2, ... Format currency fields We can see from the above output that a number of the currency fields are not formatted correctly. We will use the GraphQL annotation NumberFormat to format this as currency. You may also use the JsonbNumberFormat annotation as well. Add the NumberFormat to getBalance on the Customer class. <markup lang=\"java\" >/** * Returns the customer's balance. * * @return the customer's balance */ @NumberFormat(\"$###,##0.00\") public double getBalance() { return balance; } By adding the NumberFormat to the get method, the format will be applied to the output type only. If we add the NumberFormat to the set method it will be applied to the input type only. E.g. when Customer is used as a parameter. If it is added to the attribute it will apply to both input and output types. Add the NumberFormat to getOrderTotal on the Order class. <markup lang=\"java\" >/** * Returns the order total. * * @return the order total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderTotal() { return orderLines.stream().mapToDouble(OrderLine::getOrderLineTotal).sum(); } Add the NumberFormat to getCostPerItem and getOrderLineTotal on the OrderLine class. <markup lang=\"java\" >/** * Return the cost per item. * * @return the cost per item */ @NumberFormat(\"$###,###,##0.00\") public double getCostPerItem() { return costPerItem; } <markup lang=\"java\" >/** * Returns the order line total. * * @return he order line total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderLineTotal() { return itemCount * costPerItem; } Stop the running project, rebuild and re-run. Refresh GraphiQL and run the customers and orders queries and you will see the number values formatted as shown below: <markup lang=\"json\" >{ \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": \"$100.00\" } <markup lang=\"json\" >... \"orderTotal\": \"$13,029.54\", ... \"costPerItem\": \"$2,456.27\", \"orderLineTotal\": \"$2,456.27\" ",
            "title": "Create Queries to Show Customer and Orders"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " From the above output for orders, we can see we have customerId field only. It would be useful to also be able to return any attributes for the customer customer. Conversely it would be useful to be able to show the order details for a customer. We can achieve this using Coherence by making the class implement Injectable . When the class is deserialized on the client, any @Inject statements are processed, and we will use this to inject the NamedMap for customer and use to retrieve the customer details if required. Return the Customer for the Order Make the Order class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Order implements Serializable, Injectable { Inject the customer NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private transient NamedMap&lt;Integer, Customer&gt; customers; Finally, add the getCustomer method. <markup lang=\"java\" >/** * Returns the {@link Customer} for this {@link Order}. * * @return the {@link Customer} for this {@link Order} */ public Customer getCustomer() { return customers.get(customerId); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Order object in the Documentation Explorer . You will see a customer field that returns a Customer object. Change the orders query to the following and execute. You will notice the customers name and email returned. <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal customer { name email } orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } <markup lang=\"json\" > \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$7,946.81\", \"customer\": { \"name\": \"John Williams\", \"email\": \"john@starwars.com\" }, ... Return the Orders for a Customer Make the Customer class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Customer implements Serializable, Injectable { Inject the orders NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for orders. */ @Inject private transient NamedMap&lt;Integer, Order&gt; orders; Finally, add the getOrders method to get the orders for the current customer by specifying a Coherence filter. <markup lang=\"java\" >/** * Returns the {@link Order}s for a {@link Customer}. * * @return the {@link Order}s for a {@link Customer} */ public Collection&lt;Order&gt; getOrders() { return orders.values(Filters.equal(Order::getCustomerId, customerId)); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Customer object in the Documentation Explorer . You will see an orders field that returns an array of Customer objects. Change the customers query to add the orders for a customer and execute. You will notice the orders for the customers returned. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance orders { orderId orderDate orderTotal } } } <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": \"$0.00\", \"orders\": [ { \"orderId\": 100, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$1,572.23\" }, { \"orderId\": 101, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$2,201.91\" } ] }, ... ",
            "title": "Inject Related Objects"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " In this section we will add mutations to create or update data. Create a Customer Add the following to the CustomerApi class to create a customer: <markup lang=\"java\" >/** * Creates and saves a {@link Customer}. * * @param customer and saves a {@link Customer} * * @return the new {@link Customer} */ @Mutation @Timed public Customer createCustomer(@Name(\"customer\") Customer customer) { if (customers.containsKey(customer.getCustomerId())) { throw new IllegalArgumentException(\"Customer \" + customer.getCustomerId() + \" already exists\"); } customers.put(customer.getCustomerId(), customer); return customers.get(customer.getCustomerId()); } Include the @Timed microprofile metrics annotation to time the mutation In the above code we throw an IllegalArgumentException if the customer already exists. By default in the MicroProfile GraphQL specification, messages from unchecked exceptions are hidden from the client and \"Server Error\" is returned. In this case we have overridden this behaviour in the META-INF/microprofile-config.properties as shown below: <markup lang=\"java\" >mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException Checked exceptions, which we will show below will return the message back to the client by default and the message can be hidden as well if required. Stop the running project, rebuild and re-run. Refresh GraphiQL and create a fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } You can also update your existing customers query to use this fragment. Execute the following mutation: <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } <markup lang=\"json\" >{ \"data\": { \"createCustomer\": { \"customerId\": 12, \"name\": \"Tim\", \"address\": null, \"email\": null, \"balance\": \"$1,000.00\", \"orders\": [] } } } Making Attributes Mandatory If you execute the following query, you will notice that a customer is created with a null name. This is because in MP GraphQL any primitive is mandatory and all Objects are optional. Name is a String and therefore is optional. <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 11 balance: 1000}) { ...customer } } View the Documentation Explorer and note that the createCustomer mutation has the following schema: <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer CustomerInput has the following structure: <markup lang=\"graphql\" >input CustomerInput { address: String balance: Float! customerId: Int! email: String name: String orders: [OrderInput] } Add the NonNull annotation to the name field in the Customer object: <markup lang=\"java\" >/** * Name. */ @NonNull private String name; Stop the running project, rebuild and re-run. Refresh GraphiQL and try to execute the following mutation again. You will notice the UI will show an error indicating that name is now mandatory. <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer Create an Order Add the following to the CustomerApi class to create an order: <markup lang=\"java\" >/** * Creates and saves an {@link Order} for a given customer id. * * @param customerId customer id to create the {@link Order} for * @param orderId order id * * @return the new {@link Order} * * @throws CustomerNotFoundException if the {@link Customer} was not found */ @Mutation @Timed public Order createOrder(@Name(\"customerId\") int customerId, @Name(\"orderId\") int orderId) throws CustomerNotFoundException { if (!customers.containsKey(customerId)) { throw new CustomerNotFoundException(\"Customer id \" + customerId + \" was not found\"); } if (orders.containsKey(orderId)) { throw new IllegalArgumentException(\"Order \" + orderId + \" already exists\"); } Order order = new Order(orderId, customerId); orders.put(orderId, order); return orders.get(orderId); } Include the @Timed microprofile metrics annotation to time the mutation The validation ensures that we have a valid customer and the order id does not already exist. Create a new checked exception called CustomerNotFoundException in the api package. By default in MP GraphQL the messages from checked exceptions will be automatically returned to the client. <markup lang=\"java\" >public class CustomerNotFoundException extends Exception { /** * Constructs a new exception to indicate that a customer was not found. * * @param message the detail message. */ public CustomerNotFoundException(String message) { super(message); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and add the following fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } You can also update the orders query to use the new fragment: <markup lang=\"graphql\" >query orders { displayOrders { ...order } } Try to create an order with a non-existent customer number 12. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 100) { ...order } } This shows the following message from the CustomerNotFoundException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Customer id 12 was not found\" } ] } Try to create an order with an already existing order id 100. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 100) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Order 100 already exists\" } ] } Create a new order with valid values: <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } Add an OrderLine to an Order Add the following to the CustomerApi class to add an OrderLine to an Order: <markup lang=\"java\" >/** * Adds an {@link OrderLine} to an existing {@link Order}. * * @param orderId order id to add to * @param orderLine {@link OrderLine} to add * * @return the updates {@link Order} * * @throws OrderNotFoundException the {@link Order} was not found */ @Mutation @Timed public Order addOrderLineToOrder(@Name(\"orderId\") int orderId, @Name(\"orderLine\") OrderLine orderLine) throws OrderNotFoundException { if (!orders.containsKey(orderId)) { throw new OrderNotFoundException(\"Order number \" + orderId + \" was not found\"); } if (orderLine.getProductDescription() == null || orderLine.getProductDescription().equals(\"\") || orderLine.getItemCount() &lt;= 0 || orderLine.getCostPerItem() &lt;= 0) { throw new IllegalArgumentException(\"Supplied Order Line is invalid: \" + orderLine); } return orders.compute(orderId, (k, v)-&gt;{ v.addOrderLine(orderLine); return v; }); } Include the @Timed microprofile metrics annotation to time the mutation Create a new checked exception called OrderNotFoundException in the api package. <markup lang=\"java\" >public class OrderNotFoundException extends Exception { /** * Constructs a new exception to indicate that an order was not found. * * @param message the detail message. */ public OrderNotFoundException(String message) { super(message); } } To make input easier, we can add DefaultValue annotations to the setLineNumber method and setItemCount methods in the OrderLine` class. Ensure you import DefaultValue from the org.eclipse.microprofile.graphql package. <markup lang=\"java\" >@DefaultValue(\"1\") public void setLineNumber(int lineNumber) { this.lineNumber = lineNumber; } <markup lang=\"java\" >@DefaultValue(\"1\") public void setItemCount(int itemCount) { this.itemCount = itemCount; } By placing the DefaultValue on the setter methods only, it applies to input types only. If we wanted the DefaultValue to apply to output type only we would apply to the getters. If we wish to appy to both input and output we can place on the field. Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the OrderLineInput object in the Documentation Explorer . You will see the default values applied. They are also no longer mandatory as they have a default value. <markup lang=\"graphql\" >lineNumber: Int = 1 itemCount: Int = 1 Create a new order 200 for customer 1 and then add a new order line. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } This shows the following output for the new order. <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } And the result of the new order line. <markup lang=\"json\" >{ \"data\": { \"addOrderLineToOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$1,500.00\", \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"iPhone 12\", \"itemCount\": 1, \"costPerItem\": \"$1,500.00\", \"orderLineTotal\": \"$1,500.00\" } ] } } } Experiment with invalid order id and customer id as input. ",
            "title": "Add Mutations"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Finally, we will enhance the orders query and add a dynamic where clause. Update the getOrders method in the CustomerApi to add the where clause and pass this to the QuerHelper to generate the Coherence Filter . The code will ask return an error message if the where clause is invalid. <markup lang=\"java\" >/** * Returns {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null. * * @param whereClause where clause to restrict selection of {@link Order}s * * @return {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null */ @Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders(@Name(\"whereClause\") String whereClause) { try { Filter filter = whereClause == null ? Filters.always() : QueryHelper.createFilter(whereClause); return orders.values(filter); } catch (Exception e) { throw new IllegalArgumentException(\"Invalid where clause: [\" + whereClause + \"]\"); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and execute the following query to find all orders with a orderTotal greater than $4000. <markup lang=\"graphql\" >query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } }, { \"orderId\": 105, \"orderTotal\": \"$4,629.24\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } }, { \"orderId\": 104, \"orderTotal\": \"$8,078.11\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } } ] } } Use a more complex where clause: <markup lang=\"graphql\" >query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } } ] } } ",
            "title": "Add a Dynamic Where Clause"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " As we can see from the above examples, metrics can be easily enabled for queries and mutations by including the @Counted or @Timed annotations. After running a number of queries and mutations you can access the metrics end point using the following curl command: The base metrics endpoint is http://localhost:7001/metrics , but we have added the /application path to restrict the metrics returned. <markup lang=\"bash\" >curl -H 'Accept: application/json' http://127.0.0.1:7001/metrics/application | jq { \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.addOrderLineToOrder\": { \"count\": 1, \"meanRate\": 0.020786416474669184, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 63082260, \"max\": 63082260, \"mean\": 63082260, \"stddev\": 0, \"p50\": 63082260, \"p75\": 63082260, \"p95\": 63082260, \"p98\": 63082260, \"p99\": 63082260, \"p999\": 63082260 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createCustomer\": { \"count\": 1, \"meanRate\": 0.02078651489493201, \"oneMinRate\": 0.013536188363841833, \"fiveMinRate\": 0.0031973351962583784, \"fifteenMinRate\": 0.001095787094460976, \"min\": 4184923, \"max\": 4184923, \"mean\": 4184923, \"stddev\": 0, \"p50\": 4184923, \"p75\": 4184923, \"p95\": 4184923, \"p98\": 4184923, \"p99\": 4184923, \"p999\": 4184923 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createOrder\": { \"count\": 1, \"meanRate\": 0.020786437087696893, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 5411268, \"max\": 5411268, \"mean\": 5411268, \"stddev\": 0, \"p50\": 5411268, \"p75\": 5411268, \"p95\": 5411268, \"p98\": 5411268, \"p99\": 5411268, \"p999\": 5411268 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getCustomers\": 1, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getOrders\": { \"count\": 3, \"meanRate\": 0.06235852925082789, \"oneMinRate\": 0.04423984338571901, \"fiveMinRate\": 0.009754115099857198, \"fifteenMinRate\": 0.003305709235676515, \"min\": 6507371, \"max\": 47080043, \"mean\": 20945553.135424484, \"stddev\": 19245930.056725293, \"p50\": 7014199, \"p75\": 47080043, \"p95\": 47080043, \"p98\": 47080043, \"p99\": 47080043, \"p999\": 47080043 } } jq has been used to format the JSON output. This can be downloaded from https://stedolan.github.io/jq/download/ or you can format the output with an alternate utility. ",
            "title": "Access Metrics"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw exec:exec Run with Gradle <markup lang=\"bash\" >./gradlew runApp ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } ",
            "title": "Run the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Building the Example Code As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw exec:exec Run with Gradle <markup lang=\"bash\" >./gradlew runApp Run the Example Code Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } ",
            "title": "Run the Completed Tutorial"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " In this tutorial you have seen how easy it is to expose Coherence Data using GraphQL. ",
            "title": "Summary"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Helidon MP Documentation Microprofile GraphQL Specification ",
            "title": "See Also"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " This tutorial walks through the steps to enable access to Coherence data from GraphQL using Helidon’s MicroProfile (MP) GraphQL support and Coherence CDI . Table of Contents What You Will Build What You Need Getting Started Follow the Tutorial Review the Initial Project Configure MicroProfile GraphQL Create Queries to Show Customer and Orders Inject Related Objects Add Mutations Add a Dynamic Where Clause Access Metrics Run the Completed Tutorial Summary See Also What You Will Build You will build on an existing mock sample Coherence data model and create an application that will expose a GraphQL endpoint to perform various queries and mutations against the data model. If you wish to read more about GraphQL or Helidon&#8217;s support in GraphQL, please see this Medium post . What You Need About 30-45 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Getting Started This tutorial contains both the completed codebase as well as the initial state from which you build the complete the tutorial on. If you would like to run the completed example, please follow the instructions here otherwise continue below for the tutorial. Follow the Tutorial Ensure you have the project in tutorials/500-graphql/initial imported into your IDE. Review the Initial Project Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi , coherence-cdi-server and helidon-microprofile-metrics are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.metrics&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-metrics&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;io.smallrye&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@starwars.com', address='Address 3', balance=0.0} ===ORDERS=== .... Configure MicroProfile GraphQL Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. Create Queries to Show Customer and Orders Create the CustomerApi Class Firstly we need to create a class to expose our GraphQL endpoint. Create a new Class called CustomerApi in the package com.oracle.coherence.tutorials.graphql.api . Add the GraphQLApi annotation to mark this class as a GraphQL Endpoint and make it application scoped. <markup lang=\"java\" >@ApplicationScoped @GraphQLApi public class CustomerApi { Inject the Coherence `NamedMap`s for customers and orders <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; Add a Query to return all customers Add the following code to CustomerApi to create a query to return all customers: <markup lang=\"java\" >/** * Returns all of the {@link Customer}s. * * @return all of the {@link Customer}s. */ @Query @Description(\"Displays customers\") @Counted public Collection&lt;Customer&gt; getCustomers() { return customers.values(); } Include the @Counted microprofile metrics annotation to count the number of invocations Ensure you import the Query and Description annotations from org.eclipse.microprofile.graphql Build and run the project. Issue the following to display the automatically generated schema: <markup lang=\"bash\" >curl http://localhost:7001/graphql/schema.graphql type Customer { address: String balance: String! customerId: Int! email: String name: String orders: [Order] } type Query { \"Displays customers\" customers: [Customer] } Open the URL http://localhost:7001/ui . You should see the GraphiQL UI. Notice the Documentation Explorer on the right, which will allow you to explore the generated schema. Enter the following in the left-hand pane and click the Play button. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance } } This will result in the following JSON output: <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": 0 }, { \"customerId\": 4, \"name\": \"Tom Jones\", \"address\": \"Address 4\", \"email\": \"tom@jones.com\", \"balance\": 0 }, { \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": 100 }, { \"customerId\": 3, \"name\": \"John Williams\", \"address\": \"Address 3\", \"email\": \"john@starwars.com\", \"balance\": 0 } ] } } Add a Query to return all Orders Add the following code to CustomerApi to create a query to return all orders: <markup lang=\"java\" >@Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders() { return orders.values(); } Include the @Timed microprofile metrics annotation to time the query In this case we are overriding the default name for the query, which would be orders , with displayOrders . Stop the running project, rebuild and re-run. Refresh GraphiQL and enter the following in the left-hand pane and click the Play button and choose orders . <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } This will result in the following JSON output. The output below has been shortened. Notice that because we included the orderLines field and it is an object, then we must specify the individual fields to return. <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": 12163.024674447412, \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"Samsung TU8000 55 inch Crystal UHD 4K Smart TV [2020]\", \"itemCount\": 1, \"costPerItem\": 1695.3084188228172, \"orderLineTotal\": 1695.3084188228172 }, { \"lineNumber\": 4, \"productDescription\": \"Sony X7000G 49 inch 4k Ultra HD HDR Smart TV\", \"itemCount\": 2, \"costPerItem\": 2003.1246529714456, \"orderLineTotal\": 4006.249305942891 }, { \"lineNumber\": 3, \"productDescription\": \"TCL S615 40 inch Full HD Android TV\", \"itemCount\": 2, \"costPerItem\": 1171.4274805289924, \"orderLineTotal\": 2342.854961057985 }, { \"lineNumber\": 2, \"productDescription\": \"Samsung Q80T 85 inch QLED Ultra HD 4K Smart TV [2020]\", \"itemCount\": 2, \"costPerItem\": 2059.305994311859, \"orderLineTotal\": 4118.611988623718 } ] }, { \"orderId\": 102, \"customerId\": 2, ... Format currency fields We can see from the above output that a number of the currency fields are not formatted correctly. We will use the GraphQL annotation NumberFormat to format this as currency. You may also use the JsonbNumberFormat annotation as well. Add the NumberFormat to getBalance on the Customer class. <markup lang=\"java\" >/** * Returns the customer's balance. * * @return the customer's balance */ @NumberFormat(\"$###,##0.00\") public double getBalance() { return balance; } By adding the NumberFormat to the get method, the format will be applied to the output type only. If we add the NumberFormat to the set method it will be applied to the input type only. E.g. when Customer is used as a parameter. If it is added to the attribute it will apply to both input and output types. Add the NumberFormat to getOrderTotal on the Order class. <markup lang=\"java\" >/** * Returns the order total. * * @return the order total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderTotal() { return orderLines.stream().mapToDouble(OrderLine::getOrderLineTotal).sum(); } Add the NumberFormat to getCostPerItem and getOrderLineTotal on the OrderLine class. <markup lang=\"java\" >/** * Return the cost per item. * * @return the cost per item */ @NumberFormat(\"$###,###,##0.00\") public double getCostPerItem() { return costPerItem; } <markup lang=\"java\" >/** * Returns the order line total. * * @return he order line total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderLineTotal() { return itemCount * costPerItem; } Stop the running project, rebuild and re-run. Refresh GraphiQL and run the customers and orders queries and you will see the number values formatted as shown below: <markup lang=\"json\" >{ \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": \"$100.00\" } <markup lang=\"json\" >... \"orderTotal\": \"$13,029.54\", ... \"costPerItem\": \"$2,456.27\", \"orderLineTotal\": \"$2,456.27\" Inject Related Objects From the above output for orders, we can see we have customerId field only. It would be useful to also be able to return any attributes for the customer customer. Conversely it would be useful to be able to show the order details for a customer. We can achieve this using Coherence by making the class implement Injectable . When the class is deserialized on the client, any @Inject statements are processed, and we will use this to inject the NamedMap for customer and use to retrieve the customer details if required. Return the Customer for the Order Make the Order class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Order implements Serializable, Injectable { Inject the customer NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private transient NamedMap&lt;Integer, Customer&gt; customers; Finally, add the getCustomer method. <markup lang=\"java\" >/** * Returns the {@link Customer} for this {@link Order}. * * @return the {@link Customer} for this {@link Order} */ public Customer getCustomer() { return customers.get(customerId); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Order object in the Documentation Explorer . You will see a customer field that returns a Customer object. Change the orders query to the following and execute. You will notice the customers name and email returned. <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal customer { name email } orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } <markup lang=\"json\" > \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$7,946.81\", \"customer\": { \"name\": \"John Williams\", \"email\": \"john@starwars.com\" }, ... Return the Orders for a Customer Make the Customer class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Customer implements Serializable, Injectable { Inject the orders NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for orders. */ @Inject private transient NamedMap&lt;Integer, Order&gt; orders; Finally, add the getOrders method to get the orders for the current customer by specifying a Coherence filter. <markup lang=\"java\" >/** * Returns the {@link Order}s for a {@link Customer}. * * @return the {@link Order}s for a {@link Customer} */ public Collection&lt;Order&gt; getOrders() { return orders.values(Filters.equal(Order::getCustomerId, customerId)); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Customer object in the Documentation Explorer . You will see an orders field that returns an array of Customer objects. Change the customers query to add the orders for a customer and execute. You will notice the orders for the customers returned. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance orders { orderId orderDate orderTotal } } } <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": \"$0.00\", \"orders\": [ { \"orderId\": 100, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$1,572.23\" }, { \"orderId\": 101, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$2,201.91\" } ] }, ... Add Mutations In this section we will add mutations to create or update data. Create a Customer Add the following to the CustomerApi class to create a customer: <markup lang=\"java\" >/** * Creates and saves a {@link Customer}. * * @param customer and saves a {@link Customer} * * @return the new {@link Customer} */ @Mutation @Timed public Customer createCustomer(@Name(\"customer\") Customer customer) { if (customers.containsKey(customer.getCustomerId())) { throw new IllegalArgumentException(\"Customer \" + customer.getCustomerId() + \" already exists\"); } customers.put(customer.getCustomerId(), customer); return customers.get(customer.getCustomerId()); } Include the @Timed microprofile metrics annotation to time the mutation In the above code we throw an IllegalArgumentException if the customer already exists. By default in the MicroProfile GraphQL specification, messages from unchecked exceptions are hidden from the client and \"Server Error\" is returned. In this case we have overridden this behaviour in the META-INF/microprofile-config.properties as shown below: <markup lang=\"java\" >mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException Checked exceptions, which we will show below will return the message back to the client by default and the message can be hidden as well if required. Stop the running project, rebuild and re-run. Refresh GraphiQL and create a fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } You can also update your existing customers query to use this fragment. Execute the following mutation: <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } <markup lang=\"json\" >{ \"data\": { \"createCustomer\": { \"customerId\": 12, \"name\": \"Tim\", \"address\": null, \"email\": null, \"balance\": \"$1,000.00\", \"orders\": [] } } } Making Attributes Mandatory If you execute the following query, you will notice that a customer is created with a null name. This is because in MP GraphQL any primitive is mandatory and all Objects are optional. Name is a String and therefore is optional. <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 11 balance: 1000}) { ...customer } } View the Documentation Explorer and note that the createCustomer mutation has the following schema: <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer CustomerInput has the following structure: <markup lang=\"graphql\" >input CustomerInput { address: String balance: Float! customerId: Int! email: String name: String orders: [OrderInput] } Add the NonNull annotation to the name field in the Customer object: <markup lang=\"java\" >/** * Name. */ @NonNull private String name; Stop the running project, rebuild and re-run. Refresh GraphiQL and try to execute the following mutation again. You will notice the UI will show an error indicating that name is now mandatory. <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer Create an Order Add the following to the CustomerApi class to create an order: <markup lang=\"java\" >/** * Creates and saves an {@link Order} for a given customer id. * * @param customerId customer id to create the {@link Order} for * @param orderId order id * * @return the new {@link Order} * * @throws CustomerNotFoundException if the {@link Customer} was not found */ @Mutation @Timed public Order createOrder(@Name(\"customerId\") int customerId, @Name(\"orderId\") int orderId) throws CustomerNotFoundException { if (!customers.containsKey(customerId)) { throw new CustomerNotFoundException(\"Customer id \" + customerId + \" was not found\"); } if (orders.containsKey(orderId)) { throw new IllegalArgumentException(\"Order \" + orderId + \" already exists\"); } Order order = new Order(orderId, customerId); orders.put(orderId, order); return orders.get(orderId); } Include the @Timed microprofile metrics annotation to time the mutation The validation ensures that we have a valid customer and the order id does not already exist. Create a new checked exception called CustomerNotFoundException in the api package. By default in MP GraphQL the messages from checked exceptions will be automatically returned to the client. <markup lang=\"java\" >public class CustomerNotFoundException extends Exception { /** * Constructs a new exception to indicate that a customer was not found. * * @param message the detail message. */ public CustomerNotFoundException(String message) { super(message); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and add the following fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } You can also update the orders query to use the new fragment: <markup lang=\"graphql\" >query orders { displayOrders { ...order } } Try to create an order with a non-existent customer number 12. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 100) { ...order } } This shows the following message from the CustomerNotFoundException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Customer id 12 was not found\" } ] } Try to create an order with an already existing order id 100. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 100) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Order 100 already exists\" } ] } Create a new order with valid values: <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } Add an OrderLine to an Order Add the following to the CustomerApi class to add an OrderLine to an Order: <markup lang=\"java\" >/** * Adds an {@link OrderLine} to an existing {@link Order}. * * @param orderId order id to add to * @param orderLine {@link OrderLine} to add * * @return the updates {@link Order} * * @throws OrderNotFoundException the {@link Order} was not found */ @Mutation @Timed public Order addOrderLineToOrder(@Name(\"orderId\") int orderId, @Name(\"orderLine\") OrderLine orderLine) throws OrderNotFoundException { if (!orders.containsKey(orderId)) { throw new OrderNotFoundException(\"Order number \" + orderId + \" was not found\"); } if (orderLine.getProductDescription() == null || orderLine.getProductDescription().equals(\"\") || orderLine.getItemCount() &lt;= 0 || orderLine.getCostPerItem() &lt;= 0) { throw new IllegalArgumentException(\"Supplied Order Line is invalid: \" + orderLine); } return orders.compute(orderId, (k, v)-&gt;{ v.addOrderLine(orderLine); return v; }); } Include the @Timed microprofile metrics annotation to time the mutation Create a new checked exception called OrderNotFoundException in the api package. <markup lang=\"java\" >public class OrderNotFoundException extends Exception { /** * Constructs a new exception to indicate that an order was not found. * * @param message the detail message. */ public OrderNotFoundException(String message) { super(message); } } To make input easier, we can add DefaultValue annotations to the setLineNumber method and setItemCount methods in the OrderLine` class. Ensure you import DefaultValue from the org.eclipse.microprofile.graphql package. <markup lang=\"java\" >@DefaultValue(\"1\") public void setLineNumber(int lineNumber) { this.lineNumber = lineNumber; } <markup lang=\"java\" >@DefaultValue(\"1\") public void setItemCount(int itemCount) { this.itemCount = itemCount; } By placing the DefaultValue on the setter methods only, it applies to input types only. If we wanted the DefaultValue to apply to output type only we would apply to the getters. If we wish to appy to both input and output we can place on the field. Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the OrderLineInput object in the Documentation Explorer . You will see the default values applied. They are also no longer mandatory as they have a default value. <markup lang=\"graphql\" >lineNumber: Int = 1 itemCount: Int = 1 Create a new order 200 for customer 1 and then add a new order line. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } This shows the following output for the new order. <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } And the result of the new order line. <markup lang=\"json\" >{ \"data\": { \"addOrderLineToOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$1,500.00\", \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"iPhone 12\", \"itemCount\": 1, \"costPerItem\": \"$1,500.00\", \"orderLineTotal\": \"$1,500.00\" } ] } } } Experiment with invalid order id and customer id as input. Add a Dynamic Where Clause Finally, we will enhance the orders query and add a dynamic where clause. Update the getOrders method in the CustomerApi to add the where clause and pass this to the QuerHelper to generate the Coherence Filter . The code will ask return an error message if the where clause is invalid. <markup lang=\"java\" >/** * Returns {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null. * * @param whereClause where clause to restrict selection of {@link Order}s * * @return {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null */ @Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders(@Name(\"whereClause\") String whereClause) { try { Filter filter = whereClause == null ? Filters.always() : QueryHelper.createFilter(whereClause); return orders.values(filter); } catch (Exception e) { throw new IllegalArgumentException(\"Invalid where clause: [\" + whereClause + \"]\"); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and execute the following query to find all orders with a orderTotal greater than $4000. <markup lang=\"graphql\" >query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } }, { \"orderId\": 105, \"orderTotal\": \"$4,629.24\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } }, { \"orderId\": 104, \"orderTotal\": \"$8,078.11\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } } ] } } Use a more complex where clause: <markup lang=\"graphql\" >query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } } ] } } Access Metrics As we can see from the above examples, metrics can be easily enabled for queries and mutations by including the @Counted or @Timed annotations. After running a number of queries and mutations you can access the metrics end point using the following curl command: The base metrics endpoint is http://localhost:7001/metrics , but we have added the /application path to restrict the metrics returned. <markup lang=\"bash\" >curl -H 'Accept: application/json' http://127.0.0.1:7001/metrics/application | jq { \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.addOrderLineToOrder\": { \"count\": 1, \"meanRate\": 0.020786416474669184, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 63082260, \"max\": 63082260, \"mean\": 63082260, \"stddev\": 0, \"p50\": 63082260, \"p75\": 63082260, \"p95\": 63082260, \"p98\": 63082260, \"p99\": 63082260, \"p999\": 63082260 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createCustomer\": { \"count\": 1, \"meanRate\": 0.02078651489493201, \"oneMinRate\": 0.013536188363841833, \"fiveMinRate\": 0.0031973351962583784, \"fifteenMinRate\": 0.001095787094460976, \"min\": 4184923, \"max\": 4184923, \"mean\": 4184923, \"stddev\": 0, \"p50\": 4184923, \"p75\": 4184923, \"p95\": 4184923, \"p98\": 4184923, \"p99\": 4184923, \"p999\": 4184923 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createOrder\": { \"count\": 1, \"meanRate\": 0.020786437087696893, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 5411268, \"max\": 5411268, \"mean\": 5411268, \"stddev\": 0, \"p50\": 5411268, \"p75\": 5411268, \"p95\": 5411268, \"p98\": 5411268, \"p99\": 5411268, \"p999\": 5411268 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getCustomers\": 1, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getOrders\": { \"count\": 3, \"meanRate\": 0.06235852925082789, \"oneMinRate\": 0.04423984338571901, \"fiveMinRate\": 0.009754115099857198, \"fifteenMinRate\": 0.003305709235676515, \"min\": 6507371, \"max\": 47080043, \"mean\": 20945553.135424484, \"stddev\": 19245930.056725293, \"p50\": 7014199, \"p75\": 47080043, \"p95\": 47080043, \"p98\": 47080043, \"p99\": 47080043, \"p999\": 47080043 } } jq has been used to format the JSON output. This can be downloaded from https://stedolan.github.io/jq/download/ or you can format the output with an alternate utility. Run the Completed Tutorial Building the Example Code As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw exec:exec Run with Gradle <markup lang=\"bash\" >./gradlew runApp Run the Example Code Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } Summary In this tutorial you have seen how easy it is to expose Coherence Data using GraphQL. See Also Helidon MP Documentation Microprofile GraphQL Specification ",
            "title": "GraphQL"
        },
        {
            "location": "/examples/tutorials/000-overview",
            "text": " These tutorials provide a deeper understanding of larger Coherence features and concepts that cannot be usually be explained with a few simple code snippets. They might, for example, require a running Coherence cluster to properly show a feature. Tutorials are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. GraphQL This tutorial shows you how to access Coherence Data using GraphQL. Persistence This tutorial shows you how to use Persistence from CohQL and how to monitor Persistence events. ",
            "title": "Tutorials"
        },
        {
            "location": "/coherence-grpc/README",
            "text": "",
            "title": "Coherence gRPC"
        },
        {
            "location": "/coherence-grpc/README",
            "text": "",
            "title": "Developing Remote Clients for Oracle Coherence"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Coherence gRPC for Java allows Java applications to access Coherence clustered services, including data, data events, and data processing from outside the Coherence cluster. Typical uses for Java gRPC clients include desktop and Web applications that require access to remote Coherence resources. This provides an alternative to using Coherence*Extend when writing client applications. Note The Coherence gRPC client and Coherence Extend client feature sets do not match exactly, some functionality in gRPC is not available in Extend and vice-versa. The Coherence gRPC for Java library connects to a Coherence clustered service instance running within the Coherence cluster using a high performance gRPC based communication layer. This library sends all client requests to the Coherence clustered gRPC proxy service which, in turn, responds to client requests by delegating to an actual Coherence clustered service (for example, a partitioned cache service). Like cache clients that are members of the cluster, Java gRPC clients use the Session API call to retrieve a resources such as NamedMap , NamedCache , etc. After it is obtained, a client accesses these resources in the same way as it would if it were part of the Coherence cluster. The fact that operations on Coherence resources are being sent to a remote cluster node (over gRPC) is completely transparent to the client application. There are two parts to Coherence gRPC, the coherence-grpc-proxy module, that provides the server-side gRPC proxy, and the coherence-java-client module that provides the gRPC client. Other non-java Coherence clients are also available that use the Coherence gRPC protocol. ",
            "title": "24 Introduction to gRPC"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " To set up and start using the Coherence gRPC Server, you should declare it as a dependency of your project. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy\" } ",
            "title": "Setting Up the Coherence gRPC Proxy Server"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The gRPC server starts automatically when you run com.tangosol.coherence.net.Coherence (or com.tangosol.coherence.net.DefaultCacheServer ). Typically, com.tangosol.coherence.net.Coherence class should be used as the application’s main class. Alternatively, you can start an instance of com.tangosol.coherence.net.Coherence by using the Bootstrap API. By default, the gRPC server will listen on all local addresses using an ephemeral port. Just like with Coherence*Extend, the endpoints the gRPC server has bound to can be discovered by a client using the Coherence NameService, so using ephemeral ports allows the server to start without needing to be concerned with port clashes. When reviewing the log output, two log messages appear as shown below to indicate which ports the gRPC server has bound to. <markup >In-Process GrpcAcceptor is now listening for connections using name \"default\" GrpcAcceptor now listening for connections on 0.0.0.0:55550 The service is ready to process requests from one of the Coherence gRPC client implementations. ",
            "title": "Starting the Server"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The address and port that the gRPC server binds to when starting can be configured at runtime by setting system properties or environment variables. By default, the server binds to the address 0.0.0.0 which equates to all the local host&#8217;s network interfaces. This can be changed by setting the coherence.grpc.server.address system property or COHERENCE_GRPC_SERVER_ADDRESS environment variable. For example, if the host had a local IP address 192.168.0.25 the server could be configured to bind to just this address as follows: Using System properties <markup >-Dcoherence.grpc.server.address=192.168.0.2 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_ADDRESS=192.168.0.2 The port that the gRPC server binds to can be configured using the coherence.grpc.server.port system property or COHERENCE_GRPC_SERVER_PORT environment variable For example, to configure the server to listen on port 1408: Using System properties <markup >-Dcoherence.grpc.server.port=1408 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_PORT=1408 ",
            "title": "Configuring the gRPC Server Listen Address and Port"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " In common with the rest of Coherence, the Coherence gRPC server can be configured to use SSL by specifying the name of a socket provider. Named socket providers are configured in the Coherence operational configuration file (override file). There are various ways to configure an SSL socket provider, which are covered in the Coherence documentation section Using SSL to Secure Communication Once a named socket provider has been configured, the gRPC server can be configured to use that provider by setting the coherence.grpc.server.socketprovider system property or COHERENCE_GRPC_SERVER_SOCKETPROVIDER environment variable. For example, if a socket provider named tls has been configured in the operational configuration file, the gRPC server can be configured to use it: <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" > &lt;socket-providers&gt; &lt;socket-provider id=\"tls\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"coherence.security.key\"&gt;server.key&lt;/key&gt; &lt;cert system-property=\"coherence.security.cert\"&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"coherence.security.ca.cert\"&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; Using System properties <markup >-Dcoherence.grpc.server.socketprovider=tls Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_SOCKETPROVIDER=tls For more information on socket providers see Using SSL to Secure Communication ",
            "title": "Configuring SSL/TLS"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Like other Coherence services, the gRPC server uses a dynamically sized thread pool to process requests. The thread pool size can be configured if the dynamic sizing algorithm provies to not be optimal. Set the Minimum Thread Count Adjusting the minimum number of threads can be useful when dealing with bursts in load. Sometimes it can take the dynamic pool some time to increase the thread count to a suitable number to quickly deal with an increase in load. Setting the minimum size will ensure there are always a certain number of threads to service load. The minimum number of threads in the pool can be set using the coherence.grpc.server.threads.min system property, or the COHERENCE_GRPC_SERVER_THREADS_MIN environment variable. For example, the minimum thread count can be set to 10 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.min=10 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MIN=10 Set the Maximum Thread Count Adjusting the maximum number of threads can be useful to stop the dynamic pool going too high and consuming too much CPU resource. The maximum number of threads in the pool can be set using the coherence.grpc.server.threads.max system property, or the COHERENCE_GRPC_SERVER_THREADS_MAX environment variable. If both maximum and minimum thread counts are specified, the maximum thread count should obviously be set to a value higher than the minimum thread count. For example, the maximum thread count can be set to 20 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.max=20 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MAX=20 ",
            "title": "Configuring the gRPC Server Thread Pool"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The Coherence gRPC proxy is configured using an internal default cache configuration file named grpc-proxy-cache-config.xml which only contains a single &lt;proxy-scheme&gt; configuration for the gRPC proxy. There is no reason to override this file as the server can be configured with System properties and environment variables. Configuring the gRPC Server Listen Address and Port The address and port that the gRPC server binds to when starting can be configured at runtime by setting system properties or environment variables. By default, the server binds to the address 0.0.0.0 which equates to all the local host&#8217;s network interfaces. This can be changed by setting the coherence.grpc.server.address system property or COHERENCE_GRPC_SERVER_ADDRESS environment variable. For example, if the host had a local IP address 192.168.0.25 the server could be configured to bind to just this address as follows: Using System properties <markup >-Dcoherence.grpc.server.address=192.168.0.2 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_ADDRESS=192.168.0.2 The port that the gRPC server binds to can be configured using the coherence.grpc.server.port system property or COHERENCE_GRPC_SERVER_PORT environment variable For example, to configure the server to listen on port 1408: Using System properties <markup >-Dcoherence.grpc.server.port=1408 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_PORT=1408 Configuring SSL/TLS In common with the rest of Coherence, the Coherence gRPC server can be configured to use SSL by specifying the name of a socket provider. Named socket providers are configured in the Coherence operational configuration file (override file). There are various ways to configure an SSL socket provider, which are covered in the Coherence documentation section Using SSL to Secure Communication Once a named socket provider has been configured, the gRPC server can be configured to use that provider by setting the coherence.grpc.server.socketprovider system property or COHERENCE_GRPC_SERVER_SOCKETPROVIDER environment variable. For example, if a socket provider named tls has been configured in the operational configuration file, the gRPC server can be configured to use it: <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" > &lt;socket-providers&gt; &lt;socket-provider id=\"tls\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"coherence.security.key\"&gt;server.key&lt;/key&gt; &lt;cert system-property=\"coherence.security.cert\"&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"coherence.security.ca.cert\"&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; Using System properties <markup >-Dcoherence.grpc.server.socketprovider=tls Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_SOCKETPROVIDER=tls For more information on socket providers see Using SSL to Secure Communication Configuring the gRPC Server Thread Pool Like other Coherence services, the gRPC server uses a dynamically sized thread pool to process requests. The thread pool size can be configured if the dynamic sizing algorithm provies to not be optimal. Set the Minimum Thread Count Adjusting the minimum number of threads can be useful when dealing with bursts in load. Sometimes it can take the dynamic pool some time to increase the thread count to a suitable number to quickly deal with an increase in load. Setting the minimum size will ensure there are always a certain number of threads to service load. The minimum number of threads in the pool can be set using the coherence.grpc.server.threads.min system property, or the COHERENCE_GRPC_SERVER_THREADS_MIN environment variable. For example, the minimum thread count can be set to 10 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.min=10 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MIN=10 Set the Maximum Thread Count Adjusting the maximum number of threads can be useful to stop the dynamic pool going too high and consuming too much CPU resource. The maximum number of threads in the pool can be set using the coherence.grpc.server.threads.max system property, or the COHERENCE_GRPC_SERVER_THREADS_MAX environment variable. If both maximum and minimum thread counts are specified, the maximum thread count should obviously be set to a value higher than the minimum thread count. For example, the maximum thread count can be set to 20 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.max=20 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MAX=20 ",
            "title": "Configuring the Server"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " If the coherence-grpc-proxy module is on the class path (or module path) then the gRPC server will be started automatically. This behaviour can be disabled by setting the coherence.grpc.enabled system property or COHERENCE_GRPC_ENABLED environment variable to false . ",
            "title": "Disabling the gRPC Proxy Server"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The Coherence gRPC proxy is the server-side implementation of the gRPC services defined within the Coherence gRPC module. The gRPC proxy uses standard gRPC Java libraries to provide Coherence APIs over gRPC. This chapter includes the following sections: Setting Up the Coherence gRPC Server To set up and start using the Coherence gRPC Server, you should declare it as a dependency of your project. Configuring the Server Configuring the gRPC Server includes setting the server port, specifying the in-process server name, and enabling TLS. Disabling the gRPC Proxy Server The Coherence gRPC server starts automatically based on the lifecycle events of DefaultCacheServer , but it can be disabled. Deploying the Proxy Service with Helidon Microprofile gRPC Server If you use the Helidon Microprofile server with the microprofile gRPC server enabled, you can deploy the Coherence gRPC proxy into the Helidon gRPC server instead of the Coherence default gRPC server. Setting Up the Coherence gRPC Proxy Server To set up and start using the Coherence gRPC Server, you should declare it as a dependency of your project. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy\" } Starting the Server The gRPC server starts automatically when you run com.tangosol.coherence.net.Coherence (or com.tangosol.coherence.net.DefaultCacheServer ). Typically, com.tangosol.coherence.net.Coherence class should be used as the application’s main class. Alternatively, you can start an instance of com.tangosol.coherence.net.Coherence by using the Bootstrap API. By default, the gRPC server will listen on all local addresses using an ephemeral port. Just like with Coherence*Extend, the endpoints the gRPC server has bound to can be discovered by a client using the Coherence NameService, so using ephemeral ports allows the server to start without needing to be concerned with port clashes. When reviewing the log output, two log messages appear as shown below to indicate which ports the gRPC server has bound to. <markup >In-Process GrpcAcceptor is now listening for connections using name \"default\" GrpcAcceptor now listening for connections on 0.0.0.0:55550 The service is ready to process requests from one of the Coherence gRPC client implementations. Configuring the Server The Coherence gRPC proxy is configured using an internal default cache configuration file named grpc-proxy-cache-config.xml which only contains a single &lt;proxy-scheme&gt; configuration for the gRPC proxy. There is no reason to override this file as the server can be configured with System properties and environment variables. Configuring the gRPC Server Listen Address and Port The address and port that the gRPC server binds to when starting can be configured at runtime by setting system properties or environment variables. By default, the server binds to the address 0.0.0.0 which equates to all the local host&#8217;s network interfaces. This can be changed by setting the coherence.grpc.server.address system property or COHERENCE_GRPC_SERVER_ADDRESS environment variable. For example, if the host had a local IP address 192.168.0.25 the server could be configured to bind to just this address as follows: Using System properties <markup >-Dcoherence.grpc.server.address=192.168.0.2 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_ADDRESS=192.168.0.2 The port that the gRPC server binds to can be configured using the coherence.grpc.server.port system property or COHERENCE_GRPC_SERVER_PORT environment variable For example, to configure the server to listen on port 1408: Using System properties <markup >-Dcoherence.grpc.server.port=1408 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_PORT=1408 Configuring SSL/TLS In common with the rest of Coherence, the Coherence gRPC server can be configured to use SSL by specifying the name of a socket provider. Named socket providers are configured in the Coherence operational configuration file (override file). There are various ways to configure an SSL socket provider, which are covered in the Coherence documentation section Using SSL to Secure Communication Once a named socket provider has been configured, the gRPC server can be configured to use that provider by setting the coherence.grpc.server.socketprovider system property or COHERENCE_GRPC_SERVER_SOCKETPROVIDER environment variable. For example, if a socket provider named tls has been configured in the operational configuration file, the gRPC server can be configured to use it: <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" > &lt;socket-providers&gt; &lt;socket-provider id=\"tls\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"coherence.security.key\"&gt;server.key&lt;/key&gt; &lt;cert system-property=\"coherence.security.cert\"&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"coherence.security.ca.cert\"&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; Using System properties <markup >-Dcoherence.grpc.server.socketprovider=tls Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_SOCKETPROVIDER=tls For more information on socket providers see Using SSL to Secure Communication Configuring the gRPC Server Thread Pool Like other Coherence services, the gRPC server uses a dynamically sized thread pool to process requests. The thread pool size can be configured if the dynamic sizing algorithm provies to not be optimal. Set the Minimum Thread Count Adjusting the minimum number of threads can be useful when dealing with bursts in load. Sometimes it can take the dynamic pool some time to increase the thread count to a suitable number to quickly deal with an increase in load. Setting the minimum size will ensure there are always a certain number of threads to service load. The minimum number of threads in the pool can be set using the coherence.grpc.server.threads.min system property, or the COHERENCE_GRPC_SERVER_THREADS_MIN environment variable. For example, the minimum thread count can be set to 10 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.min=10 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MIN=10 Set the Maximum Thread Count Adjusting the maximum number of threads can be useful to stop the dynamic pool going too high and consuming too much CPU resource. The maximum number of threads in the pool can be set using the coherence.grpc.server.threads.max system property, or the COHERENCE_GRPC_SERVER_THREADS_MAX environment variable. If both maximum and minimum thread counts are specified, the maximum thread count should obviously be set to a value higher than the minimum thread count. For example, the maximum thread count can be set to 20 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.max=20 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MAX=20 Disabling the gRPC Proxy Server If the coherence-grpc-proxy module is on the class path (or module path) then the gRPC server will be started automatically. This behaviour can be disabled by setting the coherence.grpc.enabled system property or COHERENCE_GRPC_ENABLED environment variable to false . ",
            "title": "25 Using the Coherence gRPC Proxy Server"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " To set up and start using the Coherence gRPC Java client, you should declare it as a dependency of your project. The gRPC client is provided in the coherence-java-client module. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-java-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-java-client\" } ",
            "title": "Setting Up the Coherence gRPC Client"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " A remote gRPC cache is specialized cache service that routes cache operations to a cache on the Coherence cluster via the gRPC proxy. The remote cache and the cache on the cluster must have the same cache name. Coherence gRPC clients use the NamedMap or NamedCache interfaces as normal to get an instance of the cache. At runtime, the cache operations are not executed locally but instead are sent using gRPC to a gRPC proxy service on the cluster. The fact that the cache operations are delegated to a cache on the cluster is transparent to the client. A remote gRPC cache is defined within a &lt;caching-schemes&gt; section using the &lt;remote-grpc-cache-scheme&gt; element. There are two approaches to configure a gRPC client: NameService - the gRPC client uses the Coherence NameService to discover the gRPC endpoints in the cluster. This is the simplest configuration. Coherence will discover all the endpoints in the cluster that the gRPC proxy is listening on and the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Fixed Endpoints - a fixed set of gRPC endpoints can be supplied, either hard coded or via a custom AddressProvider configuration. If multiple endpoints are provided, the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Some approaches work in some types of deployment environment and not in others, for example the NameService configurations are not suitable where the cluster is inside a containerized environment, such as Kubernetes and the client is external to this. Choose the simplest configuration that works in your environment. If both clients and cluster are inside the same containerized environment the NameService will work. In containerized environments such as Kubernetes, this is typically configured with a single ingress point which load balances connections to the Coherence cluster Pods. The address of this ingress point is then used as a single fixed address in the &lt;remote-grpc-cache-scheme&gt; configuration. ",
            "title": "Defining a Remote gRPC Cache"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The simplest configuration for a gRPC client is to use the NameService to locate the gRPC proxy endpoints, but without adding any address or port information in the &lt;remote-grpc-cache-scheme&gt; in the configuration file. This configuration will use Coherence&#8217;s default cluster discovery mechanism to locate the Coherence cluster&#8217;s NameService and look up the gRPC endpoints. This requires the client to be configured with the same cluster name and well-known-address list (or multicast configuration) as the cluster being connected to. The example below shows a &lt;remote-grpc-cache-scheme&gt; configured with just &lt;scheme-name&gt; and &lt;service-name&gt; elements. This is the absolute minimum, required configuration. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "A Minimal NameService Configuration"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " If the client is configured with a different cluster name to the cluster being connected to (for example the client is actually in a different Coherence cluster), then the &lt;remote-grpc-cache-scheme&gt; can be configured with a cluster name. For example, the &lt;remote-grpc-cache-scheme&gt; below is configured with &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; so Coherence will use the NameService to discover the gRPC endpoints in the Coherence cluster named test-cluster . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "A Minimal NameService Configuration with Different Cluster Name"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " If the client cannot use the standard Coherence cluster discovery mechanism to look up the target cluster, the NameService endpoints can be supplied in the &lt;grpc-channel&gt; section of the &lt;remote-grpc-cache-scheme&gt; configuration. The example below creates a remote cache scheme that is named RemoteGrpcCache , which connects to the Coherence NameService on 198.168.1.5:7574 , which then redirects the request to the address of the gRPC proxy service. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;name-service-addresses&gt; &lt;socket-address&gt; &lt;address&gt;198.168.1.5&lt;/address&gt; &lt;port&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/name-service-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Configure the NameService Endpoints"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " If the NameService cannot be used to discover the gRPC endpoints, a fixed set of addresses can be configured. In the &lt;grpc-channel&gt; section, configure a &lt;remote-addresses&gt; element containing one or more &lt;socket-address&gt; elements. For example, the client configured below will connect to a gRPC proxy listening on the endpoint test-cluster.svc:1408 . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Configure Fixed Endpoints"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " To configure the client to use SSL a socket provider can be configured in the &lt;grpc-channel&gt; section. Socket providers are configured exactly the same way as in other parts of Coherence. The &lt;socket-provider&gt; element can either contain the name of a socket provider configured in the Operational override file, or can be configured with an inline socket provider configuration. For example, the &lt;remote-grpc-cache-scheme&gt; is configured with a reference to the socket provider named ssl that is configured in the operational override file. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt;ssl&lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; The &lt;remote-grpc-cache-scheme&gt; below is configured with an inline socket provider. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key&gt;server.key&lt;/key&gt; &lt;cert&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; For more information on socket providers see Using SSL to Secure Communication ",
            "title": "Configure SSL"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Unlike an Extend client, the gRPC client is built on top of a gRPC asynchronous client. This is configured with a thread pool, to allow the client to process multiple parallel requests and responses. The thread pool used by the gRPC client is a standard Coherence dynamically sized thread pool, the number of threads will automatically be adjusted depending on load. Sometimes Coherence does not adjust the thread pool optimally for an application use-case, so it can be configured to set the pool size. Any of the thread count, minimum thread count and maximum thread count can be configured. Obviously the thread-count must be greater than or equal to the minimum count, and less than or equal the maximum count, and the maximum count must be greater than or equal to the minimum count. To configure a fixed size pool, just set the minimum and maximum to the same value. The example below configures all three thread counts. The pool will start with 10 threads and by automatically sized between 5 and 15 threads depending on load. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;thread-count&gt;10&lt;/thread-count&gt; &lt;thread-count-max&gt;15&lt;/thread-count-max&gt; &lt;thread-count-min&gt;5&lt;/thread-count-min&gt; &lt;/remote-grpc-cache-scheme&gt; ",
            "title": "Configuring the Client Thread Pool"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Just like Coherence*Extend, a Coherence gRPC client accesses remote clustered resources by configuring remote schemes in the applications cache configuration file. Defining a Remote gRPC Cache A remote gRPC cache is specialized cache service that routes cache operations to a cache on the Coherence cluster via the gRPC proxy. The remote cache and the cache on the cluster must have the same cache name. Coherence gRPC clients use the NamedMap or NamedCache interfaces as normal to get an instance of the cache. At runtime, the cache operations are not executed locally but instead are sent using gRPC to a gRPC proxy service on the cluster. The fact that the cache operations are delegated to a cache on the cluster is transparent to the client. A remote gRPC cache is defined within a &lt;caching-schemes&gt; section using the &lt;remote-grpc-cache-scheme&gt; element. There are two approaches to configure a gRPC client: NameService - the gRPC client uses the Coherence NameService to discover the gRPC endpoints in the cluster. This is the simplest configuration. Coherence will discover all the endpoints in the cluster that the gRPC proxy is listening on and the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Fixed Endpoints - a fixed set of gRPC endpoints can be supplied, either hard coded or via a custom AddressProvider configuration. If multiple endpoints are provided, the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Some approaches work in some types of deployment environment and not in others, for example the NameService configurations are not suitable where the cluster is inside a containerized environment, such as Kubernetes and the client is external to this. Choose the simplest configuration that works in your environment. If both clients and cluster are inside the same containerized environment the NameService will work. In containerized environments such as Kubernetes, this is typically configured with a single ingress point which load balances connections to the Coherence cluster Pods. The address of this ingress point is then used as a single fixed address in the &lt;remote-grpc-cache-scheme&gt; configuration. A Minimal NameService Configuration The simplest configuration for a gRPC client is to use the NameService to locate the gRPC proxy endpoints, but without adding any address or port information in the &lt;remote-grpc-cache-scheme&gt; in the configuration file. This configuration will use Coherence&#8217;s default cluster discovery mechanism to locate the Coherence cluster&#8217;s NameService and look up the gRPC endpoints. This requires the client to be configured with the same cluster name and well-known-address list (or multicast configuration) as the cluster being connected to. The example below shows a &lt;remote-grpc-cache-scheme&gt; configured with just &lt;scheme-name&gt; and &lt;service-name&gt; elements. This is the absolute minimum, required configuration. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; A Minimal NameService Configuration with Different Cluster Name If the client is configured with a different cluster name to the cluster being connected to (for example the client is actually in a different Coherence cluster), then the &lt;remote-grpc-cache-scheme&gt; can be configured with a cluster name. For example, the &lt;remote-grpc-cache-scheme&gt; below is configured with &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; so Coherence will use the NameService to discover the gRPC endpoints in the Coherence cluster named test-cluster . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure the NameService Endpoints If the client cannot use the standard Coherence cluster discovery mechanism to look up the target cluster, the NameService endpoints can be supplied in the &lt;grpc-channel&gt; section of the &lt;remote-grpc-cache-scheme&gt; configuration. The example below creates a remote cache scheme that is named RemoteGrpcCache , which connects to the Coherence NameService on 198.168.1.5:7574 , which then redirects the request to the address of the gRPC proxy service. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;name-service-addresses&gt; &lt;socket-address&gt; &lt;address&gt;198.168.1.5&lt;/address&gt; &lt;port&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/name-service-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure Fixed Endpoints If the NameService cannot be used to discover the gRPC endpoints, a fixed set of addresses can be configured. In the &lt;grpc-channel&gt; section, configure a &lt;remote-addresses&gt; element containing one or more &lt;socket-address&gt; elements. For example, the client configured below will connect to a gRPC proxy listening on the endpoint test-cluster.svc:1408 . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure SSL To configure the client to use SSL a socket provider can be configured in the &lt;grpc-channel&gt; section. Socket providers are configured exactly the same way as in other parts of Coherence. The &lt;socket-provider&gt; element can either contain the name of a socket provider configured in the Operational override file, or can be configured with an inline socket provider configuration. For example, the &lt;remote-grpc-cache-scheme&gt; is configured with a reference to the socket provider named ssl that is configured in the operational override file. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt;ssl&lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; The &lt;remote-grpc-cache-scheme&gt; below is configured with an inline socket provider. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key&gt;server.key&lt;/key&gt; &lt;cert&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; For more information on socket providers see Using SSL to Secure Communication Configuring the Client Thread Pool Unlike an Extend client, the gRPC client is built on top of a gRPC asynchronous client. This is configured with a thread pool, to allow the client to process multiple parallel requests and responses. The thread pool used by the gRPC client is a standard Coherence dynamically sized thread pool, the number of threads will automatically be adjusted depending on load. Sometimes Coherence does not adjust the thread pool optimally for an application use-case, so it can be configured to set the pool size. Any of the thread count, minimum thread count and maximum thread count can be configured. Obviously the thread-count must be greater than or equal to the minimum count, and less than or equal the maximum count, and the maximum count must be greater than or equal to the minimum count. To configure a fixed size pool, just set the minimum and maximum to the same value. The example below configures all three thread counts. The pool will start with 10 threads and by automatically sized between 5 and 15 threads depending on load. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;thread-count&gt;10&lt;/thread-count&gt; &lt;thread-count-max&gt;15&lt;/thread-count-max&gt; &lt;thread-count-min&gt;5&lt;/thread-count-min&gt; &lt;/remote-grpc-cache-scheme&gt; ",
            "title": "Configure the Coherence gRPC Client"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " A remote gRPC cache can be used as the back cache of a near-cache or a view-cache in the same way as other types of caches. The example below shows a near scheme configured to use a &lt;remote-grpc-cache-scheme&gt; as the back scheme. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;10000&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-ref&gt;remote-grpc&lt;/scheme-ref&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/back-scheme&gt; &lt;/near-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Using a Remote gRPC Cache as a Back Cache"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " As the gRPC client is configured as a remote scheme in the cache configuration file, Coherence resources can be accessed using the same Coherence APIs as used on cluster members or Extend clients. If the client has been started using the Coherence bootstrap API, running a com.tangosol.net.Coherence instance, a Session and NamedMap can be accessed as shown below: <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); NamedMap&lt;String, String&gt; map = session.getMap(\"test-cache\"); Using a Remote gRPC Cache as a Back Cache A remote gRPC cache can be used as the back cache of a near-cache or a view-cache in the same way as other types of caches. The example below shows a near scheme configured to use a &lt;remote-grpc-cache-scheme&gt; as the back scheme. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;10000&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-ref&gt;remote-grpc&lt;/scheme-ref&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/back-scheme&gt; &lt;/near-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Accessing Coherence Resources"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The Coherence Java gRPC Client is a library that enables a Java application to connect to a Coherence gRPC proxy server. This chapter includes the following sections: Setting Up the Coherence gRPC Client To set up and start using the Coherence gRPC Client, you should declare it as an application dependency. There should also be a corresponding Coherence server running the gRPC proxy to which the client can connect. Configure the Coherence gRPC Client Add the gRPC client configuration to the application&#8217;s cache configuration file. Accessing Coherence Resources The simplest way to access the remote Coherence resources, such as a NamedMap when using the gRPC client is through a Coherence Session . Setting Up the Coherence gRPC Client To set up and start using the Coherence gRPC Java client, you should declare it as a dependency of your project. The gRPC client is provided in the coherence-java-client module. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-java-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-java-client\" } Configure the Coherence gRPC Client Just like Coherence*Extend, a Coherence gRPC client accesses remote clustered resources by configuring remote schemes in the applications cache configuration file. Defining a Remote gRPC Cache A remote gRPC cache is specialized cache service that routes cache operations to a cache on the Coherence cluster via the gRPC proxy. The remote cache and the cache on the cluster must have the same cache name. Coherence gRPC clients use the NamedMap or NamedCache interfaces as normal to get an instance of the cache. At runtime, the cache operations are not executed locally but instead are sent using gRPC to a gRPC proxy service on the cluster. The fact that the cache operations are delegated to a cache on the cluster is transparent to the client. A remote gRPC cache is defined within a &lt;caching-schemes&gt; section using the &lt;remote-grpc-cache-scheme&gt; element. There are two approaches to configure a gRPC client: NameService - the gRPC client uses the Coherence NameService to discover the gRPC endpoints in the cluster. This is the simplest configuration. Coherence will discover all the endpoints in the cluster that the gRPC proxy is listening on and the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Fixed Endpoints - a fixed set of gRPC endpoints can be supplied, either hard coded or via a custom AddressProvider configuration. If multiple endpoints are provided, the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Some approaches work in some types of deployment environment and not in others, for example the NameService configurations are not suitable where the cluster is inside a containerized environment, such as Kubernetes and the client is external to this. Choose the simplest configuration that works in your environment. If both clients and cluster are inside the same containerized environment the NameService will work. In containerized environments such as Kubernetes, this is typically configured with a single ingress point which load balances connections to the Coherence cluster Pods. The address of this ingress point is then used as a single fixed address in the &lt;remote-grpc-cache-scheme&gt; configuration. A Minimal NameService Configuration The simplest configuration for a gRPC client is to use the NameService to locate the gRPC proxy endpoints, but without adding any address or port information in the &lt;remote-grpc-cache-scheme&gt; in the configuration file. This configuration will use Coherence&#8217;s default cluster discovery mechanism to locate the Coherence cluster&#8217;s NameService and look up the gRPC endpoints. This requires the client to be configured with the same cluster name and well-known-address list (or multicast configuration) as the cluster being connected to. The example below shows a &lt;remote-grpc-cache-scheme&gt; configured with just &lt;scheme-name&gt; and &lt;service-name&gt; elements. This is the absolute minimum, required configuration. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; A Minimal NameService Configuration with Different Cluster Name If the client is configured with a different cluster name to the cluster being connected to (for example the client is actually in a different Coherence cluster), then the &lt;remote-grpc-cache-scheme&gt; can be configured with a cluster name. For example, the &lt;remote-grpc-cache-scheme&gt; below is configured with &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; so Coherence will use the NameService to discover the gRPC endpoints in the Coherence cluster named test-cluster . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure the NameService Endpoints If the client cannot use the standard Coherence cluster discovery mechanism to look up the target cluster, the NameService endpoints can be supplied in the &lt;grpc-channel&gt; section of the &lt;remote-grpc-cache-scheme&gt; configuration. The example below creates a remote cache scheme that is named RemoteGrpcCache , which connects to the Coherence NameService on 198.168.1.5:7574 , which then redirects the request to the address of the gRPC proxy service. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;name-service-addresses&gt; &lt;socket-address&gt; &lt;address&gt;198.168.1.5&lt;/address&gt; &lt;port&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/name-service-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure Fixed Endpoints If the NameService cannot be used to discover the gRPC endpoints, a fixed set of addresses can be configured. In the &lt;grpc-channel&gt; section, configure a &lt;remote-addresses&gt; element containing one or more &lt;socket-address&gt; elements. For example, the client configured below will connect to a gRPC proxy listening on the endpoint test-cluster.svc:1408 . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure SSL To configure the client to use SSL a socket provider can be configured in the &lt;grpc-channel&gt; section. Socket providers are configured exactly the same way as in other parts of Coherence. The &lt;socket-provider&gt; element can either contain the name of a socket provider configured in the Operational override file, or can be configured with an inline socket provider configuration. For example, the &lt;remote-grpc-cache-scheme&gt; is configured with a reference to the socket provider named ssl that is configured in the operational override file. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt;ssl&lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; The &lt;remote-grpc-cache-scheme&gt; below is configured with an inline socket provider. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key&gt;server.key&lt;/key&gt; &lt;cert&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; For more information on socket providers see Using SSL to Secure Communication Configuring the Client Thread Pool Unlike an Extend client, the gRPC client is built on top of a gRPC asynchronous client. This is configured with a thread pool, to allow the client to process multiple parallel requests and responses. The thread pool used by the gRPC client is a standard Coherence dynamically sized thread pool, the number of threads will automatically be adjusted depending on load. Sometimes Coherence does not adjust the thread pool optimally for an application use-case, so it can be configured to set the pool size. Any of the thread count, minimum thread count and maximum thread count can be configured. Obviously the thread-count must be greater than or equal to the minimum count, and less than or equal the maximum count, and the maximum count must be greater than or equal to the minimum count. To configure a fixed size pool, just set the minimum and maximum to the same value. The example below configures all three thread counts. The pool will start with 10 threads and by automatically sized between 5 and 15 threads depending on load. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;thread-count&gt;10&lt;/thread-count&gt; &lt;thread-count-max&gt;15&lt;/thread-count-max&gt; &lt;thread-count-min&gt;5&lt;/thread-count-min&gt; &lt;/remote-grpc-cache-scheme&gt; Accessing Coherence Resources As the gRPC client is configured as a remote scheme in the cache configuration file, Coherence resources can be accessed using the same Coherence APIs as used on cluster members or Extend clients. If the client has been started using the Coherence bootstrap API, running a com.tangosol.net.Coherence instance, a Session and NamedMap can be accessed as shown below: <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); NamedMap&lt;String, String&gt; map = session.getMap(\"test-cache\"); Using a Remote gRPC Cache as a Back Cache A remote gRPC cache can be used as the back cache of a near-cache or a view-cache in the same way as other types of caches. The example below shows a near scheme configured to use a &lt;remote-grpc-cache-scheme&gt; as the back scheme. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;10000&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-ref&gt;remote-grpc&lt;/scheme-ref&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/back-scheme&gt; &lt;/near-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "26 Using the Coherence Java gRPC Client"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Learn how to use the Coherence gRPC library to interact with a Coherence data management services. This part contains the following chapters: Introduction to gRPC Coherence gRPC provides the protobuf definitions necessary to interact with a Coherence data management services over gRPC. Using the Coherence gRPC Server The Coherence gRPC proxy is the server-side implementation of the services defined within the Coherence gRPC module. The gRPC proxy uses standard gRPC Java libraries to provide Coherence APIs over gRPC. Using the Coherence Java gRPC Client The Coherence Java gRPC Client is a library that enables a Java application to connect to a Coherence gRPC proxy server. 24 Introduction to gRPC Coherence gRPC for Java allows Java applications to access Coherence clustered services, including data, data events, and data processing from outside the Coherence cluster. Typical uses for Java gRPC clients include desktop and Web applications that require access to remote Coherence resources. This provides an alternative to using Coherence*Extend when writing client applications. Note The Coherence gRPC client and Coherence Extend client feature sets do not match exactly, some functionality in gRPC is not available in Extend and vice-versa. The Coherence gRPC for Java library connects to a Coherence clustered service instance running within the Coherence cluster using a high performance gRPC based communication layer. This library sends all client requests to the Coherence clustered gRPC proxy service which, in turn, responds to client requests by delegating to an actual Coherence clustered service (for example, a partitioned cache service). Like cache clients that are members of the cluster, Java gRPC clients use the Session API call to retrieve a resources such as NamedMap , NamedCache , etc. After it is obtained, a client accesses these resources in the same way as it would if it were part of the Coherence cluster. The fact that operations on Coherence resources are being sent to a remote cluster node (over gRPC) is completely transparent to the client application. There are two parts to Coherence gRPC, the coherence-grpc-proxy module, that provides the server-side gRPC proxy, and the coherence-java-client module that provides the gRPC client. Other non-java Coherence clients are also available that use the Coherence gRPC protocol. 25 Using the Coherence gRPC Proxy Server The Coherence gRPC proxy is the server-side implementation of the gRPC services defined within the Coherence gRPC module. The gRPC proxy uses standard gRPC Java libraries to provide Coherence APIs over gRPC. This chapter includes the following sections: Setting Up the Coherence gRPC Server To set up and start using the Coherence gRPC Server, you should declare it as a dependency of your project. Configuring the Server Configuring the gRPC Server includes setting the server port, specifying the in-process server name, and enabling TLS. Disabling the gRPC Proxy Server The Coherence gRPC server starts automatically based on the lifecycle events of DefaultCacheServer , but it can be disabled. Deploying the Proxy Service with Helidon Microprofile gRPC Server If you use the Helidon Microprofile server with the microprofile gRPC server enabled, you can deploy the Coherence gRPC proxy into the Helidon gRPC server instead of the Coherence default gRPC server. Setting Up the Coherence gRPC Proxy Server To set up and start using the Coherence gRPC Server, you should declare it as a dependency of your project. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy\" } Starting the Server The gRPC server starts automatically when you run com.tangosol.coherence.net.Coherence (or com.tangosol.coherence.net.DefaultCacheServer ). Typically, com.tangosol.coherence.net.Coherence class should be used as the application’s main class. Alternatively, you can start an instance of com.tangosol.coherence.net.Coherence by using the Bootstrap API. By default, the gRPC server will listen on all local addresses using an ephemeral port. Just like with Coherence*Extend, the endpoints the gRPC server has bound to can be discovered by a client using the Coherence NameService, so using ephemeral ports allows the server to start without needing to be concerned with port clashes. When reviewing the log output, two log messages appear as shown below to indicate which ports the gRPC server has bound to. <markup >In-Process GrpcAcceptor is now listening for connections using name \"default\" GrpcAcceptor now listening for connections on 0.0.0.0:55550 The service is ready to process requests from one of the Coherence gRPC client implementations. Configuring the Server The Coherence gRPC proxy is configured using an internal default cache configuration file named grpc-proxy-cache-config.xml which only contains a single &lt;proxy-scheme&gt; configuration for the gRPC proxy. There is no reason to override this file as the server can be configured with System properties and environment variables. Configuring the gRPC Server Listen Address and Port The address and port that the gRPC server binds to when starting can be configured at runtime by setting system properties or environment variables. By default, the server binds to the address 0.0.0.0 which equates to all the local host&#8217;s network interfaces. This can be changed by setting the coherence.grpc.server.address system property or COHERENCE_GRPC_SERVER_ADDRESS environment variable. For example, if the host had a local IP address 192.168.0.25 the server could be configured to bind to just this address as follows: Using System properties <markup >-Dcoherence.grpc.server.address=192.168.0.2 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_ADDRESS=192.168.0.2 The port that the gRPC server binds to can be configured using the coherence.grpc.server.port system property or COHERENCE_GRPC_SERVER_PORT environment variable For example, to configure the server to listen on port 1408: Using System properties <markup >-Dcoherence.grpc.server.port=1408 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_PORT=1408 Configuring SSL/TLS In common with the rest of Coherence, the Coherence gRPC server can be configured to use SSL by specifying the name of a socket provider. Named socket providers are configured in the Coherence operational configuration file (override file). There are various ways to configure an SSL socket provider, which are covered in the Coherence documentation section Using SSL to Secure Communication Once a named socket provider has been configured, the gRPC server can be configured to use that provider by setting the coherence.grpc.server.socketprovider system property or COHERENCE_GRPC_SERVER_SOCKETPROVIDER environment variable. For example, if a socket provider named tls has been configured in the operational configuration file, the gRPC server can be configured to use it: <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" > &lt;socket-providers&gt; &lt;socket-provider id=\"tls\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"coherence.security.key\"&gt;server.key&lt;/key&gt; &lt;cert system-property=\"coherence.security.cert\"&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"coherence.security.ca.cert\"&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; Using System properties <markup >-Dcoherence.grpc.server.socketprovider=tls Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_SOCKETPROVIDER=tls For more information on socket providers see Using SSL to Secure Communication Configuring the gRPC Server Thread Pool Like other Coherence services, the gRPC server uses a dynamically sized thread pool to process requests. The thread pool size can be configured if the dynamic sizing algorithm provies to not be optimal. Set the Minimum Thread Count Adjusting the minimum number of threads can be useful when dealing with bursts in load. Sometimes it can take the dynamic pool some time to increase the thread count to a suitable number to quickly deal with an increase in load. Setting the minimum size will ensure there are always a certain number of threads to service load. The minimum number of threads in the pool can be set using the coherence.grpc.server.threads.min system property, or the COHERENCE_GRPC_SERVER_THREADS_MIN environment variable. For example, the minimum thread count can be set to 10 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.min=10 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MIN=10 Set the Maximum Thread Count Adjusting the maximum number of threads can be useful to stop the dynamic pool going too high and consuming too much CPU resource. The maximum number of threads in the pool can be set using the coherence.grpc.server.threads.max system property, or the COHERENCE_GRPC_SERVER_THREADS_MAX environment variable. If both maximum and minimum thread counts are specified, the maximum thread count should obviously be set to a value higher than the minimum thread count. For example, the maximum thread count can be set to 20 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.max=20 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MAX=20 Disabling the gRPC Proxy Server If the coherence-grpc-proxy module is on the class path (or module path) then the gRPC server will be started automatically. This behaviour can be disabled by setting the coherence.grpc.enabled system property or COHERENCE_GRPC_ENABLED environment variable to false . 26 Using the Coherence Java gRPC Client The Coherence Java gRPC Client is a library that enables a Java application to connect to a Coherence gRPC proxy server. This chapter includes the following sections: Setting Up the Coherence gRPC Client To set up and start using the Coherence gRPC Client, you should declare it as an application dependency. There should also be a corresponding Coherence server running the gRPC proxy to which the client can connect. Configure the Coherence gRPC Client Add the gRPC client configuration to the application&#8217;s cache configuration file. Accessing Coherence Resources The simplest way to access the remote Coherence resources, such as a NamedMap when using the gRPC client is through a Coherence Session . Setting Up the Coherence gRPC Client To set up and start using the Coherence gRPC Java client, you should declare it as a dependency of your project. The gRPC client is provided in the coherence-java-client module. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-java-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-java-client\" } Configure the Coherence gRPC Client Just like Coherence*Extend, a Coherence gRPC client accesses remote clustered resources by configuring remote schemes in the applications cache configuration file. Defining a Remote gRPC Cache A remote gRPC cache is specialized cache service that routes cache operations to a cache on the Coherence cluster via the gRPC proxy. The remote cache and the cache on the cluster must have the same cache name. Coherence gRPC clients use the NamedMap or NamedCache interfaces as normal to get an instance of the cache. At runtime, the cache operations are not executed locally but instead are sent using gRPC to a gRPC proxy service on the cluster. The fact that the cache operations are delegated to a cache on the cluster is transparent to the client. A remote gRPC cache is defined within a &lt;caching-schemes&gt; section using the &lt;remote-grpc-cache-scheme&gt; element. There are two approaches to configure a gRPC client: NameService - the gRPC client uses the Coherence NameService to discover the gRPC endpoints in the cluster. This is the simplest configuration. Coherence will discover all the endpoints in the cluster that the gRPC proxy is listening on and the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Fixed Endpoints - a fixed set of gRPC endpoints can be supplied, either hard coded or via a custom AddressProvider configuration. If multiple endpoints are provided, the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Some approaches work in some types of deployment environment and not in others, for example the NameService configurations are not suitable where the cluster is inside a containerized environment, such as Kubernetes and the client is external to this. Choose the simplest configuration that works in your environment. If both clients and cluster are inside the same containerized environment the NameService will work. In containerized environments such as Kubernetes, this is typically configured with a single ingress point which load balances connections to the Coherence cluster Pods. The address of this ingress point is then used as a single fixed address in the &lt;remote-grpc-cache-scheme&gt; configuration. A Minimal NameService Configuration The simplest configuration for a gRPC client is to use the NameService to locate the gRPC proxy endpoints, but without adding any address or port information in the &lt;remote-grpc-cache-scheme&gt; in the configuration file. This configuration will use Coherence&#8217;s default cluster discovery mechanism to locate the Coherence cluster&#8217;s NameService and look up the gRPC endpoints. This requires the client to be configured with the same cluster name and well-known-address list (or multicast configuration) as the cluster being connected to. The example below shows a &lt;remote-grpc-cache-scheme&gt; configured with just &lt;scheme-name&gt; and &lt;service-name&gt; elements. This is the absolute minimum, required configuration. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; A Minimal NameService Configuration with Different Cluster Name If the client is configured with a different cluster name to the cluster being connected to (for example the client is actually in a different Coherence cluster), then the &lt;remote-grpc-cache-scheme&gt; can be configured with a cluster name. For example, the &lt;remote-grpc-cache-scheme&gt; below is configured with &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; so Coherence will use the NameService to discover the gRPC endpoints in the Coherence cluster named test-cluster . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure the NameService Endpoints If the client cannot use the standard Coherence cluster discovery mechanism to look up the target cluster, the NameService endpoints can be supplied in the &lt;grpc-channel&gt; section of the &lt;remote-grpc-cache-scheme&gt; configuration. The example below creates a remote cache scheme that is named RemoteGrpcCache , which connects to the Coherence NameService on 198.168.1.5:7574 , which then redirects the request to the address of the gRPC proxy service. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;name-service-addresses&gt; &lt;socket-address&gt; &lt;address&gt;198.168.1.5&lt;/address&gt; &lt;port&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/name-service-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure Fixed Endpoints If the NameService cannot be used to discover the gRPC endpoints, a fixed set of addresses can be configured. In the &lt;grpc-channel&gt; section, configure a &lt;remote-addresses&gt; element containing one or more &lt;socket-address&gt; elements. For example, the client configured below will connect to a gRPC proxy listening on the endpoint test-cluster.svc:1408 . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure SSL To configure the client to use SSL a socket provider can be configured in the &lt;grpc-channel&gt; section. Socket providers are configured exactly the same way as in other parts of Coherence. The &lt;socket-provider&gt; element can either contain the name of a socket provider configured in the Operational override file, or can be configured with an inline socket provider configuration. For example, the &lt;remote-grpc-cache-scheme&gt; is configured with a reference to the socket provider named ssl that is configured in the operational override file. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt;ssl&lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; The &lt;remote-grpc-cache-scheme&gt; below is configured with an inline socket provider. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key&gt;server.key&lt;/key&gt; &lt;cert&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; For more information on socket providers see Using SSL to Secure Communication Configuring the Client Thread Pool Unlike an Extend client, the gRPC client is built on top of a gRPC asynchronous client. This is configured with a thread pool, to allow the client to process multiple parallel requests and responses. The thread pool used by the gRPC client is a standard Coherence dynamically sized thread pool, the number of threads will automatically be adjusted depending on load. Sometimes Coherence does not adjust the thread pool optimally for an application use-case, so it can be configured to set the pool size. Any of the thread count, minimum thread count and maximum thread count can be configured. Obviously the thread-count must be greater than or equal to the minimum count, and less than or equal the maximum count, and the maximum count must be greater than or equal to the minimum count. To configure a fixed size pool, just set the minimum and maximum to the same value. The example below configures all three thread counts. The pool will start with 10 threads and by automatically sized between 5 and 15 threads depending on load. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;thread-count&gt;10&lt;/thread-count&gt; &lt;thread-count-max&gt;15&lt;/thread-count-max&gt; &lt;thread-count-min&gt;5&lt;/thread-count-min&gt; &lt;/remote-grpc-cache-scheme&gt; Accessing Coherence Resources As the gRPC client is configured as a remote scheme in the cache configuration file, Coherence resources can be accessed using the same Coherence APIs as used on cluster members or Extend clients. If the client has been started using the Coherence bootstrap API, running a com.tangosol.net.Coherence instance, a Session and NamedMap can be accessed as shown below: <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); NamedMap&lt;String, String&gt; map = session.getMap(\"test-cache\"); Using a Remote gRPC Cache as a Back Cache A remote gRPC cache can be used as the back cache of a near-cache or a view-cache in the same way as other types of caches. The example below shows a near scheme configured to use a &lt;remote-grpc-cache-scheme&gt; as the back scheme. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;10000&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-ref&gt;remote-grpc&lt;/scheme-ref&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/back-scheme&gt; &lt;/near-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Part V Getting Started with gRPC"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " The POF Gradle Plugin provides automated instrumentation of classes with the @PortableType annotation to generate consistent (and correct) implementations of Evolvable POF serialization methods. It is a far from a trivial exercise to manually write serialization methods that support serializing inheritance hierarchies that support the Evolvable concept. However, with static type analysis these methods can be deterministically generated. This allows developers to focus on business logic rather than implementing boilerplate code for the above-mentioned methods. Please see Portable Types documentation for more information and detailed instructions on Portable Types creation and usage. ",
            "title": "preambule"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " The default behavior of the Coherence Gradle Plugin, can be customized using several optional properties. Simply provide a coherencePof closure to your build.gradle script containing any additional configuration properties, e.g.: <markup lang=\"groovy\" title=\"Build.gradle\" >coherencePof { debug=true } This will instruct Coherence to provide more logging output in regard to the instrumented classes ",
            "title": "Custom Configuration"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " Set the boolean debug property to true in order to instruct the underlying PortableTypeGenerator to generate debug code in regards the instrumented classes. If not specified, this property defaults to false . ",
            "title": "Enable Debugging"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " Set the boolean instrumentTestClasses property to true in order to instrument test classes. If not specified, this property defaults to false . ",
            "title": "Instrumentation of Test Classes"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " Enable Debugging Set the boolean debug property to true in order to instruct the underlying PortableTypeGenerator to generate debug code in regards the instrumented classes. If not specified, this property defaults to false . Instrumentation of Test Classes Set the boolean instrumentTestClasses property to true in order to instrument test classes. If not specified, this property defaults to false . ",
            "title": "Available Configuration Properties"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " In some cases, it may be necessary to expand the type system with the types that are not annotated with the @PortableType annotation, and are not discovered automatically. This is typically the case when some of your portable types have enum values, or existing classes that implement the PortableObject interface explicitly as attributes. You can add those types to the schema by creating a META-INF/schema.xml file and specifying them explicitly. For example, if you assume that the Color class from the earlier code examples: <markup lang=\"xml\" title=\"META-INF/schema.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;schema xmlns=\"http://xmlns.oracle.com/coherence/schema\" xmlns:java=\"http://xmlns.oracle.com/coherence/schema/java\" external=\"true\"&gt; &lt;type name=\"Color\"&gt; &lt;java:type name=\"petstore.Color\"/&gt; &lt;/type&gt; &lt;/schema&gt; In order to use a schema.xml file, you need to set usePofSchemaXml to true . Also, you can customize the location of the schema.xml by setting pofSchemaXmlPath to a value other than META-INF/schema.xml . For example: <markup lang=\"groovy\" title=\"Build.gradle\" >coherencePof { usePofSchemaXml = true pofSchemaXmlPath = 'META-INF/my-schema.xml' } ",
            "title": "What about classes without the @PortableType annotation?"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " In order to use the POF Gradle Plugin, you need to declare it as a plugin dependency in your build.gradle file: <markup lang=\"groovy\" >plugins { id 'java' id 'com.oracle.coherence.ce' } Without any further configuration, the plugin will add a task named coherencePof to your project and you will see the task listed under the task group Coherence when you execute: <markup lang=\"bash\" >gradle tasks The coherencePof task is added as a doLast action to the compileJava task. Therefore, executing: <markup lang=\"bash\" >gradle compileJava will automatically execute the coherencePof task. By default , the coherencePof task will instrument all Java classes excluding any test classes. The POF Gradle Plugin supports incremental builds . This means that only if Java classes have changed, the coherencePof task will execute. The coherencePof task will also become a dependency to all tasks that depend on the compileJava . Therefore, executing the build or jar task will invoke the coherencePof task in case of class changes. By just adding the plugin using the configuration above, the Coherence Gradle Plugin will discover and instrument all project classes annotated with the @PortableType annotation, excluding test classes. If you do need to instrument test classes, you can add the coherencePof closure and provide additional configuration properties. Custom Configuration The default behavior of the Coherence Gradle Plugin, can be customized using several optional properties. Simply provide a coherencePof closure to your build.gradle script containing any additional configuration properties, e.g.: <markup lang=\"groovy\" title=\"Build.gradle\" >coherencePof { debug=true } This will instruct Coherence to provide more logging output in regard to the instrumented classes Available Configuration Properties Enable Debugging Set the boolean debug property to true in order to instruct the underlying PortableTypeGenerator to generate debug code in regards the instrumented classes. If not specified, this property defaults to false . Instrumentation of Test Classes Set the boolean instrumentTestClasses property to true in order to instrument test classes. If not specified, this property defaults to false . What about classes without the @PortableType annotation? In some cases, it may be necessary to expand the type system with the types that are not annotated with the @PortableType annotation, and are not discovered automatically. This is typically the case when some of your portable types have enum values, or existing classes that implement the PortableObject interface explicitly as attributes. You can add those types to the schema by creating a META-INF/schema.xml file and specifying them explicitly. For example, if you assume that the Color class from the earlier code examples: <markup lang=\"xml\" title=\"META-INF/schema.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;schema xmlns=\"http://xmlns.oracle.com/coherence/schema\" xmlns:java=\"http://xmlns.oracle.com/coherence/schema/java\" external=\"true\"&gt; &lt;type name=\"Color\"&gt; &lt;java:type name=\"petstore.Color\"/&gt; &lt;/type&gt; &lt;/schema&gt; In order to use a schema.xml file, you need to set usePofSchemaXml to true . Also, you can customize the location of the schema.xml by setting pofSchemaXmlPath to a value other than META-INF/schema.xml . For example: <markup lang=\"groovy\" title=\"Build.gradle\" >coherencePof { usePofSchemaXml = true pofSchemaXmlPath = 'META-INF/my-schema.xml' } ",
            "title": "Usage"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " The portable type discovery feature of Coherence can use index files to speed up the discovery of @PortableType annotated classes. By default, the Gradle plugin will generate index files under META-INF/pod.idx that contain class names of @PortableType annotated classes. You can skip the generation of those index files by setting the indexPofClasses property in your Gradle plugin configuration to false . ",
            "title": "Generating POF Index Files"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " You can skip the execution of the coherencePof task by running the Gradle build using the -x flag, e.g.: <markup lang=\"bash\" >gradle clean build -x coherencePof ",
            "title": "Skip Execution"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " An example Person class (below) when processed with the plugin, results in the bytecode shown below. <markup lang=\"java\" title=\"Person.java\" >@PortableType(id=1000) public class Person { public Person() { } public Person(int id, String name, Address address) { super(); this.id = id; this.name = name; this.address = address; } int id; String name; Address address; // getters and setters omitted for brevity } Let&#8217;s inspect the generated bytecode: <markup lang=\"bash\" >javap Person.class This should yield the following output: <markup lang=\"java\" >public class demo.Person implements com.tangosol.io.pof.PortableObject,com.tangosol.io.pof.EvolvableObject { int id; java.lang.String name; demo.Address address; public demo.Person(); public demo.Person(int, java.lang.String, demo.Address); public int getId(); public void setId(int); public java.lang.String getName(); public void setName(java.lang.String); public demo.Address getAddress(); public void setAddress(demo.Address); public java.lang.String toString(); public int hashCode(); public boolean equals(java.lang.Object); public void readExternal(com.tangosol.io.pof.PofReader) throws java.io.IOException; public void writeExternal(com.tangosol.io.pof.PofWriter) throws java.io.IOException; public com.tangosol.io.Evolvable getEvolvable(int); public com.tangosol.io.pof.EvolvableHolder getEvolvableHolder(); } Additional methods generated by Coherence POF plugin. Additionally, you will see that under the META-INF directory you will have a generated POF index file pof.idx . I will contain the package and class name of the Person class. This will later during the execution of your application speed up the POF type discovery process for your @PortableType annotated classes. Skip Execution You can skip the execution of the coherencePof task by running the Gradle build using the -x flag, e.g.: <markup lang=\"bash\" >gradle clean build -x coherencePof ",
            "title": "Example"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " The easiest way to provide the proxy settings (when building the Coherence Gradle plugin by invoking Gradle directly), is to add the proxy settings to the gradle.properties file: <markup lang=\"properties\" >systemProp.http.proxyHost=your-proxy-host.com systemProp.http.proxyPort=80 systemProp.https.proxyHost=your-proxy-host.com systemProp.https.proxyPort=80 systemProp.https.nonProxyHosts=localhost|127.0.0.1 ",
            "title": "Using Gradle directly"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " When building the entire Coherence project using Maven, we configure the relevant proxy properties in tools/maven/settings.xml . <markup lang=\"xml\" > &lt;properties&gt; &lt;gradle.https.proxyHost&gt;your-proxy-host.com&lt;/gradle.https.proxyHost&gt; &lt;gradle.https.proxyPort&gt;80&lt;/gradle.https.proxyPort&gt; &lt;/properties&gt; In the pom.xml of the Coherence Gradle plugin module, the proxy properties are then populated using the gradleProxy Maven profile which is activated as soon as the property gradle.https.proxyHost is present. The Gradle integration tests are activated once the Maven profile stage1 is explicitly activated, and the build is executed with the Maven phase verify being triggered. ",
            "title": "Building the Project using Maven"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " When building the Coherence Gradle using a proxy server instead of accessing remote repositories directly, you must ensure that the proxy configuration is propagated all the way down to the Gradle integration tests as they use the GradleRunner (Gradle TestKit). Using Gradle directly The easiest way to provide the proxy settings (when building the Coherence Gradle plugin by invoking Gradle directly), is to add the proxy settings to the gradle.properties file: <markup lang=\"properties\" >systemProp.http.proxyHost=your-proxy-host.com systemProp.http.proxyPort=80 systemProp.https.proxyHost=your-proxy-host.com systemProp.https.proxyPort=80 systemProp.https.nonProxyHosts=localhost|127.0.0.1 Building the Project using Maven When building the entire Coherence project using Maven, we configure the relevant proxy properties in tools/maven/settings.xml . <markup lang=\"xml\" > &lt;properties&gt; &lt;gradle.https.proxyHost&gt;your-proxy-host.com&lt;/gradle.https.proxyHost&gt; &lt;gradle.https.proxyPort&gt;80&lt;/gradle.https.proxyPort&gt; &lt;/properties&gt; In the pom.xml of the Coherence Gradle plugin module, the proxy properties are then populated using the gradleProxy Maven profile which is activated as soon as the property gradle.https.proxyHost is present. The Gradle integration tests are activated once the Maven profile stage1 is explicitly activated, and the build is executed with the Maven phase verify being triggered. ",
            "title": "Building using a Proxy Server"
        },
        {
            "location": "/plugins/gradle/README",
            "text": " During development, it is extremely useful to rapidly test the plugin code against separate example projects. For this, we can use Gradle&#8217;s composite build feature. Therefore, the Coherence POF Gradle Plugin module itself can be easily integrated into a separate sample project for rapid testing of code changes. From within the sample directory you can execute: <markup lang=\"bash\" >gradle clean compileJava --include-build path/to/plugin This will not only build the sample but will also build the plugin and developers can make plugin code changes and see changes rapidly reflected in the execution of the sample module. Alternatively, you can build and install the Coherence Gradle plugin to your local Maven repository using: <markup lang=\"bash\" >gradle publishToMavenLocal For projects to pick up the local changes ensure the following configuration: <markup lang=\"groovy\" title=\"build.gradle\" >plugins { id 'java' id 'com.oracle.coherence.ce' version '24.03-SNAPSHOT' } dependencies { &#8230;&#8203; implementation 'com.oracle.coherence:coherence:24.03-SNAPSHOT' } repositories { mavenLocal() mavenCentral() } <markup lang=\"groovy\" title=\"settings.gradle\" >pluginManagement { repositories { mavenLocal() gradlePluginPortal() } } Building using a Proxy Server When building the Coherence Gradle using a proxy server instead of accessing remote repositories directly, you must ensure that the proxy configuration is propagated all the way down to the Gradle integration tests as they use the GradleRunner (Gradle TestKit). Using Gradle directly The easiest way to provide the proxy settings (when building the Coherence Gradle plugin by invoking Gradle directly), is to add the proxy settings to the gradle.properties file: <markup lang=\"properties\" >systemProp.http.proxyHost=your-proxy-host.com systemProp.http.proxyPort=80 systemProp.https.proxyHost=your-proxy-host.com systemProp.https.proxyPort=80 systemProp.https.nonProxyHosts=localhost|127.0.0.1 Building the Project using Maven When building the entire Coherence project using Maven, we configure the relevant proxy properties in tools/maven/settings.xml . <markup lang=\"xml\" > &lt;properties&gt; &lt;gradle.https.proxyHost&gt;your-proxy-host.com&lt;/gradle.https.proxyHost&gt; &lt;gradle.https.proxyPort&gt;80&lt;/gradle.https.proxyPort&gt; &lt;/properties&gt; In the pom.xml of the Coherence Gradle plugin module, the proxy properties are then populated using the gradleProxy Maven profile which is activated as soon as the property gradle.https.proxyHost is present. The Gradle integration tests are activated once the Maven profile stage1 is explicitly activated, and the build is executed with the Maven phase verify being triggered. ",
            "title": "Development"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Tests Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " In this example you will run a number of tests and that show the following features of client events including: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " The example code comprises the ClientEventsTest class, which runs a test showing various aspects of client events. The testMapListeners runs the following test code for various scenarios testStandardMapListener - standard MapListener implementation listening to all events testMultiplexingMapListener - MultiplexingMapListener listening to all events through the onMapEvent() method testSimpleMapListener - SimpleMapListener allows the use of lambdas to add event handlers to listen to events testListenOnQueries - listening for only for events on New York customers testEventTypes - listening for new or updated GOLD customers Review the Customer class All the tests use the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private int id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review the test boostrap and cleanup to start the cluster before all the tests and shutdown after the tests <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); coherence.start().join(); customers = coherence.getSession().getMap(\"customers\"); } <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } Review the testStandardMapListener code This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. <markup lang=\"java\" >/** * Simple {@link MapListener} implementation for Customers. */ public static class CustomerMapListener implements MapListener&lt;Integer, Customer&gt; { private final AtomicInteger insertCount = new AtomicInteger(); private final AtomicInteger updateCount = new AtomicInteger(); private final AtomicInteger removeCount = new AtomicInteger(); private final AtomicInteger liteEvents = new AtomicInteger(); @Override public void entryInserted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"New customer: new key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getNewValue()); insertCount.incrementAndGet(); if (mapEvent.getNewValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryUpdated(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Updated customer key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); updateCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryDeleted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Deleted customer: old key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getOldValue()); removeCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } public int getInsertCount() { return insertCount.get(); } public int getUpdateCount() { return updateCount.get(); } public int getRemoveCount() { return removeCount.get(); } public int getLiteEvents() { return liteEvents.get(); } } Implements MapListener interface AtomicIntegers for test validation Respond to insert events with new value Respond to update events with old and new values Respond to delete events with old value <markup lang=\"java\" >Logger.info(\"*** testStandardMapListener\"); customers.clear(); CustomerMapListener mapListener = new CustomerMapListener(); customers.addMapListener(mapListener); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.invoke(1, Processors.update(Customer::setCreditLimit, 2000L)); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(1)); Eventually.assertThat(invoking(mapListener).getRemoveCount(), is(1)); customers.removeMapListener(mapListener); Create the MapListener Add the MapListener to listen for all events Add the customers Update the credit limit for customer 1 Remove customer 1 Wait for all events Review the testMultiplexingMapListener code This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"java\" >/** * Simple {@link MultiplexingMapListener} implementation for Customers. */ public static class MultiplexingCustomerMapListener extends MultiplexingMapListener&lt;Integer, Customer&gt; { private final AtomicInteger counter = new AtomicInteger(); @Override protected void onMapEvent(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"isInsert=\" + mapEvent.isInsert() + \", isDelete=\" + mapEvent.isDelete() + \", isUpdate=\" + mapEvent.isUpdate()); Logger.info(\"key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); Logger.info(mapEvent.toString()); counter.incrementAndGet(); } public int getCount() { return counter.get(); } } Extends abstract class MultiplexingMapListener AtomicInteger for test validation Respond to all events and use MapEvent methods to determine type of event <markup lang=\"java\" >Logger.info(\"*** testMultiplexingMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; multiplexingMapListener = new MultiplexingCustomerMapListener(); // Multiplexing MapListener listening on all entries customers.addMapListener(multiplexingMapListener); customer1 = new Customer(1, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.invoke(1, Processors.update(Customer::setAddress, \"Updated address\")); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking((MultiplexingCustomerMapListener) multiplexingMapListener).getCount(), is(3)); customers.removeMapListener(multiplexingMapListener); Create the MapListener Add the MapListener to listen for all events Mutate the customers Wait for all events Review the testSimpleMapListener code This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"java\" >Logger.info(\"*** testSimpleMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; simpleMapListener = new SimpleMapListener&lt;Integer, Customer&gt;() .addInsertHandler((e) -&gt; Logger.info(\"New Customer added with id=\" + e.getNewValue().getId())) .addDeleteHandler((e) -&gt; Logger.info(\"Deleted customer id =\" + e.getOldValue().getId())) .addInsertHandler((e) -&gt; insertCount.incrementAndGet()) .addDeleteHandler((e) -&gt; deleteCount.incrementAndGet()); customers.addMapListener(simpleMapListener, 1, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.clear(); // should only be 1 insert and 1 delete as we are listening on the key Eventually.assertThat(invoking(this).getInsertCount(), is(1)); Eventually.assertThat(invoking(this).getDeleteCount(), is(1)); Create the SimpleMapListener instance Add an insert handler to display new customers Add delete a handler to display deleted customers Add an insert handler to increment an atomic Add delete a handler to increment an atomic Register the listener on the key 1 (customer id 1) wait for all events Review the testListenOnQueries code This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"java\" >Logger.info(\"*** testListenOnQueries\"); customers.clear(); mapListener = new CustomerMapListener(); // MapListener listening only to new customers from NY Filter&lt;Customer&gt; filter = Filters.like(Customer::getAddress, \"%NY%\"); MapEventFilter&lt;Integer, Customer&gt; eventFilter = new MapEventFilter&lt;&gt;(filter); customer1 = new Customer(1, \"Tim\", \"123 James Street, Perth, Australia\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street, New York, NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customer4 = new Customer(4, \"James Stewart\", \"123 5th Ave, New York, NY\", Customer.SILVER, 200); // Listen only for events where address is in New York customers.addMapListener(mapListener, eventFilter, true); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); customers.put(customer4.getId(), customer4); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); // ensure we only receive lite events Eventually.assertThat(invoking(mapListener).getLiteEvents(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the MapListener instance Create a like filter to select only customers whose address contains NY Add the map listener and specify a MapEventFilter which takes the filter created above as well as specifying the event is lite event where the new and old values may not necessarily be present wait for all events Review the testEventTypes code This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"java\" >Logger.info(\"*** testEventTypes\"); customers.clear(); mapListener = new CustomerMapListener(); filter = Filters.equal(Customer::getCustomerType, Customer.GOLD); // listen only for events where customers has been inserted as GOLD or updated to GOLD status or were changed from GOLD int mask = MapEventFilter.E_INSERTED | MapEventFilter.E_UPDATED_ENTERED| MapEventFilter.E_UPDATED_LEFT; eventFilter = new MapEventFilter&lt;&gt;(mask, filter); customers.addMapListener(mapListener, eventFilter, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); // update customer 1 from BRONZE to GOLD customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); customers.invoke(2, Processors.update(Customer::setCustomerType, Customer.SILVER)); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(1)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the CustomerMapListener instance Create an equals filter to select only GOLD customers Create a mask for inserted events for when the filter is matched or events that are updated and now the filter matches Add the map listener and specify a MapEventFilter which takes the filter created above/ wait for all events ",
            "title": "Review the Tests"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " Run the examples using the test case below. Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.clientevents.ClientEventsTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the various parts of the test code testStandardMapListener Output This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. Output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testStandardMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer: old key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} Insert event from new customer id 1 Insert event from new customer id 2 Update event from updating of customer 1&#8217;s credit limit Delete event containing old version of deleted customer 1 testMultiplexingMapListener Output This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=true, isDelete=false, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=null, new=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=false, isUpdate=true &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=true, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000}, new=null Insert event from new customer id 1 Update event from an update of customer 1 address Delete event from customer 1 testSimpleMapListener Output This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testSimpleMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New Customer added with id=1 &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer id =1 testListenOnQueries Output This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/null &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=4/null Both above queries only return the key because they are lite events and only customer 2 and 4 are returned as they are the only ones with NY in the address. testEventTypes Output This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testEventTypes &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='GOLD', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=2, old=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='SILVER', balance=10000} Insert event from new GOLD customer id 2 Update event changing customer type from BRONZE to GOLD for customer id 1 Update event changing customer type from GOLD to BRONZE for customer id 2 ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " In this example you have seen how to use the following features of client events: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " Develop Applications using Map Events Understanding Near Caches Using Continuous Query Caches ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " This guide walks you through how to use client events within Coherence to listen for insert, update or delete events on a Coherence NamedMap . An application object that implements the MapListener interface can sign up for events from any Coherence NamedMap simply by passing an instance of the application&#8217;s MapListener implementation to a addMapListener() method. The MapListener can be registered against all entries, a specific key, or a Filter. Registrations with filters can use MapEventFilter which provide more fine-grained control for event registrations or InKeySetFilter which can be used to register against a Set of keys. The MapListener interface provides a call back mechanism for NamedMap events where any changes that happen to the source (NamedMap) are delivered to relevant clients asynchronously. The MapEvent object that is passed to the MapListener carries all the necessary information about the event that has occurred Including the event type (insert, update, or delete), the key, old value, new value, and the source ( NameMap ) that emitted the event. Client events are the key building blocks for other Coherence functionality including Near Cache and Continuous Query Caches (CQC). See the Coherence Documentation links below for more information: Develop Applications using Map Events Understanding Near Caches Using Continuous Query Caches Table of Contents What You Will Build What You Need Building the Example Code Review the Tests Run the Examples Summary See Also What You Will Build In this example you will run a number of tests and that show the following features of client events including: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Review the Tests The example code comprises the ClientEventsTest class, which runs a test showing various aspects of client events. The testMapListeners runs the following test code for various scenarios testStandardMapListener - standard MapListener implementation listening to all events testMultiplexingMapListener - MultiplexingMapListener listening to all events through the onMapEvent() method testSimpleMapListener - SimpleMapListener allows the use of lambdas to add event handlers to listen to events testListenOnQueries - listening for only for events on New York customers testEventTypes - listening for new or updated GOLD customers Review the Customer class All the tests use the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private int id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review the test boostrap and cleanup to start the cluster before all the tests and shutdown after the tests <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); coherence.start().join(); customers = coherence.getSession().getMap(\"customers\"); } <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } Review the testStandardMapListener code This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. <markup lang=\"java\" >/** * Simple {@link MapListener} implementation for Customers. */ public static class CustomerMapListener implements MapListener&lt;Integer, Customer&gt; { private final AtomicInteger insertCount = new AtomicInteger(); private final AtomicInteger updateCount = new AtomicInteger(); private final AtomicInteger removeCount = new AtomicInteger(); private final AtomicInteger liteEvents = new AtomicInteger(); @Override public void entryInserted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"New customer: new key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getNewValue()); insertCount.incrementAndGet(); if (mapEvent.getNewValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryUpdated(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Updated customer key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); updateCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryDeleted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Deleted customer: old key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getOldValue()); removeCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } public int getInsertCount() { return insertCount.get(); } public int getUpdateCount() { return updateCount.get(); } public int getRemoveCount() { return removeCount.get(); } public int getLiteEvents() { return liteEvents.get(); } } Implements MapListener interface AtomicIntegers for test validation Respond to insert events with new value Respond to update events with old and new values Respond to delete events with old value <markup lang=\"java\" >Logger.info(\"*** testStandardMapListener\"); customers.clear(); CustomerMapListener mapListener = new CustomerMapListener(); customers.addMapListener(mapListener); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.invoke(1, Processors.update(Customer::setCreditLimit, 2000L)); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(1)); Eventually.assertThat(invoking(mapListener).getRemoveCount(), is(1)); customers.removeMapListener(mapListener); Create the MapListener Add the MapListener to listen for all events Add the customers Update the credit limit for customer 1 Remove customer 1 Wait for all events Review the testMultiplexingMapListener code This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"java\" >/** * Simple {@link MultiplexingMapListener} implementation for Customers. */ public static class MultiplexingCustomerMapListener extends MultiplexingMapListener&lt;Integer, Customer&gt; { private final AtomicInteger counter = new AtomicInteger(); @Override protected void onMapEvent(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"isInsert=\" + mapEvent.isInsert() + \", isDelete=\" + mapEvent.isDelete() + \", isUpdate=\" + mapEvent.isUpdate()); Logger.info(\"key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); Logger.info(mapEvent.toString()); counter.incrementAndGet(); } public int getCount() { return counter.get(); } } Extends abstract class MultiplexingMapListener AtomicInteger for test validation Respond to all events and use MapEvent methods to determine type of event <markup lang=\"java\" >Logger.info(\"*** testMultiplexingMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; multiplexingMapListener = new MultiplexingCustomerMapListener(); // Multiplexing MapListener listening on all entries customers.addMapListener(multiplexingMapListener); customer1 = new Customer(1, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.invoke(1, Processors.update(Customer::setAddress, \"Updated address\")); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking((MultiplexingCustomerMapListener) multiplexingMapListener).getCount(), is(3)); customers.removeMapListener(multiplexingMapListener); Create the MapListener Add the MapListener to listen for all events Mutate the customers Wait for all events Review the testSimpleMapListener code This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"java\" >Logger.info(\"*** testSimpleMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; simpleMapListener = new SimpleMapListener&lt;Integer, Customer&gt;() .addInsertHandler((e) -&gt; Logger.info(\"New Customer added with id=\" + e.getNewValue().getId())) .addDeleteHandler((e) -&gt; Logger.info(\"Deleted customer id =\" + e.getOldValue().getId())) .addInsertHandler((e) -&gt; insertCount.incrementAndGet()) .addDeleteHandler((e) -&gt; deleteCount.incrementAndGet()); customers.addMapListener(simpleMapListener, 1, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.clear(); // should only be 1 insert and 1 delete as we are listening on the key Eventually.assertThat(invoking(this).getInsertCount(), is(1)); Eventually.assertThat(invoking(this).getDeleteCount(), is(1)); Create the SimpleMapListener instance Add an insert handler to display new customers Add delete a handler to display deleted customers Add an insert handler to increment an atomic Add delete a handler to increment an atomic Register the listener on the key 1 (customer id 1) wait for all events Review the testListenOnQueries code This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"java\" >Logger.info(\"*** testListenOnQueries\"); customers.clear(); mapListener = new CustomerMapListener(); // MapListener listening only to new customers from NY Filter&lt;Customer&gt; filter = Filters.like(Customer::getAddress, \"%NY%\"); MapEventFilter&lt;Integer, Customer&gt; eventFilter = new MapEventFilter&lt;&gt;(filter); customer1 = new Customer(1, \"Tim\", \"123 James Street, Perth, Australia\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street, New York, NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customer4 = new Customer(4, \"James Stewart\", \"123 5th Ave, New York, NY\", Customer.SILVER, 200); // Listen only for events where address is in New York customers.addMapListener(mapListener, eventFilter, true); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); customers.put(customer4.getId(), customer4); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); // ensure we only receive lite events Eventually.assertThat(invoking(mapListener).getLiteEvents(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the MapListener instance Create a like filter to select only customers whose address contains NY Add the map listener and specify a MapEventFilter which takes the filter created above as well as specifying the event is lite event where the new and old values may not necessarily be present wait for all events Review the testEventTypes code This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"java\" >Logger.info(\"*** testEventTypes\"); customers.clear(); mapListener = new CustomerMapListener(); filter = Filters.equal(Customer::getCustomerType, Customer.GOLD); // listen only for events where customers has been inserted as GOLD or updated to GOLD status or were changed from GOLD int mask = MapEventFilter.E_INSERTED | MapEventFilter.E_UPDATED_ENTERED| MapEventFilter.E_UPDATED_LEFT; eventFilter = new MapEventFilter&lt;&gt;(mask, filter); customers.addMapListener(mapListener, eventFilter, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); // update customer 1 from BRONZE to GOLD customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); customers.invoke(2, Processors.update(Customer::setCustomerType, Customer.SILVER)); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(1)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the CustomerMapListener instance Create an equals filter to select only GOLD customers Create a mask for inserted events for when the filter is matched or events that are updated and now the filter matches Add the map listener and specify a MapEventFilter which takes the filter created above/ wait for all events Run the Examples Run the examples using the test case below. Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.clientevents.ClientEventsTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the various parts of the test code testStandardMapListener Output This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. Output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testStandardMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer: old key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} Insert event from new customer id 1 Insert event from new customer id 2 Update event from updating of customer 1&#8217;s credit limit Delete event containing old version of deleted customer 1 testMultiplexingMapListener Output This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=true, isDelete=false, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=null, new=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=false, isUpdate=true &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=true, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000}, new=null Insert event from new customer id 1 Update event from an update of customer 1 address Delete event from customer 1 testSimpleMapListener Output This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testSimpleMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New Customer added with id=1 &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer id =1 testListenOnQueries Output This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/null &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=4/null Both above queries only return the key because they are lite events and only customer 2 and 4 are returned as they are the only ones with NY in the address. testEventTypes Output This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testEventTypes &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='GOLD', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=2, old=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='SILVER', balance=10000} Insert event from new GOLD customer id 2 Update event changing customer type from BRONZE to GOLD for customer id 1 Update event changing customer type from GOLD to BRONZE for customer id 2 Summary In this example you have seen how to use the following features of client events: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters See Also Develop Applications using Map Events Understanding Near Caches Using Continuous Query Caches ",
            "title": "Client Events"
        },
        {
            "location": "/docs/core/06_virtual_threads",
            "text": " The main benefit virtual threads provide is that they don&#8217;t tie up platform/hardware threads while waiting for a blocking operation to complete. While most operations in Coherence never block, there are some very important ones that do, such as put (while waiting for a backup, write to persistent store, or write-through to a database to complete), and sometimes even get (while waiting for a read-through from a database). Another important example are Extend proxies, which translate remote client requests into cluster requests, and are effectively cluster clients which block on pretty much every proxied operation. Using dynamic daemon pools, and thus virtual threads in any of these scenarios makes a lot of sense, as it allows us to significantly grow daemon pool when necessary, without paying the cost associated with starting many platform threads. This can help us achieve significantly higher throughput under heavy load, and reduce the amount of time requests spend in a queue waiting for a daemon thread to free up to process them. ",
            "title": "Benefits"
        },
        {
            "location": "/docs/core/06_virtual_threads",
            "text": " Of course, there is no such thing as free lunch, and virtual threads are no exception. The biggest downside we see is that all virtual threads share the same pool of carrier platform threads to perform their processing on, and this pool can get quite busy. If you have some heavily used, critical services that you want to completely isolate and tune separately from other services, you may want to disable virtual threads either by switching to a fixed-size daemon pool, or via configuration, which I&#8217;ll discuss in a minute. Another potential issue we have seen is that there are some libraries, especially ones that write to file system directly, that don&#8217;t play nice with virtual threads and can cause excessive pinning of carrier threads and even deadlocks. If you run into situation like that, it also makes sense to disable virtual threads, either for the service in question, or globally. Finally, the tooling related to virtual threads is not quite there yet. It is hard to see what&#8217;s happening where in the debugger, profiler, and other tools you may use during development or production to better understand what&#8217;s happening in your application. In those cases it may also be beneficial to disable virtual threads, even if only temporary. ",
            "title": "Downsides"
        },
        {
            "location": "/docs/core/06_virtual_threads",
            "text": " Because of these issues, and because any other unanticipated issues that may arise with such a promising, but very new technology, we have decided to provide a way to disable virtual threads, either at the service level or globally, using configuration properties. To disable virtual threads for all services, you can specify -Dcoherence.virtualthreads.enabled=false system property when starting your Coherence JVMs (or an equivalent of that property in your Helidon, Spring, or Micronaut configuration file). To disable virtual threads for a specific service, you can specify -Dcoherence.service.&lt;serviceName&gt;.virtualthreads.enabled=false configuration property. This will allow you to continue to use dynamic daemon pool, but to use standard platform threads instead of virtual threads. Finally, you can combine the two settings, and disable virtual threads globally, but then selectively enable them for some services (Extend proxy, for example) by specifying config property such as -Dcoherence.service.Proxy.virtualthreads.enabled=true on the command line or in the config file. Ultimately, we believe that virtual threads are an amazing addition to Java that will alleviate many issues our customers are currently facing and make Coherence applications better, faster, more scalable and much easier to configure and tune in a long run. However, we also understand that they are very new, and may not work for all possible workloads. For those reasons, we have made them the default, but gave you the ability to disable them if necessary or desired. ",
            "title": "Enabling and Disabling Virtual Threads"
        },
        {
            "location": "/docs/core/06_virtual_threads",
            "text": " Starting with Coherence CE 23.09, Coherence uses virtual threads for all dynamic daemon pools, when running on Java 21 or later. Considering that all Coherence services use dynamic daemon pool by default, this means that you will likely benefit from the switch to virtual threads without having to change anything other than the Java and Coherence version you use. Benefits The main benefit virtual threads provide is that they don&#8217;t tie up platform/hardware threads while waiting for a blocking operation to complete. While most operations in Coherence never block, there are some very important ones that do, such as put (while waiting for a backup, write to persistent store, or write-through to a database to complete), and sometimes even get (while waiting for a read-through from a database). Another important example are Extend proxies, which translate remote client requests into cluster requests, and are effectively cluster clients which block on pretty much every proxied operation. Using dynamic daemon pools, and thus virtual threads in any of these scenarios makes a lot of sense, as it allows us to significantly grow daemon pool when necessary, without paying the cost associated with starting many platform threads. This can help us achieve significantly higher throughput under heavy load, and reduce the amount of time requests spend in a queue waiting for a daemon thread to free up to process them. Downsides Of course, there is no such thing as free lunch, and virtual threads are no exception. The biggest downside we see is that all virtual threads share the same pool of carrier platform threads to perform their processing on, and this pool can get quite busy. If you have some heavily used, critical services that you want to completely isolate and tune separately from other services, you may want to disable virtual threads either by switching to a fixed-size daemon pool, or via configuration, which I&#8217;ll discuss in a minute. Another potential issue we have seen is that there are some libraries, especially ones that write to file system directly, that don&#8217;t play nice with virtual threads and can cause excessive pinning of carrier threads and even deadlocks. If you run into situation like that, it also makes sense to disable virtual threads, either for the service in question, or globally. Finally, the tooling related to virtual threads is not quite there yet. It is hard to see what&#8217;s happening where in the debugger, profiler, and other tools you may use during development or production to better understand what&#8217;s happening in your application. In those cases it may also be beneficial to disable virtual threads, even if only temporary. Enabling and Disabling Virtual Threads Because of these issues, and because any other unanticipated issues that may arise with such a promising, but very new technology, we have decided to provide a way to disable virtual threads, either at the service level or globally, using configuration properties. To disable virtual threads for all services, you can specify -Dcoherence.virtualthreads.enabled=false system property when starting your Coherence JVMs (or an equivalent of that property in your Helidon, Spring, or Micronaut configuration file). To disable virtual threads for a specific service, you can specify -Dcoherence.service.&lt;serviceName&gt;.virtualthreads.enabled=false configuration property. This will allow you to continue to use dynamic daemon pool, but to use standard platform threads instead of virtual threads. Finally, you can combine the two settings, and disable virtual threads globally, but then selectively enable them for some services (Extend proxy, for example) by specifying config property such as -Dcoherence.service.Proxy.virtualthreads.enabled=true on the command line or in the config file. Ultimately, we believe that virtual threads are an amazing addition to Java that will alleviate many issues our customers are currently facing and make Coherence applications better, faster, more scalable and much easier to configure and tune in a long run. However, we also understand that they are very new, and may not work for all possible workloads. For those reasons, we have made them the default, but gave you the ability to disable them if necessary or desired. ",
            "title": "Virtual Threads Support"
        },
        {
            "location": "/coherence-docker/README",
            "text": " The Coherence image uses a distroless base image containing OpenJDK. There are many advantages of a distroless image, security being the main one. Of course, you are free to use whatever base image or build mechanism you want for your own images. The image built by the coherence-docker module contains the following Coherence components: Component Description Coherence The core Coherence server Coherence Extend A Coherence*Extend proxy, exposed on port 20000 Coherence gRPC Proxy A Coherence gRPC proxy, exposed on port 1408 Coherence Management Coherence Management over REST, exposed on port 30000 Coherence Metrics Standard Coherence metrics is installed and exposed on port 9612 , but is disabled by default. Coherence metrics can be enabled with the System property coherence.metrics.http.enabled=true Coherence Tracing Coherence tracing is configured to use a Jaeger tracing server. See the Tracing section below. ",
            "title": "Image Contents"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Assuming you have first cloned the Coherence CE project the to build the Coherence image run the following command from the top-level Maven prj/ folder: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker The name of the image produced comes from properties in the coherence-docker module pom.xml file. ${docker.registry}/coherence-ce:&lt;version&gt; Where &lt;version&gt; , is the version of the product from the pom.xml file. The ${docker.registry} property is the name of the registry that the image will be published to, by default this is oraclecoherence . So, if the version in the pom.xml is 24.09.1 the image produced will be oraclecoherence/coherence-ce:24.09.1 To change the registry name the image can be built by specifying the docker.registry property, for example: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker -Ddocker.registry=foo The example above would build an image named foo/coherence:24.09.1 ",
            "title": "Building the Image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. ",
            "title": "Run the Image in Kubernetes"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Run the image just like any other image. In Docker this command would be: <markup lang=\"bash\" >docker run -d -P oraclecoherence/coherence-ce:{version-coherence-maven} The -P parameter will ensure that the Extend, gRPC, management and metrics ports will all be exposed. By default, when started the image will run com.tangosol.net.DefaultCacheServer . This may be changed by setting the COH_MAIN_CLASS environment variable to the name of another main class. <markup lang=\"bash\" >docker run -d -P \\ -e COH_MAIN_CLASS=com.tangosol.net.DefaultCacheServer \\ oraclecoherence/coherence-ce:{version-coherence-maven} Run the Image in Kubernetes This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. ",
            "title": "Run the image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Many options in Coherence can be set from System properties prefixed with coherence. . The issue here is that System properties are not very easy to pass into the JVM in the container, whereas environment variables are. To help with this the main class which runs in the container will convert any environment variable prefixed with coherence. into a System property before it starts Coherence. <markup lang=\"bash\" >docker run -d -P \\ -e coherence.cluster=testing \\ -e coherence.role=storage \\ oraclecoherence/coherence-ce:{version-coherence-maven} The example above sets two environment variables, coherence.cluster=testing and coherence.role=storage . These will be converted to System properties so Coherence will start the same as it would if the variables had been passed to the JVM command line as -Dcoherence.cluster=testing -Dcoherence.role=storage This only applies to environment variables prefixed with coherence. that have not already set as System properties some other way. ",
            "title": "Specifying Coherence System Properties"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Images built with JIB have a fixed entrypoint configured to run the application. This is not very flexible if additional options need to be passed to the JVM. The Coherence image makes use of the JVM&#8217;s ability to load options at start-up from a file by using a JVM option @&lt;file-name&gt; . The Coherence image entrypoint contains @/args/jvm-args.txt , so the JVM will load additional options on start-up from a file named /args/jvm-args.txt . This means that additional options can be provided by adding a volume mapping that adds this file to the container. For example, to set the heap to 5g, the Coherence cluster name to test-cluster and role name to storage then additional JVM arguments will be required. Create a file named jvm-args.txt containing these properties: <markup title=\"jvm-args.txt\" >-Xms5g -Xmx5g -Dcoherence.cluster=test-cluster -Dcoherence.role=storage If the file has been created in a local directory named /home/oracle/test-args then the image can be run with the following command: <markup lang=\"bash\" >docker run -d -P -v /home/oracle/test-args:/args oraclecoherence/coherence-ce:{version-coherence-maven} This will cause Docker to mount the local /home/oracle/test-args directory to the /args directory in the container where the JVM will find the jvm-args.txt file. ",
            "title": "Specifying JVM Options"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Images built with JIB have a fixed classpath configured, which is not very flexible if additional resources need to be added to the classpath. The Coherence image maps two additional directories to the classpath that are empty in the image and may be used to add items to the classpath by mapping external volumes to these directories. The additional classpath entries are: /coherence/ext/lib/* - this will add all .jar files under the /coherence/ext/lib/ directory to the classpath /coherence/ext/conf - this adds /coherence/ext/conf to the classpath so that any classes, packages or other resource files in this directory will be added to the classpath. For example: On the local Docker host there is a folder called /dev/my-app/lib that contains .jar files to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/lib:/coherence/ext/lib oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/lib to the /coherence/ext/lib in the container so that any .jar files in the /dev/my-app/lib directory will now be on the Coherence JVM&#8217;s classpath. On the local Docker host there is a folder called /dev/my-app/classes that contains .class files and other application resources to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/classes:/coherence/ext/conf oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/classes to the /coherence/ext/conf in the container so that any classes and resource files in the /dev/my-app/classes directory will now be on the Coherence JVM&#8217;s classpath. ",
            "title": "Adding to the Classpath"
        },
        {
            "location": "/coherence-docker/README",
            "text": " This module builds an example Coherence OCI compatible image. The image built in this module is a demo and example of how to build a Coherence image using the JIB Maven Plugin . The image is not intended to be used in production deployments or as a base image, it is specifically for demos, experimentation and learning purposes. Image Contents The Coherence image uses a distroless base image containing OpenJDK. There are many advantages of a distroless image, security being the main one. Of course, you are free to use whatever base image or build mechanism you want for your own images. The image built by the coherence-docker module contains the following Coherence components: Component Description Coherence The core Coherence server Coherence Extend A Coherence*Extend proxy, exposed on port 20000 Coherence gRPC Proxy A Coherence gRPC proxy, exposed on port 1408 Coherence Management Coherence Management over REST, exposed on port 30000 Coherence Metrics Standard Coherence metrics is installed and exposed on port 9612 , but is disabled by default. Coherence metrics can be enabled with the System property coherence.metrics.http.enabled=true Coherence Tracing Coherence tracing is configured to use a Jaeger tracing server. See the Tracing section below. Building the Image Assuming you have first cloned the Coherence CE project the to build the Coherence image run the following command from the top-level Maven prj/ folder: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker The name of the image produced comes from properties in the coherence-docker module pom.xml file. ${docker.registry}/coherence-ce:&lt;version&gt; Where &lt;version&gt; , is the version of the product from the pom.xml file. The ${docker.registry} property is the name of the registry that the image will be published to, by default this is oraclecoherence . So, if the version in the pom.xml is 24.09.1 the image produced will be oraclecoherence/coherence-ce:24.09.1 To change the registry name the image can be built by specifying the docker.registry property, for example: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker -Ddocker.registry=foo The example above would build an image named foo/coherence:24.09.1 Run the image Run the image just like any other image. In Docker this command would be: <markup lang=\"bash\" >docker run -d -P oraclecoherence/coherence-ce:{version-coherence-maven} The -P parameter will ensure that the Extend, gRPC, management and metrics ports will all be exposed. By default, when started the image will run com.tangosol.net.DefaultCacheServer . This may be changed by setting the COH_MAIN_CLASS environment variable to the name of another main class. <markup lang=\"bash\" >docker run -d -P \\ -e COH_MAIN_CLASS=com.tangosol.net.DefaultCacheServer \\ oraclecoherence/coherence-ce:{version-coherence-maven} Run the Image in Kubernetes This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. Specifying Coherence System Properties Many options in Coherence can be set from System properties prefixed with coherence. . The issue here is that System properties are not very easy to pass into the JVM in the container, whereas environment variables are. To help with this the main class which runs in the container will convert any environment variable prefixed with coherence. into a System property before it starts Coherence. <markup lang=\"bash\" >docker run -d -P \\ -e coherence.cluster=testing \\ -e coherence.role=storage \\ oraclecoherence/coherence-ce:{version-coherence-maven} The example above sets two environment variables, coherence.cluster=testing and coherence.role=storage . These will be converted to System properties so Coherence will start the same as it would if the variables had been passed to the JVM command line as -Dcoherence.cluster=testing -Dcoherence.role=storage This only applies to environment variables prefixed with coherence. that have not already set as System properties some other way. Specifying JVM Options Images built with JIB have a fixed entrypoint configured to run the application. This is not very flexible if additional options need to be passed to the JVM. The Coherence image makes use of the JVM&#8217;s ability to load options at start-up from a file by using a JVM option @&lt;file-name&gt; . The Coherence image entrypoint contains @/args/jvm-args.txt , so the JVM will load additional options on start-up from a file named /args/jvm-args.txt . This means that additional options can be provided by adding a volume mapping that adds this file to the container. For example, to set the heap to 5g, the Coherence cluster name to test-cluster and role name to storage then additional JVM arguments will be required. Create a file named jvm-args.txt containing these properties: <markup title=\"jvm-args.txt\" >-Xms5g -Xmx5g -Dcoherence.cluster=test-cluster -Dcoherence.role=storage If the file has been created in a local directory named /home/oracle/test-args then the image can be run with the following command: <markup lang=\"bash\" >docker run -d -P -v /home/oracle/test-args:/args oraclecoherence/coherence-ce:{version-coherence-maven} This will cause Docker to mount the local /home/oracle/test-args directory to the /args directory in the container where the JVM will find the jvm-args.txt file. Adding to the Classpath Images built with JIB have a fixed classpath configured, which is not very flexible if additional resources need to be added to the classpath. The Coherence image maps two additional directories to the classpath that are empty in the image and may be used to add items to the classpath by mapping external volumes to these directories. The additional classpath entries are: /coherence/ext/lib/* - this will add all .jar files under the /coherence/ext/lib/ directory to the classpath /coherence/ext/conf - this adds /coherence/ext/conf to the classpath so that any classes, packages or other resource files in this directory will be added to the classpath. For example: On the local Docker host there is a folder called /dev/my-app/lib that contains .jar files to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/lib:/coherence/ext/lib oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/lib to the /coherence/ext/lib in the container so that any .jar files in the /dev/my-app/lib directory will now be on the Coherence JVM&#8217;s classpath. On the local Docker host there is a folder called /dev/my-app/classes that contains .class files and other application resources to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/classes:/coherence/ext/conf oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/classes to the /coherence/ext/conf in the container so that any classes and resource files in the /dev/my-app/classes directory will now be on the Coherence JVM&#8217;s classpath. ",
            "title": "Coherence OCI Image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Multiple containers can be started to form a cluster. By default, Coherence uses multi-cast for cluster discovery but in containers this either will not work, or is not reliable, so well-known-addressing can be used. This example is going to use basic Docker commands and links between containers. There are other ways to achieve the same sort of functionality depending on the network configurations you want to use in Docker. First, determine the name to be used for the first container, in this example it will be storage-1 . Next, create a ` Start the first container in the cluster: <markup lang=\"bash\" >docker run -d -P \\ --name storage-1 \\ --hostname storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} The first container has been started with a container name of storage-1 , and the host name also set to storage-1 . The container sets the WKA host name to storage-1 using -e coherence.wka=storage-1 (this will be converted to the System property coherence.wka=storage-1 see Specifying Coherence System Properties above). The container sets the Coherence cluster name to testing using -e coherence.cluster=testing (this will be converted to the System property coherence.cluster=testing see Specifying Coherence System Properties above). The important part here is that the container has a name, and the --hostname option has also been set. This will allow the subsequent cluster members to find this container. Now, subsequent containers can be started using the same cluster name and WKA host name, but with different container names and a link to the first container, all the containers will form a single Coherence cluster: <markup lang=\"bash\" >docker run -d -P \\ --name storage-2 \\ --link storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} docker run -d -P \\ --name storage-3 \\ --link storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} Two more containers, storage-2 and storage-3 will now be part of the cluster. All the members must have a --link option to the first container and have the same WKA and cluster name properties. ",
            "title": "Clustering"
        },
        {
            "location": "/coherence-docker/README",
            "text": " The Coherence image comes with tracing already configured, it just requires a suitable Jaeger server to send spans to. The simplest way to start is deploy the Jaeger all-in-one server, for example: <markup lang=\"bash\" >docker run -d --name jaeger \\ -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\ -p 5775:5775/udp \\ -p 6831:6831/udp \\ -p 6832:6832/udp \\ -p 5778:5778 \\ -p 16686:16686 \\ -p 14268:14268 \\ -p 14250:14250 \\ -p 9411:9411 \\ jaegertracing/all-in-one:latest The Jaeger UI will be available to browse to at http://127.0.0.1:16686 Jaeger has been started with a container name of jaeger , so it will be discoverable using that host name by the Coherence containers. Start the Coherence container with a link to the Jaeger container and set the JAEGER_AGENT_HOST environment variable to jaeger : <markup lang=\"bash\" >docker run -d -P --link jaeger \\ -e JAEGER_AGENT_HOST=jaeger \\ oraclecoherence/coherence-ce:{version-coherence-maven} Once the Coherence container is running perform some interactions with it using one of the exposed services, i.e Extend or gRPC, and spans will be sent to the Jaeger collector and will be visible in the UI by querying for the coherence service name. The service name used can be changed by setting the JAEGER_SERVICE_NAME environment variable when starting the container, for example: <markup lang=\"bash\" >docker run -d -P --link jaeger \\ -e JAEGER_AGENT_HOST=jaeger \\ -e JAEGER_SERVICE_NAME=coherence-test oraclecoherence/coherence-ce:{version-coherence-maven} Spans will now be sent to Jaeger with the service name coherence-test . Tracing is very useful to show what happens under the covers for a given Coherence API call. Traces are more interesting when they come from a Coherence cluster with multiple members, where the traces span different cluster members. This can easily be done by running multiple containers with tracing enabled and configuring Clustering as described above. ",
            "title": "Tracing"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " The example code is written as a set of unit tests. This guide will walk the reader through the code that obtains and uses a RemoteExecutor . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " While the Coherence Executor service offers a default executor, which will be demonstrated, this guide will also show how to configure and use a custom Executor. Thus, we&#8217;ll begin with the configuration of a fixed-size thread pool that the Executor service may submit tasks to. Custom thread pool definitions are defined within a cache configuration resource. For the purpose of this guide, the resource will be called custom-executors.xml which will be placed in src/test/resources : <markup lang=\"xml\" >&lt;cache-config xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\"&gt; &lt;c:fixed&gt; &lt;c:name&gt;fixed-5&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;dist-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;dist-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Defines the NamespaceHandler which validates and configures the executor services Defines a fixed thread pool (think Executors.newFixedThreadPool(int) ) Gives the thread pool a logical name of fixed-5 . This name will be used to obtain a reference to a RemoteExecutor Defines the number of threads this pool should have Each Coherence member using this configuration will have a local executor service defined with the logical name of fixed-5 . If there are multiple executor services with the same logical identifier, tasks will be submitted to a random member&#8217;s Executor Service managing the named pool. ",
            "title": "Executor Configuration"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " The first step is to create the test class that will show and test the various NamedMap operations, we’ll call this class ExecutorBasicTests: <markup lang=\"java\" >class ExecutorBasicTests { } ",
            "title": "Create the Test Class"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, this can be accomplished in a static @BeforeAll annotated setup method. <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { System.setProperty(\"coherence.wka\", \"127.0.0.1\"); System.setProperty(\"coherence.cacheconfig\", \"custom-executors.xml\"); Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Pass in the cache configuration, custom-executors.xml , created previously in this guide. Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, add a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } Since only a single default Coherence instance was created, obtain that instance with the Coherence.getInstance() method, and then close it. Now that the basic framework of the test is in place, tests may now be added to demonstrate some simple api usages of RemoteExecutor . ",
            "title": "Bootstrap Coherence"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " The first test will demonstrate submitting a single Remote.Runnable task to the grid. This task will add an entry to a NamedCache which will allow the test to verify the task was properly run. <markup lang=\"java\" > @Test void testSimpleRunnable() throws Exception { NamedMap&lt;String, String&gt; map = getMap(); RemoteExecutor defaultExecutor = RemoteExecutor.getDefault(); map.truncate(); assertTrue(map.isEmpty()); Future&lt;?&gt; result = defaultExecutor.submit((Remote.Runnable) ()-&gt; Coherence.getInstance() .getSession().getMap(\"data\").put(\"key-1\", \"value-1\")); result.get(); String sValue = map.get(\"key-1\"); assertEquals(sValue, \"value-1\"); } Obtain a local reference to the NamedMap , data . Obtain a reference to the default RemoteExecutor . Truncate the map to ensure no data is present. Submit a Remote.Runnable to the grid which will obtain a reference to the NamedMap , data , and insert an entry. The map reference isn&#8217;t used in the lambda as the Remote.Runnable may be executed in a remote JVM, therefore, a reference local to the executing JVM is obtained instead. Wait for the Future to complete. Get the value for key-1 from the local NamedMap reference. Assert the expected value was returned. ",
            "title": "Submitting a Runnable to the Grid"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " The next test will demonstrate submitting a single Remote.Callable task to the grid. The test will first add an entry to the cache. Next, it will submit a Remote.Callable that will change the value for the previously added entry and return the previous value. Finally, the test will ensure the current cache value and the value returned by the Remote.Callable are the expected values. This task will add an entry to a NamedCache which will allow the test to verify the task was properly run. <markup lang=\"java\" > @Test void testSimpleCallable() throws Exception { NamedMap&lt;String, String&gt; map = getMap(); RemoteExecutor defaultExecutor = RemoteExecutor.getDefault(); map.truncate(); assertTrue(map.isEmpty()); map.put(\"key-1\", \"value-1\"); Future&lt;String&gt; result = defaultExecutor.submit((Remote.Callable&lt;String&gt;) ()-&gt; (String) Coherence.getInstance().getSession().getMap(\"data\").put(\"key-1\", \"value-2\")); String sResult = result.get(); String sValue = map.get(\"key-1\"); assertEquals(sResult, \"value-1\"); assertEquals(sValue, \"value-2\"); } Obtain a local reference to the NamedMap , data . Obtain a reference to the default RemoteExecutor . Truncate the map to ensure no data is present. Insert an entry that the task should change. Submit a Remote.Callable to the grid which will obtain a reference to the NamedMap , data , update the existing entry, and return the previous value. Wait for the Future to complete and obtain the returned value. Get the value for key-1 from the local NamedMap reference. Assert the expected value was returned from the Remote.Callable execution. Assert the cache has the value updated by the Remote.Callable . ",
            "title": "Submitting a Callable to the Grid"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " The last test will use the thread pool fixed-5 that was configured earlier within this guide. As this thread pool has five threads, this test will submit several Remote.Callable instances that will, upon execution, wait for one second, then return the name of the executing thread. The test will then obtain the results for the multiple Remote.Callable executions, and verify all five threads were used. <markup lang=\"java\" > @Test void testCustomExecutor() throws Exception { RemoteExecutor fixed5 = RemoteExecutor.get(\"fixed-5\"); List&lt;Remote.Callable&lt;String&gt;&gt; listCallables = new ArrayList&lt;&gt;(5); for (int i = 0; i &lt; 10; i++) { listCallables.add(()-&gt; { Thread.sleep(1000); return Thread.currentThread().getName(); }); } List&lt;Future&lt;String&gt;&gt; listFutures = fixed5.invokeAll(listCallables); Set&lt;String&gt; results = new LinkedHashSet&lt;&gt;(); for (Future&lt;String&gt; listFuture : listFutures) { results.add(listFuture.get()); } System.out.printf(\"Tasks executed on threads %s\", results); assertEquals(5, results.size()); } Obtain a reference to the fixed-5 RemoteExecutor . Create a list of ten Remote.Callable instances where each instance sleeps for one second and then returns the name of the executing thread. Invoke all Remote.Callable instances within the list by calling RemoteExecutor.invokeAll which returns a List containing a Future for each Remote.Callable . Create a Set of the execution thread names. As this thread pool servicing these tasks has five threads, this Set should only have five entries. Assert that all five threads of the pool were used. ",
            "title": "Submitting a Task to a Specific Thread Pool in the Grid"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " These tests demonstrate the basic usage of the Coherence Executor service. Developers are encouraged to explore the other functionality defined by RemoteExecutor ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " The Javadoc for RemoteExecutor ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/510-executor/README",
            "text": " The Coherence Executor service allows cluster members or *Extend clients to submit arbitrary tasks to the grid for execution. Cluster members may define one or more custom Executors via configuration to support submission of tasks to different executors based on the required work load. The functionality offered by the Coherence Executor service is through the RemoteExecutor class, which, upon inspection, should look similar to java.util.concurrent.Executors in the JDK. Also notice that RemoteExecutor doesn&#8217;t use Runnable or Callable , and instead uses Remote.Runnable and Remote.Callable . The remote versions are functionally equivalent, but are Serializable . Serialization is necessary as these tasks may be dispatched to a remote JVM for execution. Additionally, internally, the tasks are stored within Coherence caches (which requires keys/values to be Serializable) to allow task re-execution of a member executing as task fails (for example, the member dies) - the task is still within the cache ready to be dispatched to another member for execution. What You Will Build The example code is written as a set of unit tests. This guide will walk the reader through the code that obtains and uses a RemoteExecutor . What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Executor Configuration While the Coherence Executor service offers a default executor, which will be demonstrated, this guide will also show how to configure and use a custom Executor. Thus, we&#8217;ll begin with the configuration of a fixed-size thread pool that the Executor service may submit tasks to. Custom thread pool definitions are defined within a cache configuration resource. For the purpose of this guide, the resource will be called custom-executors.xml which will be placed in src/test/resources : <markup lang=\"xml\" >&lt;cache-config xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\"&gt; &lt;c:fixed&gt; &lt;c:name&gt;fixed-5&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;dist-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;dist-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Defines the NamespaceHandler which validates and configures the executor services Defines a fixed thread pool (think Executors.newFixedThreadPool(int) ) Gives the thread pool a logical name of fixed-5 . This name will be used to obtain a reference to a RemoteExecutor Defines the number of threads this pool should have Each Coherence member using this configuration will have a local executor service defined with the logical name of fixed-5 . If there are multiple executor services with the same logical identifier, tasks will be submitted to a random member&#8217;s Executor Service managing the named pool. Create the Test Class The first step is to create the test class that will show and test the various NamedMap operations, we’ll call this class ExecutorBasicTests: <markup lang=\"java\" >class ExecutorBasicTests { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, this can be accomplished in a static @BeforeAll annotated setup method. <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { System.setProperty(\"coherence.wka\", \"127.0.0.1\"); System.setProperty(\"coherence.cacheconfig\", \"custom-executors.xml\"); Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Pass in the cache configuration, custom-executors.xml , created previously in this guide. Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, add a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } Since only a single default Coherence instance was created, obtain that instance with the Coherence.getInstance() method, and then close it. Now that the basic framework of the test is in place, tests may now be added to demonstrate some simple api usages of RemoteExecutor . Submitting a Runnable to the Grid The first test will demonstrate submitting a single Remote.Runnable task to the grid. This task will add an entry to a NamedCache which will allow the test to verify the task was properly run. <markup lang=\"java\" > @Test void testSimpleRunnable() throws Exception { NamedMap&lt;String, String&gt; map = getMap(); RemoteExecutor defaultExecutor = RemoteExecutor.getDefault(); map.truncate(); assertTrue(map.isEmpty()); Future&lt;?&gt; result = defaultExecutor.submit((Remote.Runnable) ()-&gt; Coherence.getInstance() .getSession().getMap(\"data\").put(\"key-1\", \"value-1\")); result.get(); String sValue = map.get(\"key-1\"); assertEquals(sValue, \"value-1\"); } Obtain a local reference to the NamedMap , data . Obtain a reference to the default RemoteExecutor . Truncate the map to ensure no data is present. Submit a Remote.Runnable to the grid which will obtain a reference to the NamedMap , data , and insert an entry. The map reference isn&#8217;t used in the lambda as the Remote.Runnable may be executed in a remote JVM, therefore, a reference local to the executing JVM is obtained instead. Wait for the Future to complete. Get the value for key-1 from the local NamedMap reference. Assert the expected value was returned. Submitting a Callable to the Grid The next test will demonstrate submitting a single Remote.Callable task to the grid. The test will first add an entry to the cache. Next, it will submit a Remote.Callable that will change the value for the previously added entry and return the previous value. Finally, the test will ensure the current cache value and the value returned by the Remote.Callable are the expected values. This task will add an entry to a NamedCache which will allow the test to verify the task was properly run. <markup lang=\"java\" > @Test void testSimpleCallable() throws Exception { NamedMap&lt;String, String&gt; map = getMap(); RemoteExecutor defaultExecutor = RemoteExecutor.getDefault(); map.truncate(); assertTrue(map.isEmpty()); map.put(\"key-1\", \"value-1\"); Future&lt;String&gt; result = defaultExecutor.submit((Remote.Callable&lt;String&gt;) ()-&gt; (String) Coherence.getInstance().getSession().getMap(\"data\").put(\"key-1\", \"value-2\")); String sResult = result.get(); String sValue = map.get(\"key-1\"); assertEquals(sResult, \"value-1\"); assertEquals(sValue, \"value-2\"); } Obtain a local reference to the NamedMap , data . Obtain a reference to the default RemoteExecutor . Truncate the map to ensure no data is present. Insert an entry that the task should change. Submit a Remote.Callable to the grid which will obtain a reference to the NamedMap , data , update the existing entry, and return the previous value. Wait for the Future to complete and obtain the returned value. Get the value for key-1 from the local NamedMap reference. Assert the expected value was returned from the Remote.Callable execution. Assert the cache has the value updated by the Remote.Callable . Submitting a Task to a Specific Thread Pool in the Grid The last test will use the thread pool fixed-5 that was configured earlier within this guide. As this thread pool has five threads, this test will submit several Remote.Callable instances that will, upon execution, wait for one second, then return the name of the executing thread. The test will then obtain the results for the multiple Remote.Callable executions, and verify all five threads were used. <markup lang=\"java\" > @Test void testCustomExecutor() throws Exception { RemoteExecutor fixed5 = RemoteExecutor.get(\"fixed-5\"); List&lt;Remote.Callable&lt;String&gt;&gt; listCallables = new ArrayList&lt;&gt;(5); for (int i = 0; i &lt; 10; i++) { listCallables.add(()-&gt; { Thread.sleep(1000); return Thread.currentThread().getName(); }); } List&lt;Future&lt;String&gt;&gt; listFutures = fixed5.invokeAll(listCallables); Set&lt;String&gt; results = new LinkedHashSet&lt;&gt;(); for (Future&lt;String&gt; listFuture : listFutures) { results.add(listFuture.get()); } System.out.printf(\"Tasks executed on threads %s\", results); assertEquals(5, results.size()); } Obtain a reference to the fixed-5 RemoteExecutor . Create a list of ten Remote.Callable instances where each instance sleeps for one second and then returns the name of the executing thread. Invoke all Remote.Callable instances within the list by calling RemoteExecutor.invokeAll which returns a List containing a Future for each Remote.Callable . Create a Set of the execution thread names. As this thread pool servicing these tasks has five threads, this Set should only have five entries. Assert that all five threads of the pool were used. Summary These tests demonstrate the basic usage of the Coherence Executor service. Developers are encouraged to explore the other functionality defined by RemoteExecutor See Also The Javadoc for RemoteExecutor ",
            "title": "The Coherence Executor Service"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Classes Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " In this example you will run a test that will demonstrate using Durable Events. The test does the following: Starts 2 Cache Servers using Oracle Bedrock Creates and registers a version aware MapListener Inserts, updates and deletes cache entries Simulates the client being disconnected Issues cache mutations remotely while the client is disconnected Reconnects the client and validate that events generated while the client was disconnected are received To enable Durable Events you must have the following system properties set for cache servers: Enable active persistence by using -Dcoherence.distributed.persistence.mode=active Set the directory to store Durable Events using -Dcoherence.distributed.persistence.events.dir=/my/events/dir Optionally set the directory to store active persistence using -Dcoherence.distributed.persistence.base.dir=/my/persistence/dir Register a versioned MapListener on a NamedMap If you do not set the directory to store active persistence the default directory coherence off the users home directory will be chosen. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " Review the Customer class This example uses the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private long id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review how the 2 cache servers are started by Oracle Bedrock <markup lang=\"java\" >/** * Start a Coherence cluster with two cache servers using Oracle Bedrock. * * @throws IOException if any errors creating temporary directory */ @BeforeAll public static void startup() throws IOException { persistenceDir = FileHelper.createTempDir(); File eventsDir = new File(persistenceDir, \"events\"); CoherenceClusterBuilder builder = new CoherenceClusterBuilder() .with(SystemProperty.of(\"coherence.distributed.partitions\", 23), SystemProperty.of(\"coherence.distributed.persistence.mode\", \"active\"), SystemProperty.of(\"coherence.distributed.persistence.base.dir\", persistenceDir.getAbsolutePath()), SystemProperty.of(\"coherence.distributed.persistence.events.dir\", eventsDir.getAbsolutePath()), ClusterName.of(CLUSTER_NAME), RoleName.of(\"storage\"), DisplayName.of(\"storage\"), Multicast.ttl(0)) .include(2, CoherenceClusterMember.class, // testLogs, LocalStorage.enabled()); cluster = builder.build(); Eventually.assertDeferred(() -&gt; cluster.getClusterSize(), is(2)); for (CoherenceClusterMember member : cluster) { Eventually.assertDeferred(member::isReady, is(true)); } } Set the partition count to 23 to reduce the startup time Set active persistence mode Set the base directory to store persistence files Set the base directory to store persistence events Review the DurableEventsTest class <markup lang=\"java\" >/** * Runs a test to simulate a client registering a versioned {@link MapListener}, * being disconnected, reconnecting, and then receiving all the events that were * missed while the client was disconnected. */ @Test public void testDurableEvents() throws Exception { AtomicInteger eventCount = new AtomicInteger(); String cacheName = \"customers\"; System.setProperty(\"coherence.cluster\", CLUSTER_NAME); System.setProperty(\"coherence.role\", \"client\"); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); try (Coherence coherence = Coherence.clusterMember()) { coherence.start().get(5, TimeUnit.MINUTES); NamedMap&lt;Long, Customer&gt; customers = coherence.getSession().getMap(cacheName); MapListener&lt;Long, Customer&gt; mapListener = new SimpleMapListener&lt;Long, Customer&gt;() .addEventHandler(System.out::println) .addEventHandler((e) -&gt; eventCount.incrementAndGet()) .versioned(); customers.addMapListener(mapListener); Logger.info(\"Added Map Listener, generating 3 events\"); // generate 3 events, insert, update and delete Customer customer = new Customer(100L, \"Customer 100\", \"Address\", Customer.GOLD, 5000); customers.put(customer.getId(), customer); customers.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customers.remove(100L); // wait until we receive first three events Eventually.assertDeferred(eventCount::get, is(3)); // cause a service distribution for PartitionedCache service to simulate disc Logger.info(\"Disconnecting client\"); causeServiceDisruption(customers); Logger.info(\"Remotely insert, update and delete a new customer\"); // do a remote invocation to insert, update and delete a customer. This is done // remotely via Oracle Bedrock as not to reconnect the client cluster.getAny().invoke(() -&gt; { NamedMap&lt;Long, Customer&gt; customerMap = CacheFactory.getCache(cacheName); Customer newCustomer = new Customer(100L, \"Customer 101\", \"Customer address\", Customer.SILVER, 100); customerMap.put(newCustomer.getId(), newCustomer); customerMap.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customerMap.remove(100L); return null; }); // Events should still only be 3 as client has not yet reconnected Eventually.assertDeferred(eventCount::get, is(3)); Logger.info(\"Issuing size to reconnect client\"); // issue an operation that will cause a service restart and listener to be re-registered customers.size(); // we should now see the 3 events we missed because we were disconnected Eventually.assertDeferred(eventCount::get, is(6)); } } Set system properties for the client Create a new SimpleMapListener Add an event handler to output the events received Add an event handler to increment the number of events received Indicate that this MapListener is versioned Add the MapListener to the NamedMap Simulate the client being disconnected by stopping the service for the NamedMap Generate 3 new events remotely on one of the members Issue an operator that will cause the client to restart and re-register the listener Assert that we now see the additional 3 events that were generated while the client was disconnected ",
            "title": "Review the Classes"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " You can run the test in one of three ways: Using your IDE to run DurableEventsTest class Using Maven via ./mvnw clean verify Using Gradle via ./gradlew test After initial cache server startup, you will see output similar to the following: Timestamps have been removed and output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=3): Added Map Listener, generating 3 events ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, partition=20, version=1} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, new value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=2} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=3} &lt;Info&gt; (thread=main, member=3): Disconnecting client &lt;Info&gt; (thread=main, member=3): Remotely insert, update and delete a new customer &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache left the cluster &lt;Info&gt; (thread=main, member=3): Issuing size to reconnect client &lt;Info&gt; (thread=main, member=3): Restarting NamedCache: customers &lt;Info&gt; (thread=main, member=3): Restarting Service: PartitionedCache &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache joined the cluster with senior service member 1 ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, partition=20, version=4} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, new value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=5} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=6} Adding the versioned SimpleMapListener Output of three events while the client is connected Message indicating we are disconnecting client Service for the client leaving as it is disconnected Restarting the cache and service due to size() request which will also automatically re-register the MapListener Client now receives the events it missed during disconnect ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " In this example you ran a test that demonstrated using Durable Events by: Starting 2 Cache Servers using Oracle Bedrock Creating and registering a version aware MapListener Inserting, updating and deleting cache entries Simulating the client being disconnected Issuing cache mutations remotely while the client is disconnected Reconnecting the client and validate that events generated while the client was disconnected are received ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " Durable Events Documentation Client Events Develop Applications using Map Events ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " Coherence provides the MapListener interface as described in Client Events , where clients can sign up for events from any Coherence NamedMap . With traditional client events, if a client disconnects for any reason and then reconnects and automatically re-registers a MapListener , it will miss any events that were sent during that disconnected time. Durable Events is a new (experimental) feature that allows clients to create a versioned listener which will allow a client, if disconnected, to receive events missed while they were in a disconnected state. As for standard `MapListener`s you are able to register for all events, events based upon a filter or events for a specific key. More advanced use cases for Durable Events include the ability to replay all events for a NamedMap . Please see Durable Events Documentation for more information on Durable Events. Durable events are an experimental feature only and should not be used in production environments. Durable Events are not yet supported for Coherence*Extend clients. Table of Contents What You Will Build What You Need Building the Example Code Review the Classes Run the Examples Summary See Also What You Will Build In this example you will run a test that will demonstrate using Durable Events. The test does the following: Starts 2 Cache Servers using Oracle Bedrock Creates and registers a version aware MapListener Inserts, updates and deletes cache entries Simulates the client being disconnected Issues cache mutations remotely while the client is disconnected Reconnects the client and validate that events generated while the client was disconnected are received To enable Durable Events you must have the following system properties set for cache servers: Enable active persistence by using -Dcoherence.distributed.persistence.mode=active Set the directory to store Durable Events using -Dcoherence.distributed.persistence.events.dir=/my/events/dir Optionally set the directory to store active persistence using -Dcoherence.distributed.persistence.base.dir=/my/persistence/dir Register a versioned MapListener on a NamedMap If you do not set the directory to store active persistence the default directory coherence off the users home directory will be chosen. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. Review the Classes Review the Customer class This example uses the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private long id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review how the 2 cache servers are started by Oracle Bedrock <markup lang=\"java\" >/** * Start a Coherence cluster with two cache servers using Oracle Bedrock. * * @throws IOException if any errors creating temporary directory */ @BeforeAll public static void startup() throws IOException { persistenceDir = FileHelper.createTempDir(); File eventsDir = new File(persistenceDir, \"events\"); CoherenceClusterBuilder builder = new CoherenceClusterBuilder() .with(SystemProperty.of(\"coherence.distributed.partitions\", 23), SystemProperty.of(\"coherence.distributed.persistence.mode\", \"active\"), SystemProperty.of(\"coherence.distributed.persistence.base.dir\", persistenceDir.getAbsolutePath()), SystemProperty.of(\"coherence.distributed.persistence.events.dir\", eventsDir.getAbsolutePath()), ClusterName.of(CLUSTER_NAME), RoleName.of(\"storage\"), DisplayName.of(\"storage\"), Multicast.ttl(0)) .include(2, CoherenceClusterMember.class, // testLogs, LocalStorage.enabled()); cluster = builder.build(); Eventually.assertDeferred(() -&gt; cluster.getClusterSize(), is(2)); for (CoherenceClusterMember member : cluster) { Eventually.assertDeferred(member::isReady, is(true)); } } Set the partition count to 23 to reduce the startup time Set active persistence mode Set the base directory to store persistence files Set the base directory to store persistence events Review the DurableEventsTest class <markup lang=\"java\" >/** * Runs a test to simulate a client registering a versioned {@link MapListener}, * being disconnected, reconnecting, and then receiving all the events that were * missed while the client was disconnected. */ @Test public void testDurableEvents() throws Exception { AtomicInteger eventCount = new AtomicInteger(); String cacheName = \"customers\"; System.setProperty(\"coherence.cluster\", CLUSTER_NAME); System.setProperty(\"coherence.role\", \"client\"); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); try (Coherence coherence = Coherence.clusterMember()) { coherence.start().get(5, TimeUnit.MINUTES); NamedMap&lt;Long, Customer&gt; customers = coherence.getSession().getMap(cacheName); MapListener&lt;Long, Customer&gt; mapListener = new SimpleMapListener&lt;Long, Customer&gt;() .addEventHandler(System.out::println) .addEventHandler((e) -&gt; eventCount.incrementAndGet()) .versioned(); customers.addMapListener(mapListener); Logger.info(\"Added Map Listener, generating 3 events\"); // generate 3 events, insert, update and delete Customer customer = new Customer(100L, \"Customer 100\", \"Address\", Customer.GOLD, 5000); customers.put(customer.getId(), customer); customers.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customers.remove(100L); // wait until we receive first three events Eventually.assertDeferred(eventCount::get, is(3)); // cause a service distribution for PartitionedCache service to simulate disc Logger.info(\"Disconnecting client\"); causeServiceDisruption(customers); Logger.info(\"Remotely insert, update and delete a new customer\"); // do a remote invocation to insert, update and delete a customer. This is done // remotely via Oracle Bedrock as not to reconnect the client cluster.getAny().invoke(() -&gt; { NamedMap&lt;Long, Customer&gt; customerMap = CacheFactory.getCache(cacheName); Customer newCustomer = new Customer(100L, \"Customer 101\", \"Customer address\", Customer.SILVER, 100); customerMap.put(newCustomer.getId(), newCustomer); customerMap.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customerMap.remove(100L); return null; }); // Events should still only be 3 as client has not yet reconnected Eventually.assertDeferred(eventCount::get, is(3)); Logger.info(\"Issuing size to reconnect client\"); // issue an operation that will cause a service restart and listener to be re-registered customers.size(); // we should now see the 3 events we missed because we were disconnected Eventually.assertDeferred(eventCount::get, is(6)); } } Set system properties for the client Create a new SimpleMapListener Add an event handler to output the events received Add an event handler to increment the number of events received Indicate that this MapListener is versioned Add the MapListener to the NamedMap Simulate the client being disconnected by stopping the service for the NamedMap Generate 3 new events remotely on one of the members Issue an operator that will cause the client to restart and re-register the listener Assert that we now see the additional 3 events that were generated while the client was disconnected Run the Examples You can run the test in one of three ways: Using your IDE to run DurableEventsTest class Using Maven via ./mvnw clean verify Using Gradle via ./gradlew test After initial cache server startup, you will see output similar to the following: Timestamps have been removed and output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=3): Added Map Listener, generating 3 events ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, partition=20, version=1} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, new value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=2} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=3} &lt;Info&gt; (thread=main, member=3): Disconnecting client &lt;Info&gt; (thread=main, member=3): Remotely insert, update and delete a new customer &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache left the cluster &lt;Info&gt; (thread=main, member=3): Issuing size to reconnect client &lt;Info&gt; (thread=main, member=3): Restarting NamedCache: customers &lt;Info&gt; (thread=main, member=3): Restarting Service: PartitionedCache &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache joined the cluster with senior service member 1 ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, partition=20, version=4} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, new value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=5} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=6} Adding the versioned SimpleMapListener Output of three events while the client is connected Message indicating we are disconnecting client Service for the client leaving as it is disconnected Restarting the cache and service due to size() request which will also automatically re-register the MapListener Client now receives the events it missed during disconnect Summary In this example you ran a test that demonstrated using Durable Events by: Starting 2 Cache Servers using Oracle Bedrock Creating and registering a version aware MapListener Inserting, updating and deleting cache entries Simulating the client being disconnected Issuing cache mutations remotely while the client is disconnected Reconnecting the client and validate that events generated while the client was disconnected are received See Also Durable Events Documentation Client Events Develop Applications using Map Events ",
            "title": "Durable Events"
        },
        {
            "location": "/docs/core/03_microprofile_health",
            "text": " To use Coherence MP Health, you should first declare it as a dependency in the project&#8217;s pom.xml file. You can declare Coherence MP Health as follows: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-health&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;/dependency&gt; Where ${coherence.groupId} is the Maven groupId for the Coherence edition being used, com.oracle.coherence for the commercial edition or com.oracle.coherence.ce for the community edition. And ${coherence.version} is the version of Coherence being used. After the module becomes available in the class path, the Coherence HealthCheck producer CDI bean will be automatically discovered and be registered as a Microprofile health check provider. The Coherence health checks will then be available via any health endpoints served by the application and included in started, readiness and liveness checks. ",
            "title": "Enabling the Use of Coherence MP Health"
        },
        {
            "location": "/docs/core/03_microprofile_health",
            "text": " Coherence MicroProfile (MP) Health provides support for Eclipse MicroProfile Health within the Coherence cluster members. See the documentation on the Coherence Health Check API and MicroProfile Health . Coherence MP Health is a very simple module that enables you to publish Coherence health checks into the MicroProfile Health Check Registries available at runtime. Enabling the Use of Coherence MP Health To use Coherence MP Health, you should first declare it as a dependency in the project&#8217;s pom.xml file. You can declare Coherence MP Health as follows: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-health&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;/dependency&gt; Where ${coherence.groupId} is the Maven groupId for the Coherence edition being used, com.oracle.coherence for the commercial edition or com.oracle.coherence.ce for the community edition. And ${coherence.version} is the version of Coherence being used. After the module becomes available in the class path, the Coherence HealthCheck producer CDI bean will be automatically discovered and be registered as a Microprofile health check provider. The Coherence health checks will then be available via any health endpoints served by the application and included in started, readiness and liveness checks. ",
            "title": "Microprofile Health"
        },
        {
            "location": "/docs/core/10_grpc",
            "text": " To set up and start using the Netty Coherence gRPC Server, you should declare the coherence-grpc-proxy module as a dependency of your project. For example: If using Maven, declare the server as follows: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; In the pom.xml file, coherence.version property is the version of Coherence being used, and coherence.groupId property is either the Coherence commercial group id, com.oracle.coherence, or the CE group id, com.oracle.coherence.ce. If using Gradle, declare the server as follows: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy\" } In the build.gradle file, coherenceVersion property is the version of Coherence being used, and coherenceGroupId property is either the Coherence commercial group id, com.oracle.coherence or the CE group id, com.oracle.coherence.ce. ",
            "title": "Setting Up the Netty Coherence gRPC Proxy Server"
        },
        {
            "location": "/docs/core/10_grpc",
            "text": " Applications that are not using Helidon 4+ that wish to run the Coherence gRPC proxy need to use the Netty based Coherence gRPC proxy module coherence-grpc-proxy . Setting Up the Netty Coherence gRPC Proxy Server To set up and start using the Netty Coherence gRPC Server, you should declare the coherence-grpc-proxy module as a dependency of your project. For example: If using Maven, declare the server as follows: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; In the pom.xml file, coherence.version property is the version of Coherence being used, and coherence.groupId property is either the Coherence commercial group id, com.oracle.coherence, or the CE group id, com.oracle.coherence.ce. If using Gradle, declare the server as follows: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy\" } In the build.gradle file, coherenceVersion property is the version of Coherence being used, and coherenceGroupId property is either the Coherence commercial group id, com.oracle.coherence or the CE group id, com.oracle.coherence.ce. ",
            "title": "Using Coherence gRPC Proxy With Netty"
        },
        {
            "location": "/docs/core/10_grpc",
            "text": " To set up and start using the Helidon Coherence gRPC Server, you should declare the coherence-grpc-proxy-helidon module as a dependency of your project. For example: If using Maven, declare the server as follows: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy-helidon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; In the pom.xml file, coherence.version property is the version of Coherence being used, and coherence.groupId property is either the Coherence commercial group id, com.oracle.coherence, or the CE group id, com.oracle.coherence.ce. If using Gradle, declare the server as follows: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy-helidon\" } In the build.gradle file, coherenceVersion property is the version of Coherence being used, and coherenceGroupId property is either the Coherence commercial group id, com.oracle.coherence or the CE group id, com.oracle.coherence.ce. Note If both the coherence-grpc-proxy-helidon module and the coherence-grpc-proxy module are on the class path, the Helidon gRPC server will be used. ",
            "title": "Setting Up the Helidon Coherence gRPC Proxy Server"
        },
        {
            "location": "/docs/core/10_grpc",
            "text": " Applications that are using Helidon 4+ that wish to run the Coherence gRPC proxy have the option to use Helidon&#8217;s gRPC implementation for the gRPC server. Te Coherence gRPC Proxy server will run its own Helidon server to serve the Coherence gRPC requests, this will be separate from any other Helidon web servers that the application might be running. Setting Up the Helidon Coherence gRPC Proxy Server To set up and start using the Helidon Coherence gRPC Server, you should declare the coherence-grpc-proxy-helidon module as a dependency of your project. For example: If using Maven, declare the server as follows: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy-helidon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; In the pom.xml file, coherence.version property is the version of Coherence being used, and coherence.groupId property is either the Coherence commercial group id, com.oracle.coherence, or the CE group id, com.oracle.coherence.ce. If using Gradle, declare the server as follows: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy-helidon\" } In the build.gradle file, coherenceVersion property is the version of Coherence being used, and coherenceGroupId property is either the Coherence commercial group id, com.oracle.coherence or the CE group id, com.oracle.coherence.ce. Note If both the coherence-grpc-proxy-helidon module and the coherence-grpc-proxy module are on the class path, the Helidon gRPC server will be used. ",
            "title": "Using Coherence gRPC Proxy With Helidon 4+"
        },
        {
            "location": "/docs/core/10_grpc",
            "text": " The Coherence gRPC proxy server can run with either of two gRPC implementations. Netty Helidon 4+ Using Coherence gRPC Proxy With Netty Applications that are not using Helidon 4+ that wish to run the Coherence gRPC proxy need to use the Netty based Coherence gRPC proxy module coherence-grpc-proxy . Setting Up the Netty Coherence gRPC Proxy Server To set up and start using the Netty Coherence gRPC Server, you should declare the coherence-grpc-proxy module as a dependency of your project. For example: If using Maven, declare the server as follows: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; In the pom.xml file, coherence.version property is the version of Coherence being used, and coherence.groupId property is either the Coherence commercial group id, com.oracle.coherence, or the CE group id, com.oracle.coherence.ce. If using Gradle, declare the server as follows: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy\" } In the build.gradle file, coherenceVersion property is the version of Coherence being used, and coherenceGroupId property is either the Coherence commercial group id, com.oracle.coherence or the CE group id, com.oracle.coherence.ce. Using Coherence gRPC Proxy With Helidon 4+ Applications that are using Helidon 4+ that wish to run the Coherence gRPC proxy have the option to use Helidon&#8217;s gRPC implementation for the gRPC server. Te Coherence gRPC Proxy server will run its own Helidon server to serve the Coherence gRPC requests, this will be separate from any other Helidon web servers that the application might be running. Setting Up the Helidon Coherence gRPC Proxy Server To set up and start using the Helidon Coherence gRPC Server, you should declare the coherence-grpc-proxy-helidon module as a dependency of your project. For example: If using Maven, declare the server as follows: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy-helidon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; In the pom.xml file, coherence.version property is the version of Coherence being used, and coherence.groupId property is either the Coherence commercial group id, com.oracle.coherence, or the CE group id, com.oracle.coherence.ce. If using Gradle, declare the server as follows: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy-helidon\" } In the build.gradle file, coherenceVersion property is the version of Coherence being used, and coherenceGroupId property is either the Coherence commercial group id, com.oracle.coherence or the CE group id, com.oracle.coherence.ce. Note If both the coherence-grpc-proxy-helidon module and the coherence-grpc-proxy module are on the class path, the Helidon gRPC server will be used. ",
            "title": "Coherence gRPC Server"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " What You Will Build What You Need Review the Example Code Review the Tests Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " In this example you will run a number of tests and that show the following features of near caches: Configuring near caches Setting near cache size limits Changing the invalidation strategy Configuring eviction policies Exploring MBeans related to near caching What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The example code comprises the SimpleNearCachingExample class, which uses the near-cache-config.xml configuration to define a near cache. The front cache is configured with 100 entries as the high-units and the back cache is a distributed cache. When a near cache has reached it&#8217;s high-units limit, it prunes itself back to the value of the low-units element (or not less than 80% of high-units if not set). The entries chosen are done so according to the configured eviction-policy . There are a number of eviction policies that can be used including: Least Recently Used (LRU), Least Frequently Used (LFU), Hybrid or custom. The test class carries out the following steps: Inserts 100 entries into the cache Issues a get on each of the 100 entries and displays the time taken (populates the near cache&#8217;s front cache) Displays CacheMBean metrics for the front cache Carries out a second get on the 100 entries and notes the difference in the time to retrieve the entries Inserts an additional 10 entries then issue gets for those entries, which will cause cache pruning Displays CacheMBean metrics for the front cache to show cache pruning happening Displays StorageManagerMBean metrics to show listener registrations There are two tests that exercise the above SimpleNearCachingExample class and using different caches as well as different invalidation strategies set via a system property. They are described in more detail in the following sections. com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Review the Cache Config <markup lang=\"java\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;size-cache-*&lt;/cache-name&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;front-limit-entries&lt;/param-name&gt; &lt;param-value&gt;100&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt; &lt;high-units&gt;{front-limit-entries 10}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;sample-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/back-scheme&gt; &lt;invalidation-strategy system-property=\"test.invalidation.strategy\"&gt;all&lt;/invalidation-strategy&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/near-scheme&gt; Define cache mapping for caches matching size-cache-* to the near-scheme using macros to set the front limit to 100 Define an eviction policy to apply when high-units are reached Define front scheme high-units using the macro and defaulting to 10 if not set Define back scheme as standard distributed scheme System property to set the invalidation strategy for each test Review the SimpleNearCachingExample class Constructor <markup lang=\"java\" >/** * Construct the example. * * @param cacheName cache name * @param invalidationStrategy invalidation strategy to use */ public SimpleNearCachingExample(String cacheName, String invalidationStrategy) { this.cacheName = cacheName; if (invalidationStrategy != null) { System.setProperty(\"test.invalidation.strategy\", invalidationStrategy); } System.setProperty(\"coherence.management.refresh.expiry\", \"1s\"); System.setProperty(\"coherence.management\", \"all\"); } Main Example The runExample() method contains the code that exercises the near cache. A loop in the test runs twice to show the difference second time around with the near cache populated. <markup lang=\"java\" >/** * Run the example. */ public void runExample() throws Exception { final int MAX = 100; // Create the Coherence instance from the configuration CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.create(\"near-cache-config.xml\")) .build(); Coherence coherence = Coherence.clusterMember(cfg); coherence.start().join(); // retrieve a session Session session = coherence.getSession(); NamedMap&lt;Integer, String&gt; map = session.getMap(cacheName); map.clear(); Logger.info(\"Running test with cache \" + cacheName); // sleep so we don't get distribution messages intertwined with test output Base.sleep(5000L); // fill the map with MAX values putValues(map, 0, MAX); // execute two times to see the difference in access times and MBeans once the // near cache is populated on the first iteration for (int j = 1; j &lt;= 2; j++) { // issue MAX get operations and get the total time taken long start = System.nanoTime(); getValues(map, 0, MAX); long duration = (System.nanoTime() - start); Logger.info(\"Iteration #\" + j + \" Total time for gets \" + String.format(\"%.3f\", duration / 1_000_000f) + \"ms\"); // Wait for some time for the JMX stats to catch up Base.sleep(3000L); logJMXNearCacheStats(); } // issue 10 more puts putValues(map, MAX, 10); // issue 10 more gets and the high-units will be hit and cache pruning will happen when using size cache getValues(map, MAX, 10); Logger.info(\"After extra 10 values put and get\"); logJMXNearCacheStats(); logJMXStorageStats(); } Populate the cache with 100 entries Issue a get for each of the 100 entries Sleep for 3 seconds to ensure JMX stats are up to date Display the Cache MBean front cache metrics Issue 10 more puts and gets which will cause the front cache to be pruned Display the Cache MBean front cache metrics and StorageManager metrics ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The main SimpleNearCachingExample class is exercised by running the following tests : SimpleNearCachingExampleALLTest - uses all invalidation strategy and high units of 100 SimpleNearCachingExamplePRESENTTest - uses present invalidation strategy and high units of 100 There are a number of invalidation strategies, described here , but we will utilize the following for the tests above: all - This strategy instructs a near cache to listen to all back cache events. This strategy is optimal for read-heavy tiered access patterns where there is significant overlap between the different instances of front caches. present - This strategy instructs a near cache to listen to the back cache events related only to the items currently present in the front cache. This strategy works best when each instance of a front cache contains distinct subset of data relative to the other front cache instances (for example, sticky data access patterns). The default strategy is auto , which is identical to the present strategy. Review the SimpleNearCachingExampleALLTest <markup lang=\"java\" >public class SimpleNearCachingExampleALLTest { @Test public void testNearCacheAll() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-all\", \"all\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-all , which matches the size limited near cache and invalidation strategy of all . Review the SimpleNearCachingExamplePRESENTTest <markup lang=\"java\" >public class SimpleNearCachingExamplePRESENTTest { @Test public void testNearCachePresent() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-present\", \"present\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-present , which matches the size limited near cache and invalidation strategy of `present. ",
            "title": "Review the Tests"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " Run the examples using one of the test classes below: Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest or com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test If you run one or more cache servers as described earlier, you will see additional StorageManager MBean output below. SimpleNearCachingExampleALLTest Output This test will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-all &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.094ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.143ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=109 &lt;Info&gt; (thread=main, member=1): Name: Size, value=90 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5190476190476191 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.36633663366336633 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-all,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=1 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Iteration #1 for gets takes 38.094ms which includes the time to populate the front cache The Cache MBean object name for the front cache and various metrics Iteration #2 for gets takes only 0.143ms which is considerably quicker due to the entries being in the front cache The Hit Probability is 0.5 or 50% as 100 out of 200 entries were read from the front cache After the extra puts and gets, we can see that the cache was pruned the size of the front cache is now 90 Number of prune operations Because we are using the all invalidation strategy there is only 1 listener registered for all the entries SimpleNearCachingExamplePRESENTTest Output The output is similar to the above output, but you will notice that the number of listeners registered are higher as we are using the Present strategy that will register a listener for each entry in the front of the near cache. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-present &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.474ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.236ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=89 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.47619047619047616 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.4818181818181818 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-present,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=110 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Number of listener registrations ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " In this example you have seen how to use near caching within Coherence by covering the following: Configured near caches Set near cache size limits Changed the invalidation strategy Configured eviction policies Explored MBeans related to near caching ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " Understanding Near Caches Defining Near Cache Schemes Near Cache Invalidation Strategies Understanding Local Caches Near Cache local-scheme Configuration Near Cache and Cluster-node Affinity Concurrent Near Cache Misses on a Specific Hot Key ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " This guide walks you through how to use near caching within Coherence by providing various examples and configurations that showcase the different features available. A near cache is a hybrid cache; it typically fronts a distributed cache or a remote cache with a local cache. Near cache invalidates front cache entries, using a configured invalidation strategy, and provides excellent performance and synchronization. Near cache backed by a partitioned cache offers zero-millisecond local access for repeat data access, while enabling concurrency and ensuring coherency and fail over, effectively combining the best attributes of replicated and partitioned caches. See the Coherence Documentation for detailed information on near caches. Table of Contents What You Will Build What You Need Review the Example Code Review the Tests Run the Examples Summary See Also What You Will Build In this example you will run a number of tests and that show the following features of near caches: Configuring near caches Setting near cache size limits Changing the invalidation strategy Configuring eviction policies Exploring MBeans related to near caching What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Review the Example Code The example code comprises the SimpleNearCachingExample class, which uses the near-cache-config.xml configuration to define a near cache. The front cache is configured with 100 entries as the high-units and the back cache is a distributed cache. When a near cache has reached it&#8217;s high-units limit, it prunes itself back to the value of the low-units element (or not less than 80% of high-units if not set). The entries chosen are done so according to the configured eviction-policy . There are a number of eviction policies that can be used including: Least Recently Used (LRU), Least Frequently Used (LFU), Hybrid or custom. The test class carries out the following steps: Inserts 100 entries into the cache Issues a get on each of the 100 entries and displays the time taken (populates the near cache&#8217;s front cache) Displays CacheMBean metrics for the front cache Carries out a second get on the 100 entries and notes the difference in the time to retrieve the entries Inserts an additional 10 entries then issue gets for those entries, which will cause cache pruning Displays CacheMBean metrics for the front cache to show cache pruning happening Displays StorageManagerMBean metrics to show listener registrations There are two tests that exercise the above SimpleNearCachingExample class and using different caches as well as different invalidation strategies set via a system property. They are described in more detail in the following sections. com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Review the Cache Config <markup lang=\"java\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;size-cache-*&lt;/cache-name&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;front-limit-entries&lt;/param-name&gt; &lt;param-value&gt;100&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt; &lt;high-units&gt;{front-limit-entries 10}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;sample-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/back-scheme&gt; &lt;invalidation-strategy system-property=\"test.invalidation.strategy\"&gt;all&lt;/invalidation-strategy&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/near-scheme&gt; Define cache mapping for caches matching size-cache-* to the near-scheme using macros to set the front limit to 100 Define an eviction policy to apply when high-units are reached Define front scheme high-units using the macro and defaulting to 10 if not set Define back scheme as standard distributed scheme System property to set the invalidation strategy for each test Review the SimpleNearCachingExample class Constructor <markup lang=\"java\" >/** * Construct the example. * * @param cacheName cache name * @param invalidationStrategy invalidation strategy to use */ public SimpleNearCachingExample(String cacheName, String invalidationStrategy) { this.cacheName = cacheName; if (invalidationStrategy != null) { System.setProperty(\"test.invalidation.strategy\", invalidationStrategy); } System.setProperty(\"coherence.management.refresh.expiry\", \"1s\"); System.setProperty(\"coherence.management\", \"all\"); } Main Example The runExample() method contains the code that exercises the near cache. A loop in the test runs twice to show the difference second time around with the near cache populated. <markup lang=\"java\" >/** * Run the example. */ public void runExample() throws Exception { final int MAX = 100; // Create the Coherence instance from the configuration CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.create(\"near-cache-config.xml\")) .build(); Coherence coherence = Coherence.clusterMember(cfg); coherence.start().join(); // retrieve a session Session session = coherence.getSession(); NamedMap&lt;Integer, String&gt; map = session.getMap(cacheName); map.clear(); Logger.info(\"Running test with cache \" + cacheName); // sleep so we don't get distribution messages intertwined with test output Base.sleep(5000L); // fill the map with MAX values putValues(map, 0, MAX); // execute two times to see the difference in access times and MBeans once the // near cache is populated on the first iteration for (int j = 1; j &lt;= 2; j++) { // issue MAX get operations and get the total time taken long start = System.nanoTime(); getValues(map, 0, MAX); long duration = (System.nanoTime() - start); Logger.info(\"Iteration #\" + j + \" Total time for gets \" + String.format(\"%.3f\", duration / 1_000_000f) + \"ms\"); // Wait for some time for the JMX stats to catch up Base.sleep(3000L); logJMXNearCacheStats(); } // issue 10 more puts putValues(map, MAX, 10); // issue 10 more gets and the high-units will be hit and cache pruning will happen when using size cache getValues(map, MAX, 10); Logger.info(\"After extra 10 values put and get\"); logJMXNearCacheStats(); logJMXStorageStats(); } Populate the cache with 100 entries Issue a get for each of the 100 entries Sleep for 3 seconds to ensure JMX stats are up to date Display the Cache MBean front cache metrics Issue 10 more puts and gets which will cause the front cache to be pruned Display the Cache MBean front cache metrics and StorageManager metrics Review the Tests The main SimpleNearCachingExample class is exercised by running the following tests : SimpleNearCachingExampleALLTest - uses all invalidation strategy and high units of 100 SimpleNearCachingExamplePRESENTTest - uses present invalidation strategy and high units of 100 There are a number of invalidation strategies, described here , but we will utilize the following for the tests above: all - This strategy instructs a near cache to listen to all back cache events. This strategy is optimal for read-heavy tiered access patterns where there is significant overlap between the different instances of front caches. present - This strategy instructs a near cache to listen to the back cache events related only to the items currently present in the front cache. This strategy works best when each instance of a front cache contains distinct subset of data relative to the other front cache instances (for example, sticky data access patterns). The default strategy is auto , which is identical to the present strategy. Review the SimpleNearCachingExampleALLTest <markup lang=\"java\" >public class SimpleNearCachingExampleALLTest { @Test public void testNearCacheAll() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-all\", \"all\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-all , which matches the size limited near cache and invalidation strategy of all . Review the SimpleNearCachingExamplePRESENTTest <markup lang=\"java\" >public class SimpleNearCachingExamplePRESENTTest { @Test public void testNearCachePresent() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-present\", \"present\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-present , which matches the size limited near cache and invalidation strategy of `present. Run the Examples Run the examples using one of the test classes below: Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest or com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test If you run one or more cache servers as described earlier, you will see additional StorageManager MBean output below. SimpleNearCachingExampleALLTest Output This test will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-all &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.094ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.143ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=109 &lt;Info&gt; (thread=main, member=1): Name: Size, value=90 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5190476190476191 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.36633663366336633 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-all,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=1 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Iteration #1 for gets takes 38.094ms which includes the time to populate the front cache The Cache MBean object name for the front cache and various metrics Iteration #2 for gets takes only 0.143ms which is considerably quicker due to the entries being in the front cache The Hit Probability is 0.5 or 50% as 100 out of 200 entries were read from the front cache After the extra puts and gets, we can see that the cache was pruned the size of the front cache is now 90 Number of prune operations Because we are using the all invalidation strategy there is only 1 listener registered for all the entries SimpleNearCachingExamplePRESENTTest Output The output is similar to the above output, but you will notice that the number of listeners registered are higher as we are using the Present strategy that will register a listener for each entry in the front of the near cache. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-present &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.474ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.236ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=89 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.47619047619047616 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.4818181818181818 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-present,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=110 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Number of listener registrations Summary In this example you have seen how to use near caching within Coherence by covering the following: Configured near caches Set near cache size limits Changed the invalidation strategy Configured eviction policies Explored MBeans related to near caching See Also Understanding Near Caches Defining Near Cache Schemes Near Cache Invalidation Strategies Understanding Local Caches Near Cache local-scheme Configuration Near Cache and Cluster-node Affinity Concurrent Near Cache Misses on a Specific Hot Key ",
            "title": "Near Caching"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Configuration Review the Test Classes Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " In this example you will run tests that show a number of ways to configure secure communication via SSL by defining various SSL socket providers. The tests carry out the following, for a variety of socket providers: Generate keys and self-signed certificates to be used in the test Start 2 cache servers, one having a Proxy service enabled passing properties to point to the newly created keys and certificates Run a basic put/get test over SSL via Coherence*Extend passing properties to point to the newly created keys and certificates Each test showcases the different methods of configuring SSL: Using Java key stores Referring directly to keys and certificates on the file-system Using custom loaders to load key stores, private keys and certificates Custom loaders can also be configured to be refreshed based upon intervals. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " When configuring SSL, you define a &lt;socket-provider&gt; in the Coherence operational configuration and refer to this in your operational and cache configuration. The socket providers for this test are explained further below. To enable SSL for cluster communication, add a reference to the socket provider in your &lt;unicast-listener&gt; element as shown below: <markup lang=\"xml\" >&lt;unicast-listener&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;well-known-addresses&gt; &lt;address system-property=\"coherence.wka\"&gt;127.0.0.1&lt;/address&gt; &lt;/well-known-addresses&gt; &lt;/unicast-listener&gt; To enable SSL on a Proxy server, specify a &lt;socket-provider&gt; in the &lt;tcp-acceptor&gt; element of the proxy scheme as shown below: <markup lang=\"xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;local-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart system-property=\"test.proxy.enabled\"&gt;false&lt;/autostart&gt; &lt;/proxy-scheme&gt; Finally, to enable SSL on a Coherence*Extend client, specify a &lt;socket-provider&gt; in the &lt;tcp-initiator&gt; element of the &lt;remote-cache-scheme&gt; as shown below: <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; For this example, we define a number of socket providers in the operational configuration src/test/resources/tangosol-coherence-ssl.xml . Each test which is run sets the system property test.socket.provider to one of the following values to test the configuration: sslKeyStore - configure using Java key store and trust store sslKeyAndCert - configure using keys and certificates on the file system sslCustomKeyAndCert - configure using custom private key a certificate loaders (This is especially useful in Kubernetes environments to load from secrets) sslCustomKeyStore - configure using a custom key store loader when specifying a trust store, you get two-way SSL. Each configuration option is outlined below: sslKeyStore - configure SSL socket provider using Java key store and trust store <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyStore\"&gt; &lt;ssl&gt; &lt;protocol&gt;TLS&lt;/protocol&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;url system-property=\"test.server.keystore\"&gt;file:server.jks&lt;/url&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.CustomPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;type&lt;/param-name&gt; &lt;param-value&gt;identity-keystore&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/key-store&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.CustomPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;type&lt;/param-name&gt; &lt;param-value&gt;identity-key&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;url system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/url&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.CustomPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;type&lt;/param-name&gt; &lt;param-value&gt;trust-keystore&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;socket-provider&gt;tcp&lt;/socket-provider&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using Java key store Identity manager key store password using custom PasswordProvider implemenation Identity private key password using custom PasswordProvider implementation Trust manager using Java key store Trust manager key store password using custom PasswordProvider implementation sslKeyAndCert - configure SSL socket provider using key and certificate files only <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"test.server.key\"/&gt; &lt;cert system-property=\"test.server.cert\"/&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"test.server.ca.cert\"/&gt; &lt;cert system-property=\"test.client.ca.cert\"/&gt; &lt;/trust-manager&gt; &lt;!-- &lt;refresh-period&gt;24h&lt;/refresh-period&gt; --&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using key and certificate directly Trust manager using key and certificate directly Optional refresh period for keys and certificates sslCustomKeyAndCert - configure SSL socket provider using custom private key and certificate loaders <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomPrivateKeyLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.key\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-loader&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.ca.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom private key loader Identity manager using custom certificate key loader Trust manager using custom certificate key loader sslCustomKeyStore - configure SSL socket provider using a custom key store loader <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyStore\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.keystore\"&gt;file:client.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.server.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;password system-property=\"test.server.key.password\"&gt;private&lt;/password&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.trust.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom key store loader Identity manager using custom key store loader ",
            "title": "Socket Provider Definitions"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " Socket Provider Definitions When configuring SSL, you define a &lt;socket-provider&gt; in the Coherence operational configuration and refer to this in your operational and cache configuration. The socket providers for this test are explained further below. To enable SSL for cluster communication, add a reference to the socket provider in your &lt;unicast-listener&gt; element as shown below: <markup lang=\"xml\" >&lt;unicast-listener&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;well-known-addresses&gt; &lt;address system-property=\"coherence.wka\"&gt;127.0.0.1&lt;/address&gt; &lt;/well-known-addresses&gt; &lt;/unicast-listener&gt; To enable SSL on a Proxy server, specify a &lt;socket-provider&gt; in the &lt;tcp-acceptor&gt; element of the proxy scheme as shown below: <markup lang=\"xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;local-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart system-property=\"test.proxy.enabled\"&gt;false&lt;/autostart&gt; &lt;/proxy-scheme&gt; Finally, to enable SSL on a Coherence*Extend client, specify a &lt;socket-provider&gt; in the &lt;tcp-initiator&gt; element of the &lt;remote-cache-scheme&gt; as shown below: <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; For this example, we define a number of socket providers in the operational configuration src/test/resources/tangosol-coherence-ssl.xml . Each test which is run sets the system property test.socket.provider to one of the following values to test the configuration: sslKeyStore - configure using Java key store and trust store sslKeyAndCert - configure using keys and certificates on the file system sslCustomKeyAndCert - configure using custom private key a certificate loaders (This is especially useful in Kubernetes environments to load from secrets) sslCustomKeyStore - configure using a custom key store loader when specifying a trust store, you get two-way SSL. Each configuration option is outlined below: sslKeyStore - configure SSL socket provider using Java key store and trust store <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyStore\"&gt; &lt;ssl&gt; &lt;protocol&gt;TLS&lt;/protocol&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;url system-property=\"test.server.keystore\"&gt;file:server.jks&lt;/url&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.CustomPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;type&lt;/param-name&gt; &lt;param-value&gt;identity-keystore&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/key-store&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.CustomPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;type&lt;/param-name&gt; &lt;param-value&gt;identity-key&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;url system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/url&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.CustomPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;type&lt;/param-name&gt; &lt;param-value&gt;trust-keystore&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;socket-provider&gt;tcp&lt;/socket-provider&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using Java key store Identity manager key store password using custom PasswordProvider implemenation Identity private key password using custom PasswordProvider implementation Trust manager using Java key store Trust manager key store password using custom PasswordProvider implementation sslKeyAndCert - configure SSL socket provider using key and certificate files only <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"test.server.key\"/&gt; &lt;cert system-property=\"test.server.cert\"/&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"test.server.ca.cert\"/&gt; &lt;cert system-property=\"test.client.ca.cert\"/&gt; &lt;/trust-manager&gt; &lt;!-- &lt;refresh-period&gt;24h&lt;/refresh-period&gt; --&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using key and certificate directly Trust manager using key and certificate directly Optional refresh period for keys and certificates sslCustomKeyAndCert - configure SSL socket provider using custom private key and certificate loaders <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomPrivateKeyLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.key\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-loader&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.ca.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom private key loader Identity manager using custom certificate key loader Trust manager using custom certificate key loader sslCustomKeyStore - configure SSL socket provider using a custom key store loader <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyStore\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.keystore\"&gt;file:client.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.server.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;password system-property=\"test.server.key.password\"&gt;private&lt;/password&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.trust.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom key store loader Identity manager using custom key store loader ",
            "title": "Review the Configuration"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " The example code comprises the following classes, which are explained below: AbstractSSLExampleTest - abstract test implementation to create SSL configuration files and startup cluster using the required socket provider KeyStoreSSLExampleTest - test with socket provider using Java key-store KeyStoreAndCertSSLExampleTest - test with socket provider using key and certificate files only CustomKeyStoreSSLExampleTest - test with socket provider using a custom private key and certificate loaders CustomCertificateLoader - a custom certificate loader class CustomKeyStoreLoader - a custom key store loader class CustomPrivateKeyLoader - a custom private key loader class The tests use the Oracle Bedrock KeyTool utility to generate the require keys, stores and certificates. You should use your own generated artefacts and not use these for production usage. Review the AbstractSSLExampleTest class This abstract class contains various utilities used by all tests. A few snippets are included below: Generate Test Certificates <markup lang=\"java\" >/** * Create the required certificates. * * @throws Exception if any errors creating certificates. */ @BeforeAll public static void setupSSL() throws Exception { // only initialize once if (tmpDir == null) { KeyTool.assertCanCreateKeys(); tmpDir = FileHelper.createTempDir(); serverCACert = KeyTool.createCACert(tmpDir, \"server-ca\", \"PKCS12\"); serverKeyAndCert = KeyTool.createKeyCertPair(tmpDir, serverCACert, \"server\"); clientCACert = KeyTool.createCACert(tmpDir, \"client-ca\", \"PKCS12\"); clientKeyAndCert = KeyTool.createKeyCertPair(tmpDir, clientCACert, \"client\"); } } Set Cache Server Options <markup lang=\"java\" >/** * Create options to start cache servers. * * @param clusterPort cluster port * @param proxyPort proxy port * @param socketProvider socket provider to use * @param memberName member name * * @return new {@link OptionsByType} */ protected static OptionsByType createCacheServerOptions(int clusterPort, int proxyPort, String socketProvider, String memberName) { OptionsByType optionsByType = OptionsByType.empty(); optionsByType.addAll(JMXManagementMode.ALL, JmxProfile.enabled(), LocalStorage.enabled(), WellKnownAddress.of(hostName), Multicast.ttl(0), CacheConfig.of(SERVER_CACHE_CONFIG), OperationalOverride.of(OVERRIDE), Logging.at(6), ClusterName.of(\"ssl-cluster\"), MemberName.of(memberName), SystemProperty.of(\"test.socket.provider\", socketProvider), SystemProperty.of(\"test.server.keystore\", serverKeyAndCert.getKeystoreURI()), SystemProperty.of(\"test.trust.keystore\", serverCACert.getKeystoreURI()), SystemProperty.of(\"test.server.keystore.password\", serverKeyAndCert.storePasswordString()), SystemProperty.of(\"test.server.key.password\", serverKeyAndCert.keyPasswordString()), SystemProperty.of(\"test.trust.keystore.password\", serverCACert.storePasswordString()), SystemProperty.of(\"test.client.ca.cert\", clientCACert.getCertURI()), SystemProperty.of(\"test.server.key\", serverKeyAndCert.getKeyPEMNoPassURI()), SystemProperty.of(\"test.server.cert\", serverKeyAndCert.getCertURI()), SystemProperty.of(\"test.server.ca.cert\", serverCACert.getCertURI()), ClusterPort.of(clusterPort)); // enable proxy server if a proxy port is not -1 if (proxyPort != -1) { optionsByType.addAll(SystemProperty.of(\"test.extend.address\", hostName), SystemProperty.of(\"test.extend.port\", proxyPort), SystemProperty.of(\"test.proxy.enabled\", \"true\") ); } return optionsByType; } Run the Simple Test <markup lang=\"java\" >/** * Run a simple test using Coherence*Extend with the given socket-provider to validate * that SSL communications for the cluster and proxy are working. * * @param socketProvider socket provider to use */ protected void runTest(String socketProvider) { _startup(socketProvider); NamedCache&lt;Integer, String&gt; cache = getCache(socketProvider); cache.clear(); cache.put(1, \"one\"); assertEquals(\"one\", cache.get(1)); } Review the KeyStoreSSLExampleTest class which tests with a socket provider using Java key-store <markup lang=\"java\" >/** * Test SSL using Java key-store and trust-store. * * @author Tim Middleton 2022.06.15 */ public class KeyStoreSSLExampleTest extends AbstractSSLExampleTest { @Test public void testKeyStoreSocketProvider() { runTest(\"sslKeyStore\"); } } Specify the SSL socket provider Review the KeyStoreAndCertSSLExampleTest class which tests with socket provider using key and certificate files only <markup lang=\"java\" >/** * Test SSL using Key and Certificate. * * @author Tim Middleton 2022.06.15 */ public class KeyStoreAndCertSSLExampleTest extends AbstractSSLExampleTest { @Test public void testKeyAndCertSocketProvider() throws Exception { runTest(\"sslKeyAndCert\"); } } Specify the SSL socket provider Review the CustomKeyStoreSSLExampleTest class which tests with socket provider using a custom private key and certificate loader <markup lang=\"java\" >/** * Test SSL using custom key-store loader. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyStoreSSLExampleTest extends AbstractSSLExampleTest { @Test public void testCustomKeyStoreSocketProvider() { runTest(\"sslCustomKeyStore\"); } } Specify the SSL socket provider Review the CustomKeyAndCertSSLExampleTest class which tests with socket provider using a custom key store loader <markup lang=\"java\" >/** * Test SSL using custom key and certificate loader. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyAndCertSSLExampleTest extends AbstractSSLExampleTest { @Test public void testCustomKeyAndCertSocketProvider() { runTest(\"sslCustomKeyAndCert\"); } } Specify the SSL socket provider Review the CustomCertificateLoader <markup lang=\"java\" >/** * An example implementation of a {@link CertificateLoader} which loads a certificate from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomCertificateLoader extends AbstractCertificateLoader { public CustomCertificateLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } Review the CustomKeyStoreLoader <markup lang=\"java\" >/** * An example implementation of a {@link KeyStoreLoader} which loads a key store from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyStoreLoader extends AbstractKeyStoreLoader { public CustomKeyStoreLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } Review the CustomPrivateKeyLoader <markup lang=\"java\" >/** * An example implementation of a {@link PrivateKeyLoader} which loads a private key from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomPrivateKeyLoader extends AbstractPrivateKeyLoader { public CustomPrivateKeyLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } ",
            "title": "Review the Test Classes"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " Run the examples using the test case below. Run directly from your IDE by running either of the following test classes in the com.oracle.coherence.guides.ssl package. Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test has run you will see output from the various parts of the test code. See below for some key items to look out for. Messages indicating the cluster is using SSL socket provider <markup lang=\"bash\" >TCMP bound to /127.0.0.1:51684 using TCPDatagramSocketProvider[Delegate: SSLSocketProvider(SSLSocketProvider())] Cluster members connecting using tmbs (TCP Message bus over SSL) <markup lang=\"bash\" >tmbs://127.0.0.1:52311.51395 opening connection with tmbs://127.0.0.1:52315.47215 using SSLSocket(null /127.0.0.1:866404240, buffered{clear=0 encrypted=0 out=0}, handshake=NOT_HANDSHAKING, jobs=0 Cluster musing two way (key and trust stores) for communication <markup lang=\"bash\" > instantiated SSLSocketProviderDependencies: SSLSocketProvider(auth=two-way, identity=SunX509/.../examples/guides/210-ssl/target/test-classes/certs/server.jks, trust=SunX509//.../examples/guides/210-ssl/target/test-classes/certs/server-ca-ca.jks) ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " In this guide you learned how to secure Coherence communication between cluster members as well as Coherence*Extend clients. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " Introduction to Coherence Security Using SSL to Secure Communication ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " This guide walks you through how to secure Coherence communication between cluster members as well as Coherence*Extend clients. Oracle Coherence supports Secure Sockets Layer (SSL) to secure TCMP communication between cluster nodes and to secure the TCP communication between Oracle Coherence*Extend clients and proxies. Oracle Coherence supports the Transport Layer Security (TLS) protocol, which superseded the SSL protocol; however, the term SSL is used in this documentation because it is the more widely recognized term. See the Coherence documentation links below for more detailed information on Coherence Security. Introduction to Coherence Security Using SSL to Secure Communication Table of Contents What You Will Build What You Need Building the Example Code Review the Configuration Review the Test Classes Run the Examples Summary See Also What You Will Build In this example you will run tests that show a number of ways to configure secure communication via SSL by defining various SSL socket providers. The tests carry out the following, for a variety of socket providers: Generate keys and self-signed certificates to be used in the test Start 2 cache servers, one having a Proxy service enabled passing properties to point to the newly created keys and certificates Run a basic put/get test over SSL via Coherence*Extend passing properties to point to the newly created keys and certificates Each test showcases the different methods of configuring SSL: Using Java key stores Referring directly to keys and certificates on the file-system Using custom loaders to load key stores, private keys and certificates Custom loaders can also be configured to be refreshed based upon intervals. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Review the Configuration Socket Provider Definitions When configuring SSL, you define a &lt;socket-provider&gt; in the Coherence operational configuration and refer to this in your operational and cache configuration. The socket providers for this test are explained further below. To enable SSL for cluster communication, add a reference to the socket provider in your &lt;unicast-listener&gt; element as shown below: <markup lang=\"xml\" >&lt;unicast-listener&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;well-known-addresses&gt; &lt;address system-property=\"coherence.wka\"&gt;127.0.0.1&lt;/address&gt; &lt;/well-known-addresses&gt; &lt;/unicast-listener&gt; To enable SSL on a Proxy server, specify a &lt;socket-provider&gt; in the &lt;tcp-acceptor&gt; element of the proxy scheme as shown below: <markup lang=\"xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;local-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart system-property=\"test.proxy.enabled\"&gt;false&lt;/autostart&gt; &lt;/proxy-scheme&gt; Finally, to enable SSL on a Coherence*Extend client, specify a &lt;socket-provider&gt; in the &lt;tcp-initiator&gt; element of the &lt;remote-cache-scheme&gt; as shown below: <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; For this example, we define a number of socket providers in the operational configuration src/test/resources/tangosol-coherence-ssl.xml . Each test which is run sets the system property test.socket.provider to one of the following values to test the configuration: sslKeyStore - configure using Java key store and trust store sslKeyAndCert - configure using keys and certificates on the file system sslCustomKeyAndCert - configure using custom private key a certificate loaders (This is especially useful in Kubernetes environments to load from secrets) sslCustomKeyStore - configure using a custom key store loader when specifying a trust store, you get two-way SSL. Each configuration option is outlined below: sslKeyStore - configure SSL socket provider using Java key store and trust store <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyStore\"&gt; &lt;ssl&gt; &lt;protocol&gt;TLS&lt;/protocol&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;url system-property=\"test.server.keystore\"&gt;file:server.jks&lt;/url&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.CustomPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;type&lt;/param-name&gt; &lt;param-value&gt;identity-keystore&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/key-store&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.CustomPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;type&lt;/param-name&gt; &lt;param-value&gt;identity-key&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;url system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/url&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.CustomPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;type&lt;/param-name&gt; &lt;param-value&gt;trust-keystore&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;socket-provider&gt;tcp&lt;/socket-provider&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using Java key store Identity manager key store password using custom PasswordProvider implemenation Identity private key password using custom PasswordProvider implementation Trust manager using Java key store Trust manager key store password using custom PasswordProvider implementation sslKeyAndCert - configure SSL socket provider using key and certificate files only <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"test.server.key\"/&gt; &lt;cert system-property=\"test.server.cert\"/&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"test.server.ca.cert\"/&gt; &lt;cert system-property=\"test.client.ca.cert\"/&gt; &lt;/trust-manager&gt; &lt;!-- &lt;refresh-period&gt;24h&lt;/refresh-period&gt; --&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using key and certificate directly Trust manager using key and certificate directly Optional refresh period for keys and certificates sslCustomKeyAndCert - configure SSL socket provider using custom private key and certificate loaders <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomPrivateKeyLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.key\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-loader&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.ca.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom private key loader Identity manager using custom certificate key loader Trust manager using custom certificate key loader sslCustomKeyStore - configure SSL socket provider using a custom key store loader <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyStore\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.keystore\"&gt;file:client.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.server.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;password system-property=\"test.server.key.password\"&gt;private&lt;/password&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.trust.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom key store loader Identity manager using custom key store loader Review the Test Classes The example code comprises the following classes, which are explained below: AbstractSSLExampleTest - abstract test implementation to create SSL configuration files and startup cluster using the required socket provider KeyStoreSSLExampleTest - test with socket provider using Java key-store KeyStoreAndCertSSLExampleTest - test with socket provider using key and certificate files only CustomKeyStoreSSLExampleTest - test with socket provider using a custom private key and certificate loaders CustomCertificateLoader - a custom certificate loader class CustomKeyStoreLoader - a custom key store loader class CustomPrivateKeyLoader - a custom private key loader class The tests use the Oracle Bedrock KeyTool utility to generate the require keys, stores and certificates. You should use your own generated artefacts and not use these for production usage. Review the AbstractSSLExampleTest class This abstract class contains various utilities used by all tests. A few snippets are included below: Generate Test Certificates <markup lang=\"java\" >/** * Create the required certificates. * * @throws Exception if any errors creating certificates. */ @BeforeAll public static void setupSSL() throws Exception { // only initialize once if (tmpDir == null) { KeyTool.assertCanCreateKeys(); tmpDir = FileHelper.createTempDir(); serverCACert = KeyTool.createCACert(tmpDir, \"server-ca\", \"PKCS12\"); serverKeyAndCert = KeyTool.createKeyCertPair(tmpDir, serverCACert, \"server\"); clientCACert = KeyTool.createCACert(tmpDir, \"client-ca\", \"PKCS12\"); clientKeyAndCert = KeyTool.createKeyCertPair(tmpDir, clientCACert, \"client\"); } } Set Cache Server Options <markup lang=\"java\" >/** * Create options to start cache servers. * * @param clusterPort cluster port * @param proxyPort proxy port * @param socketProvider socket provider to use * @param memberName member name * * @return new {@link OptionsByType} */ protected static OptionsByType createCacheServerOptions(int clusterPort, int proxyPort, String socketProvider, String memberName) { OptionsByType optionsByType = OptionsByType.empty(); optionsByType.addAll(JMXManagementMode.ALL, JmxProfile.enabled(), LocalStorage.enabled(), WellKnownAddress.of(hostName), Multicast.ttl(0), CacheConfig.of(SERVER_CACHE_CONFIG), OperationalOverride.of(OVERRIDE), Logging.at(6), ClusterName.of(\"ssl-cluster\"), MemberName.of(memberName), SystemProperty.of(\"test.socket.provider\", socketProvider), SystemProperty.of(\"test.server.keystore\", serverKeyAndCert.getKeystoreURI()), SystemProperty.of(\"test.trust.keystore\", serverCACert.getKeystoreURI()), SystemProperty.of(\"test.server.keystore.password\", serverKeyAndCert.storePasswordString()), SystemProperty.of(\"test.server.key.password\", serverKeyAndCert.keyPasswordString()), SystemProperty.of(\"test.trust.keystore.password\", serverCACert.storePasswordString()), SystemProperty.of(\"test.client.ca.cert\", clientCACert.getCertURI()), SystemProperty.of(\"test.server.key\", serverKeyAndCert.getKeyPEMNoPassURI()), SystemProperty.of(\"test.server.cert\", serverKeyAndCert.getCertURI()), SystemProperty.of(\"test.server.ca.cert\", serverCACert.getCertURI()), ClusterPort.of(clusterPort)); // enable proxy server if a proxy port is not -1 if (proxyPort != -1) { optionsByType.addAll(SystemProperty.of(\"test.extend.address\", hostName), SystemProperty.of(\"test.extend.port\", proxyPort), SystemProperty.of(\"test.proxy.enabled\", \"true\") ); } return optionsByType; } Run the Simple Test <markup lang=\"java\" >/** * Run a simple test using Coherence*Extend with the given socket-provider to validate * that SSL communications for the cluster and proxy are working. * * @param socketProvider socket provider to use */ protected void runTest(String socketProvider) { _startup(socketProvider); NamedCache&lt;Integer, String&gt; cache = getCache(socketProvider); cache.clear(); cache.put(1, \"one\"); assertEquals(\"one\", cache.get(1)); } Review the KeyStoreSSLExampleTest class which tests with a socket provider using Java key-store <markup lang=\"java\" >/** * Test SSL using Java key-store and trust-store. * * @author Tim Middleton 2022.06.15 */ public class KeyStoreSSLExampleTest extends AbstractSSLExampleTest { @Test public void testKeyStoreSocketProvider() { runTest(\"sslKeyStore\"); } } Specify the SSL socket provider Review the KeyStoreAndCertSSLExampleTest class which tests with socket provider using key and certificate files only <markup lang=\"java\" >/** * Test SSL using Key and Certificate. * * @author Tim Middleton 2022.06.15 */ public class KeyStoreAndCertSSLExampleTest extends AbstractSSLExampleTest { @Test public void testKeyAndCertSocketProvider() throws Exception { runTest(\"sslKeyAndCert\"); } } Specify the SSL socket provider Review the CustomKeyStoreSSLExampleTest class which tests with socket provider using a custom private key and certificate loader <markup lang=\"java\" >/** * Test SSL using custom key-store loader. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyStoreSSLExampleTest extends AbstractSSLExampleTest { @Test public void testCustomKeyStoreSocketProvider() { runTest(\"sslCustomKeyStore\"); } } Specify the SSL socket provider Review the CustomKeyAndCertSSLExampleTest class which tests with socket provider using a custom key store loader <markup lang=\"java\" >/** * Test SSL using custom key and certificate loader. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyAndCertSSLExampleTest extends AbstractSSLExampleTest { @Test public void testCustomKeyAndCertSocketProvider() { runTest(\"sslCustomKeyAndCert\"); } } Specify the SSL socket provider Review the CustomCertificateLoader <markup lang=\"java\" >/** * An example implementation of a {@link CertificateLoader} which loads a certificate from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomCertificateLoader extends AbstractCertificateLoader { public CustomCertificateLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } Review the CustomKeyStoreLoader <markup lang=\"java\" >/** * An example implementation of a {@link KeyStoreLoader} which loads a key store from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyStoreLoader extends AbstractKeyStoreLoader { public CustomKeyStoreLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } Review the CustomPrivateKeyLoader <markup lang=\"java\" >/** * An example implementation of a {@link PrivateKeyLoader} which loads a private key from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomPrivateKeyLoader extends AbstractPrivateKeyLoader { public CustomPrivateKeyLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } Run the Examples Run the examples using the test case below. Run directly from your IDE by running either of the following test classes in the com.oracle.coherence.guides.ssl package. Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test has run you will see output from the various parts of the test code. See below for some key items to look out for. Messages indicating the cluster is using SSL socket provider <markup lang=\"bash\" >TCMP bound to /127.0.0.1:51684 using TCPDatagramSocketProvider[Delegate: SSLSocketProvider(SSLSocketProvider())] Cluster members connecting using tmbs (TCP Message bus over SSL) <markup lang=\"bash\" >tmbs://127.0.0.1:52311.51395 opening connection with tmbs://127.0.0.1:52315.47215 using SSLSocket(null /127.0.0.1:866404240, buffered{clear=0 encrypted=0 out=0}, handshake=NOT_HANDSHAKING, jobs=0 Cluster musing two way (key and trust stores) for communication <markup lang=\"bash\" > instantiated SSLSocketProviderDependencies: SSLSocketProvider(auth=two-way, identity=SunX509/.../examples/guides/210-ssl/target/test-classes/certs/server.jks, trust=SunX509//.../examples/guides/210-ssl/target/test-classes/certs/server-ca-ca.jks) Summary In this guide you learned how to secure Coherence communication between cluster members as well as Coherence*Extend clients. See Also Introduction to Coherence Security Using SSL to Secure Communication ",
            "title": "Securing with SSL"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Java - JDK 17 or higher Maven - 3.8.5 or higher ",
            "title": "Prerequisites"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " As Coherence is generally embedded into an application by using Coherence APIs, the natural place to consume this dependency is from Maven: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; You can also get Coherence from the official Docker image . For other language clients, use ( C&#43;&#43; and .NET ), and for the non-community edition, see Oracle Technology Network . ",
            "title": "How to Get Coherence Community Edition"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Prerequisites Java - JDK 17 or higher Maven - 3.8.5 or higher How to Get Coherence Community Edition As Coherence is generally embedded into an application by using Coherence APIs, the natural place to consume this dependency is from Maven: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; You can also get Coherence from the official Docker image . For other language clients, use ( C&#43;&#43; and .NET ), and for the non-community edition, see Oracle Technology Network . ",
            "title": "Quick Start"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " To run a CohQL console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=24.09.1 dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/24.09.1/coherence-24.09.1.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select * from welcomes CohQL&gt; insert into welcomes key 'english' value 'Hello' CohQL&gt; insert into welcomes key 'spanish' value 'Hola' CohQL&gt; insert into welcomes key 'french' value 'Bonjour' CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; kill %1 ",
            "title": " CohQL Console"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " To run the Coherence console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=24.09.1 dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/24.09.1/coherence-24.09.1.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): get english null Map (welcomes): put english Hello null Map (welcomes): put spanish Hola null Map (welcomes): put french Bonjour null Map (welcomes): get english Hello Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; kill %1 ",
            "title": " Coherence Console"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The following example illustrates the procedure to start a storage enabled Coherence Server, followed by a storage disabled Coherence Console. Using the console, data is inserted, retrieved, and then the console is terminated. The console is restarted and data is once again retrieved to illustrate the permanence of the data. This example uses the out-of-the-box cache configuration and therefore explicitly specifying the console is storage disabled is unnecessary. Coherence cluster members discover each other via one of two mechanisms; multicast (default) or Well Known Addressing (deterministic broadcast). If your system does not support multicast, enable WKA by specifying -Dcoherence.wka=localhost for both processes started in the following console examples. CohQL Console To run a CohQL console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=24.09.1 dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/24.09.1/coherence-24.09.1.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select * from welcomes CohQL&gt; insert into welcomes key 'english' value 'Hello' CohQL&gt; insert into welcomes key 'spanish' value 'Hola' CohQL&gt; insert into welcomes key 'french' value 'Bonjour' CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; kill %1 Coherence Console To run the Coherence console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=24.09.1 dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/24.09.1/coherence-24.09.1.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): get english null Map (welcomes): put english Hello null Map (welcomes): put spanish Hola null Map (welcomes): put french Bonjour null Map (welcomes): get english Hello Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; kill %1 ",
            "title": "CLI Hello Coherence"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Create a maven project either manually or by using an archetype such as maven-archetype-quickstart Add a dependency to the pom file: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Copy and paste the following source to a file named src/main/java/HelloCoherence.java: <markup lang=\"java\" title=\"HelloCoherence.java\" >import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedMap public class HelloCoherence { // ----- static methods ------------------------------------------------- public static void main(String[] asArgs) { NamedMap&lt;String, String&gt; map = CacheFactory.getCache(\"welcomes\"); System.out.printf(\"Accessing map \\\"%s\\\" containing %d entries\", map.getName(), map.size()); map.put(\"english\", \"Hello\"); map.put(\"spanish\", \"Hola\"); map.put(\"french\" , \"Bonjour\"); // list map.entrySet().forEach(System.out::println); } } Compile the maven project: <markup lang=\"shell\" >mvn package Start a Storage server <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"com.tangosol.net.DefaultCacheServer\" &amp; Run HelloCoherence <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"HelloCoherence\" Confirm that you see the output including the following: <markup lang=\"shell\" >Accessing map \"welcomes\" containing 3 entries ConverterEntry{Key=\"french\", Value=\"Bonjour\"} ConverterEntry{Key=\"spanish\", Value=\"Hola\"} ConverterEntry{Key=\"english\", Value=\"Hello\"} Kill the storage server started earlier: <markup lang=\"shell\" >kill %1 ",
            "title": "Build HelloCoherence "
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The following example illustrates starting a storage enabled Coherence server, followed by running the HelloCoherence application. The HelloCoherence application inserts and retrieves data from the Coherence server. Build HelloCoherence Create a maven project either manually or by using an archetype such as maven-archetype-quickstart Add a dependency to the pom file: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Copy and paste the following source to a file named src/main/java/HelloCoherence.java: <markup lang=\"java\" title=\"HelloCoherence.java\" >import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedMap public class HelloCoherence { // ----- static methods ------------------------------------------------- public static void main(String[] asArgs) { NamedMap&lt;String, String&gt; map = CacheFactory.getCache(\"welcomes\"); System.out.printf(\"Accessing map \\\"%s\\\" containing %d entries\", map.getName(), map.size()); map.put(\"english\", \"Hello\"); map.put(\"spanish\", \"Hola\"); map.put(\"french\" , \"Bonjour\"); // list map.entrySet().forEach(System.out::println); } } Compile the maven project: <markup lang=\"shell\" >mvn package Start a Storage server <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"com.tangosol.net.DefaultCacheServer\" &amp; Run HelloCoherence <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"HelloCoherence\" Confirm that you see the output including the following: <markup lang=\"shell\" >Accessing map \"welcomes\" containing 3 entries ConverterEntry{Key=\"french\", Value=\"Bonjour\"} ConverterEntry{Key=\"spanish\", Value=\"Hola\"} ConverterEntry{Key=\"english\", Value=\"Hello\"} Kill the storage server started earlier: <markup lang=\"shell\" >kill %1 ",
            "title": " Programmatic Hello Coherence Example"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": "<markup lang=\"shell\" >$&gt; git clone git@github.com:oracle/coherence.git $&gt; cd coherence/prj # build all modules $&gt; mvn clean install # build all modules skipping tests $&gt; mvn clean install -DskipTests # build a specific module, including all dependent modules and run tests $&gt; mvn -am -pl test/functional/persistence clean verify # build only coherence.jar without running tests $&gt; mvn -am -pl coherence clean install -DskipTests # build only coherence.jar and skip compilation of CDBs and tests $&gt; mvn -am -pl coherence clean install -DskipTests -Dtde.compile.not.required ",
            "title": " Building"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Tests Review the classes Review the cache configuration Run the Tests Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can skip the tests in the initial build by adding the following options: -DskipTests for Maven or -x test for Gradle. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " This example comprises a number of tests showing various server-side events features. Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " In this example you will run a number of tests that demonstrate the following features of server-side events including: Understanding where to declare interceptors in your cache config Listening for cache events related to mutations of cache data, and execution of entry processors Listening for transfer events related to partition transfers and loss events Listening for partitioned cache events related to creation, destruction and truncating of caches Listening for lifecycle events for ConfigurableCacheFactory instantiation What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can skip the tests in the initial build by adding the following options: -DskipTests for Maven or -x test for Gradle. Running the Examples This example comprises a number of tests showing various server-side events features. Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " Review the Customer class. Some tests use the Customer class which has the following fields: <markup lang=\"java\" >private int id; private String name; private String address; private String customerType; private long creditLimit; Review the AuditEvent class. Some tests use the AuditEvent class which has the following fields: <markup lang=\"java\" >/** * Unique Id for the audit event. */ private UUID id; /** * The target of the event such as cache, partition, etc. */ private String target; /** * The type of event. */ private String eventType; /** * Specific event data. */ private String eventData; /** * Time of the event. */ private long eventTime; Review the AuditingInterceptor which audits any mutations to caches using post-commit events. See here for details of all Partitioned Cache events. <markup lang=\"java\" >@Interceptor(identifier = \"AuditingInterceptor\", order = Interceptor.Order.HIGH) @EntryEvents({EntryEvent.Type.INSERTED, EntryEvent.Type.UPDATED, EntryEvent.Type.REMOVED}) public class AuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryEvent&lt;?, ?&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;?, ?&gt; event) { String oldValue = null; String newValue = null; EntryEvent.Type eventType = event.getType(); Object key = event.getKey(); if (eventType == EntryEvent.Type.REMOVED || eventType == EntryEvent.Type.UPDATED) { oldValue = event.getOriginalValue().toString(); } if (eventType == EntryEvent.Type.INSERTED || eventType == EntryEvent.Type.UPDATED) { newValue = event.getValue().toString(); } AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), eventType.toString(), String.format(\"key=%s, old=%s, new=%s\", key, oldValue, newValue)); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name and optional order of HIGH or LOW as the priority Identifies the EntryEvents that will be intercepted. INSERTED, UPDATED and REMOVED are raised asynchronously after the event has happened Identifies the type of events, in this case EntryEvents Overrides method to respond to the event Identifies the type of event and sets the payload accordingly Adds the audit event to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link AuditingInterceptor} which will audit any changes to caches * that fall thought and match the '*' cache-mapping. */ @Test public void testAuditingInterceptor() { System.out.println(\"testAuditingInterceptor\"); CoherenceClusterMember member = getMember1(); // create two different caches to be audited which will match to the auditing-scheme NamedCache&lt;Integer, String&gt; cache1 = member.getCache(\"test-cache\"); NamedCache&lt;Integer, Customer&gt; cache2 = member.getCache(\"test-customer\"); cache1.truncate(); cache2.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // generate some mutations that will be audited cache1.put(1, \"one\"); cache1.put(2, \"two\"); cache1.put(1, \"ONE\"); cache1.remove(1); dumpAuditEvents(\"testAuditingInterceptor-1\"); // ensure 3 inserts and 1 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); auditEvents.clear(); // generate new set of mutations for customers cache2.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10000)); cache2.put(2, new Customer(2, \"John\", \"Address 2\", Customer.SILVER, 4000)); cache2.clear(); dumpAuditEvents(\"testAuditingInterceptor-2\"); // ensure 2 insert and 2 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.values().size(), Matchers.is(4)); } Review the EntryProcessorAuditingInterceptor which audits entry processors executions using post-commit events. <markup lang=\"java\" >@Interceptor(identifier = \"EntryProcessorAuditingInterceptor\") @EntryProcessorEvents({EntryProcessorEvent.Type.EXECUTED}) public class EntryProcessorAuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryProcessorEvent&gt;, Serializable { @Override public void onEvent(EntryProcessorEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Entries=%d, processor=%s\", event.getEntrySet().size(), event.getProcessor().toString())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the EntryProcessorEvents that will be intercepted. EXECUTED event is raised asynchronously after the event has happened Identifies the type of events, in this case EntryProcessorEvents Overrides method to respond to the event and add to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link EntryProcessorAuditingInterceptor} which will audit any entry processor * executions on caches that match the '*' cache-mapping. */ @Test public void testEntryProcessorInterceptor() { System.out.println(\"testEntryProcessorInterceptor\"); CoherenceClusterMember member = getMember1(); // create a cache to audit entry processor events on NamedCache&lt;Integer, Customer&gt; cache = member.getCache(\"test-customer\"); cache.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // add some entries cache.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10_000)); cache.put(2, new Customer(2, \"Tom\", \"Address 2\", Customer.SILVER, 10_000)); cache.put(3, new Customer(3, \"Helen\", \"Address 3\", Customer.BRONZE, 10_000)); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(3)); auditEvents.clear(); cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); dumpAuditEvents(\"testEntryProcessorInterceptor-1\"); // up to 3 entry processor events and 3 updates Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"EXECUTED\")).count(), Matchers.lessThanOrEqualTo(3L)); Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"UPDATED\")).count(), Matchers.is(3L)); auditEvents.clear(); // invoke an entry processor across all customers to update credit limit to 100,000 cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); cache.invoke(1, Processors.update(Customer::setCreditLimit, 100_000L)); dumpAuditEvents(\"testEntryProcessorInterceptor-2\"); // ensure up to 4 EXECUTED events are received Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"EXECUTED\")).count(), Matchers.lessThanOrEqualTo(4L)); } Review the UppercaseInterceptor which changes the name and address attributes to uppercase. <markup lang=\"java\" >@Interceptor(identifier = \"UppercaseInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class UppercaseInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customer = entry.getValue(); customer.setName(customer.getName().toUpperCase()); customer.setAddress(customer.getAddress().toUpperCase()); entry.setValue(customer); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Ensures the changes are persisted by calling entry.setValue() This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link UppercaseInterceptor} which is defined on the 'customers' cache only, * to update name and address fields to uppercase. */ @Test public void testCustomerUppercaseInterceptor() { System.out.println(\"testCustomerUppercaseInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // put a new Customer with lowercase names and addresses customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.GOLD, 10000L)); // validate that the name and address are uppercase Customer customer = customers.get(1); assertEquals(customer.getName(), \"TIM\"); assertEquals(customer.getAddress(), \"123 JAMES STREET, PERTH\"); // update a customers name and ensure that it is updated to uppercase customers.invoke(1, Processors.update(Customer::setName, \"timothy\")); assertEquals(customers.get(1).getName(), \"TIMOTHY\"); } Review the ValidationInterceptor which rejects or accepts changes based upon some simple business rules. <markup lang=\"java\" >@Interceptor(identifier = \"ValidationInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class ValidationInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customerOld = entry.getOriginalValue(); Customer customerNew = entry.getValue(); EntryEvent.Type eventType = event.getType(); if (eventType == EntryEvent.Type.INSERTING) { // Rule 1 - New customers cannot have credit limit above 1,000,000 unless they are GOLD if (customerNew.getCreditLimit() &gt;= 1_000_000L &amp;&amp; !customerNew.getCustomerType().equals(Customer.GOLD)) { // reject the update throw new RuntimeException(\"Only gold customers may have credit limits above 1,000,000\"); } } else if (eventType == EntryEvent.Type.UPDATING) { // Rule 2 - Cannot change customer type from BRONZE directly to GOLD, must go BRONZE -&gt; SILVER -&gt; GOLD if (customerNew.getCustomerType().equals(Customer.GOLD) &amp;&amp; customerOld.getCustomerType().equals(Customer.BRONZE)) { // reject the update throw new RuntimeException(\"Cannot update customer directly to GOLD from BRONZE\"); } } // otherwise, continue with update entry.setValue(customerNew); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Validates the first business rule if the event is an insert. If the rule fails, then throw a RuntimeException Validates the second business rule if the event is an update. If the rule fails, then throw a RuntimeException Saves the entry if all the business rules pass This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link ValidationInterceptor} which will reject updates if business rules fail. */ @Test public void testValidatingInterceptor() { System.out.println(\"testValidatingInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // try adding a BRONZE customer with credit limit &gt; 1,000,000 try { customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 2_000_000L)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Put was correctly rejected: %s\", e.getMessage()); } // should be rejected assertEquals(customers.size(), 0); // add a normal BRONZE customer, should succeed with credit limit 10,000 customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 10_000L)); assertEquals(customers.size(), 1); // try and update credit limit to GOLD from BRONZE, should fail try { customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Update was correctly rejected: %s\", e.getMessage()); } assertEquals(customers.get(1).getCustomerType(), Customer.BRONZE); } Review the TransferEventsInterceptor which audits partition transfer events. <markup lang=\"java\" >@Interceptor(identifier = \"TransferEventsInterceptor\") @TransferEvents({TransferEvent.Type.ARRIVED, TransferEvent.Type.DEPARTING, TransferEvent.Type.LOST}) public class TransferEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;TransferEvent&gt;, Serializable { @Override public void onEvent(TransferEvent event) { AuditEvent auditEvent = new AuditEvent(\"partition=\" + event.getPartitionId(), event.getType().toString(), String.format(\"Partitions from remote member %s\", event.getRemoteMember())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the TransferEvents that will be intercepted. Transfer events are raised while holding a lock on the partition being transferred that blocks any operations for the partition. Identifies the type of events, in this case TransferEvents Overrides method to respond to the event This is used in the following test in ServerPartitionEventsTest : <markup lang=\"java\" >@Test public void testPartitionEvents() { System.out.println(\"testPartitionEvents\"); CoherenceClusterMember member1 = getMember1(); CoherenceClusterMember member2 = getMember2(); NamedCache&lt;Integer, String&gt; cache = member1.getCache(\"test-cache\"); for (int i = 0; i &lt; 10; i++) { cache.put(i, \"value-\" + i); } // ensure all audit events are received = 10 insert events plus 2 cache created events Eventually.assertDeferred(()-&gt;auditEvents.size(), Matchers.is(12)); // shutdown the second member member2.close(); // wait for additional partition events to be received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.greaterThan(16)); dumpAuditEvents(\"testPartitionEvents\"); } Review the CacheLifecycleEventsInterceptor which audits cache lifecycle events. <markup lang=\"java\" >@Interceptor(identifier = \"CacheLifecycleEventsInterceptor\") @CacheLifecycleEvents( {CacheLifecycleEvent.Type.CREATED, CacheLifecycleEvent.Type.DESTROYED, CacheLifecycleEvent.Type.TRUNCATED}) public class CacheLifecycleEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;CacheLifecycleEvent&gt;, Serializable { @Override public void onEvent(CacheLifecycleEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Event from service %s\", event.getServiceName())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the CacheLifecycleEvent that will be intercepted. CREATED, DESTROYED and TRUNCATED are raised asynchronously after the operation is completed. Identifies the type of events, in this case CacheLifecycleEvent Overrides method to respond to the event This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >@Test public void testTruncate() { System.out.println(\"testTruncate\"); auditEvents.clear(); NamedCache&lt;Integer, String&gt; cache1 = getMember1().getCache(\"test-cache\"); cache1.truncate(); // ensure we get two events, one from each storage node Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"TRUNCATED\")).count(), Matchers.is(2L)); dumpAuditEvents(\"truncate\"); } ",
            "title": "Review the classes"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " The interceptors are added via cache config and can be applied at the service or cache level. Review the Cache Scheme Mapping <markup lang=\"xml\" > &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;LifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.LifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;customers&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;UppercaseInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.UppercaseInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;ValidationInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.ValidationInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;!-- cache to store auditing events --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;audit-events&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;!-- any caches other than are defined above will be audited --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; Defines an interceptor called LifecycleEventsInterceptor to log any ConfigurableCacheFactory events. Defines customers cache which has the UppercaseInterceptor and ValidationInterceptor enabled for only this cache Review the Caching Schemes <markup lang=\"xml\" >&lt;!-- Any caches in this scheme will be audited and data put in \"audit-events\" cache. --&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCacheAudit&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;AuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.AuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;EntryProcessorAuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.EntryProcessorAuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;TransferEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.TransferEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;CacheLifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.CacheLifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; Defines auditing-scheme which has the AuditingInterceptor , EntryProcessorAuditingInterceptor , CacheLifecycleEventsInterceptor and TransferEventsInterceptor enabled for any caches using this scheme. ",
            "title": "Review the cache config"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " The example code comprises a number of classes: Tests ServerCacheEventsTest - tests for various cache events ServerPartitionEventsTest - tests for partition based events AbstractEventsTest - a class used by both tests which starts the clusters Model Customer - represents a fictional customer AuditEvent - represents an audit event Interceptors AuditingInterceptor - creates audit events after inserts, updates or removes on a cache EntryProcessorAuditingInterceptor - creates audit events after entry processor executions UppercaseInterceptor - a mutating interceptor that changes the name and address attributes to uppercase ValidationInterceptor - a mutating interceptor that optionally rejects updates if certain business rules are not met TransferEventsInterceptor - creates audit events after any partition transfers made CacheLifecycleEventsInterceptor - creates audit events after caches are created, truncated or destroyed LifecycleEventsInterceptor - logs a message when ConfigurableCacheFactories are activated or destroyed Review the classes Review the Customer class. Some tests use the Customer class which has the following fields: <markup lang=\"java\" >private int id; private String name; private String address; private String customerType; private long creditLimit; Review the AuditEvent class. Some tests use the AuditEvent class which has the following fields: <markup lang=\"java\" >/** * Unique Id for the audit event. */ private UUID id; /** * The target of the event such as cache, partition, etc. */ private String target; /** * The type of event. */ private String eventType; /** * Specific event data. */ private String eventData; /** * Time of the event. */ private long eventTime; Review the AuditingInterceptor which audits any mutations to caches using post-commit events. See here for details of all Partitioned Cache events. <markup lang=\"java\" >@Interceptor(identifier = \"AuditingInterceptor\", order = Interceptor.Order.HIGH) @EntryEvents({EntryEvent.Type.INSERTED, EntryEvent.Type.UPDATED, EntryEvent.Type.REMOVED}) public class AuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryEvent&lt;?, ?&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;?, ?&gt; event) { String oldValue = null; String newValue = null; EntryEvent.Type eventType = event.getType(); Object key = event.getKey(); if (eventType == EntryEvent.Type.REMOVED || eventType == EntryEvent.Type.UPDATED) { oldValue = event.getOriginalValue().toString(); } if (eventType == EntryEvent.Type.INSERTED || eventType == EntryEvent.Type.UPDATED) { newValue = event.getValue().toString(); } AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), eventType.toString(), String.format(\"key=%s, old=%s, new=%s\", key, oldValue, newValue)); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name and optional order of HIGH or LOW as the priority Identifies the EntryEvents that will be intercepted. INSERTED, UPDATED and REMOVED are raised asynchronously after the event has happened Identifies the type of events, in this case EntryEvents Overrides method to respond to the event Identifies the type of event and sets the payload accordingly Adds the audit event to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link AuditingInterceptor} which will audit any changes to caches * that fall thought and match the '*' cache-mapping. */ @Test public void testAuditingInterceptor() { System.out.println(\"testAuditingInterceptor\"); CoherenceClusterMember member = getMember1(); // create two different caches to be audited which will match to the auditing-scheme NamedCache&lt;Integer, String&gt; cache1 = member.getCache(\"test-cache\"); NamedCache&lt;Integer, Customer&gt; cache2 = member.getCache(\"test-customer\"); cache1.truncate(); cache2.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // generate some mutations that will be audited cache1.put(1, \"one\"); cache1.put(2, \"two\"); cache1.put(1, \"ONE\"); cache1.remove(1); dumpAuditEvents(\"testAuditingInterceptor-1\"); // ensure 3 inserts and 1 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); auditEvents.clear(); // generate new set of mutations for customers cache2.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10000)); cache2.put(2, new Customer(2, \"John\", \"Address 2\", Customer.SILVER, 4000)); cache2.clear(); dumpAuditEvents(\"testAuditingInterceptor-2\"); // ensure 2 insert and 2 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.values().size(), Matchers.is(4)); } Review the EntryProcessorAuditingInterceptor which audits entry processors executions using post-commit events. <markup lang=\"java\" >@Interceptor(identifier = \"EntryProcessorAuditingInterceptor\") @EntryProcessorEvents({EntryProcessorEvent.Type.EXECUTED}) public class EntryProcessorAuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryProcessorEvent&gt;, Serializable { @Override public void onEvent(EntryProcessorEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Entries=%d, processor=%s\", event.getEntrySet().size(), event.getProcessor().toString())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the EntryProcessorEvents that will be intercepted. EXECUTED event is raised asynchronously after the event has happened Identifies the type of events, in this case EntryProcessorEvents Overrides method to respond to the event and add to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link EntryProcessorAuditingInterceptor} which will audit any entry processor * executions on caches that match the '*' cache-mapping. */ @Test public void testEntryProcessorInterceptor() { System.out.println(\"testEntryProcessorInterceptor\"); CoherenceClusterMember member = getMember1(); // create a cache to audit entry processor events on NamedCache&lt;Integer, Customer&gt; cache = member.getCache(\"test-customer\"); cache.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // add some entries cache.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10_000)); cache.put(2, new Customer(2, \"Tom\", \"Address 2\", Customer.SILVER, 10_000)); cache.put(3, new Customer(3, \"Helen\", \"Address 3\", Customer.BRONZE, 10_000)); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(3)); auditEvents.clear(); cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); dumpAuditEvents(\"testEntryProcessorInterceptor-1\"); // up to 3 entry processor events and 3 updates Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"EXECUTED\")).count(), Matchers.lessThanOrEqualTo(3L)); Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"UPDATED\")).count(), Matchers.is(3L)); auditEvents.clear(); // invoke an entry processor across all customers to update credit limit to 100,000 cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); cache.invoke(1, Processors.update(Customer::setCreditLimit, 100_000L)); dumpAuditEvents(\"testEntryProcessorInterceptor-2\"); // ensure up to 4 EXECUTED events are received Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"EXECUTED\")).count(), Matchers.lessThanOrEqualTo(4L)); } Review the UppercaseInterceptor which changes the name and address attributes to uppercase. <markup lang=\"java\" >@Interceptor(identifier = \"UppercaseInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class UppercaseInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customer = entry.getValue(); customer.setName(customer.getName().toUpperCase()); customer.setAddress(customer.getAddress().toUpperCase()); entry.setValue(customer); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Ensures the changes are persisted by calling entry.setValue() This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link UppercaseInterceptor} which is defined on the 'customers' cache only, * to update name and address fields to uppercase. */ @Test public void testCustomerUppercaseInterceptor() { System.out.println(\"testCustomerUppercaseInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // put a new Customer with lowercase names and addresses customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.GOLD, 10000L)); // validate that the name and address are uppercase Customer customer = customers.get(1); assertEquals(customer.getName(), \"TIM\"); assertEquals(customer.getAddress(), \"123 JAMES STREET, PERTH\"); // update a customers name and ensure that it is updated to uppercase customers.invoke(1, Processors.update(Customer::setName, \"timothy\")); assertEquals(customers.get(1).getName(), \"TIMOTHY\"); } Review the ValidationInterceptor which rejects or accepts changes based upon some simple business rules. <markup lang=\"java\" >@Interceptor(identifier = \"ValidationInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class ValidationInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customerOld = entry.getOriginalValue(); Customer customerNew = entry.getValue(); EntryEvent.Type eventType = event.getType(); if (eventType == EntryEvent.Type.INSERTING) { // Rule 1 - New customers cannot have credit limit above 1,000,000 unless they are GOLD if (customerNew.getCreditLimit() &gt;= 1_000_000L &amp;&amp; !customerNew.getCustomerType().equals(Customer.GOLD)) { // reject the update throw new RuntimeException(\"Only gold customers may have credit limits above 1,000,000\"); } } else if (eventType == EntryEvent.Type.UPDATING) { // Rule 2 - Cannot change customer type from BRONZE directly to GOLD, must go BRONZE -&gt; SILVER -&gt; GOLD if (customerNew.getCustomerType().equals(Customer.GOLD) &amp;&amp; customerOld.getCustomerType().equals(Customer.BRONZE)) { // reject the update throw new RuntimeException(\"Cannot update customer directly to GOLD from BRONZE\"); } } // otherwise, continue with update entry.setValue(customerNew); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Validates the first business rule if the event is an insert. If the rule fails, then throw a RuntimeException Validates the second business rule if the event is an update. If the rule fails, then throw a RuntimeException Saves the entry if all the business rules pass This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link ValidationInterceptor} which will reject updates if business rules fail. */ @Test public void testValidatingInterceptor() { System.out.println(\"testValidatingInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // try adding a BRONZE customer with credit limit &gt; 1,000,000 try { customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 2_000_000L)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Put was correctly rejected: %s\", e.getMessage()); } // should be rejected assertEquals(customers.size(), 0); // add a normal BRONZE customer, should succeed with credit limit 10,000 customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 10_000L)); assertEquals(customers.size(), 1); // try and update credit limit to GOLD from BRONZE, should fail try { customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Update was correctly rejected: %s\", e.getMessage()); } assertEquals(customers.get(1).getCustomerType(), Customer.BRONZE); } Review the TransferEventsInterceptor which audits partition transfer events. <markup lang=\"java\" >@Interceptor(identifier = \"TransferEventsInterceptor\") @TransferEvents({TransferEvent.Type.ARRIVED, TransferEvent.Type.DEPARTING, TransferEvent.Type.LOST}) public class TransferEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;TransferEvent&gt;, Serializable { @Override public void onEvent(TransferEvent event) { AuditEvent auditEvent = new AuditEvent(\"partition=\" + event.getPartitionId(), event.getType().toString(), String.format(\"Partitions from remote member %s\", event.getRemoteMember())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the TransferEvents that will be intercepted. Transfer events are raised while holding a lock on the partition being transferred that blocks any operations for the partition. Identifies the type of events, in this case TransferEvents Overrides method to respond to the event This is used in the following test in ServerPartitionEventsTest : <markup lang=\"java\" >@Test public void testPartitionEvents() { System.out.println(\"testPartitionEvents\"); CoherenceClusterMember member1 = getMember1(); CoherenceClusterMember member2 = getMember2(); NamedCache&lt;Integer, String&gt; cache = member1.getCache(\"test-cache\"); for (int i = 0; i &lt; 10; i++) { cache.put(i, \"value-\" + i); } // ensure all audit events are received = 10 insert events plus 2 cache created events Eventually.assertDeferred(()-&gt;auditEvents.size(), Matchers.is(12)); // shutdown the second member member2.close(); // wait for additional partition events to be received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.greaterThan(16)); dumpAuditEvents(\"testPartitionEvents\"); } Review the CacheLifecycleEventsInterceptor which audits cache lifecycle events. <markup lang=\"java\" >@Interceptor(identifier = \"CacheLifecycleEventsInterceptor\") @CacheLifecycleEvents( {CacheLifecycleEvent.Type.CREATED, CacheLifecycleEvent.Type.DESTROYED, CacheLifecycleEvent.Type.TRUNCATED}) public class CacheLifecycleEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;CacheLifecycleEvent&gt;, Serializable { @Override public void onEvent(CacheLifecycleEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Event from service %s\", event.getServiceName())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the CacheLifecycleEvent that will be intercepted. CREATED, DESTROYED and TRUNCATED are raised asynchronously after the operation is completed. Identifies the type of events, in this case CacheLifecycleEvent Overrides method to respond to the event This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >@Test public void testTruncate() { System.out.println(\"testTruncate\"); auditEvents.clear(); NamedCache&lt;Integer, String&gt; cache1 = getMember1().getCache(\"test-cache\"); cache1.truncate(); // ensure we get two events, one from each storage node Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"TRUNCATED\")).count(), Matchers.is(2L)); dumpAuditEvents(\"truncate\"); } Review the cache config The interceptors are added via cache config and can be applied at the service or cache level. Review the Cache Scheme Mapping <markup lang=\"xml\" > &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;LifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.LifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;customers&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;UppercaseInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.UppercaseInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;ValidationInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.ValidationInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;!-- cache to store auditing events --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;audit-events&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;!-- any caches other than are defined above will be audited --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; Defines an interceptor called LifecycleEventsInterceptor to log any ConfigurableCacheFactory events. Defines customers cache which has the UppercaseInterceptor and ValidationInterceptor enabled for only this cache Review the Caching Schemes <markup lang=\"xml\" >&lt;!-- Any caches in this scheme will be audited and data put in \"audit-events\" cache. --&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCacheAudit&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;AuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.AuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;EntryProcessorAuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.EntryProcessorAuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;TransferEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.TransferEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;CacheLifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.CacheLifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; Defines auditing-scheme which has the AuditingInterceptor , EntryProcessorAuditingInterceptor , CacheLifecycleEventsInterceptor and TransferEventsInterceptor enabled for any caches using this scheme. ",
            "title": "Review the Tests"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " Run the examples using the test case below. Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.serverevents.ServerPartitionEventsTest com.oracle.coherence.guides.serverevents.ServerCacheEventsTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the various parts of the test code. Output has been truncated and formatted for easier reading. testPartitions Output <markup lang=\"bash\" >testPartitionEvents Dumping the audit events testPartitionEvents AuditEvent{id=2E1E1FE69E, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255209123} AuditEvent{id=54A54A5CED, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255209129} AuditEvent{id=AAA54A5CEE, target='cache=test-cache', eventType='INSERTED', eventData='key=0, old=null, new=value-0', eventTime=1652255209135} AuditEvent{id=A51E1FE69F, target='cache=test-cache', eventType='INSERTED', eventData='key=1, old=null, new=value-1', eventTime=1652255209141} ... AuditEvent{id=A1A54A5CF3, target='cache=test-cache', eventType='INSERTED', eventData='key=9, old=null, new=value-9', eventTime=1652255209169} ... AuditEvent{id=961E1FE6A3, target='partition=0', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209572} AuditEvent{id=261E1FE6A4, target='partition=1', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209580} ... AuditEvent{id=531E1FE6B1, target='partition=14', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209587} Lifecycle events from creation of cache from two storage nodes Insert events for cache entries Partitions arriving from remove member before shutdown testTruncate Output <markup lang=\"bash\" >testTruncate Dumping the audit events truncate AuditEvent{id=B8127D2701, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218772} AuditEvent{id=6BD64A90EA, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218784} AuditEvent{id=7E127D2702, target='cache=test-cache', eventType='TRUNCATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218802} AuditEvent{id=17D64A90EB, target='cache=test-cache', eventType='TRUNCATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218806} Both CREATED and TRUNCATED events are shown. testEntryProcessorInterceptor Output <markup lang=\"bash\" >testEntryProcessorInterceptor Dumping the audit events testEntryProcessorInterceptor-1 AuditEvent{id=AE5BC2D3EB, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(Customer$setCreditLimit...', eventTime=1652319479550} AuditEvent{id=C25BC2D3EC, target='cache=test-customer', eventType='UPDATED', eventData='key=1, old=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}, new=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=100000}', eventTime=1652319479553} AuditEvent{id=3D82ADF7F7, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=2, processor=UpdaterProcessor(Customer$setCreditLimit...'}}, arguments=[]}}, 100000)', eventTime=1652319479553} AuditEvent{id=4382ADF7F8, target='cache=test-customer', eventType='UPDATED', eventData='key=2, old=Customer{id=2, name='Tom', address='Address 2', customerType='SILVER', balance=10000}, new=Customer{id=2, name='Tom', address='Address 2', customerType='SILVER', balance=100000}', eventTime=1652319479556} AuditEvent{id=575BC2D3ED, target='cache=test-customer', eventType='UPDATED', eventData='key=3, old=Customer{id=3, name='Helen', address='Address 3', customerType='BRONZE', balance=10000}, new=Customer{id=3, name='Helen', address='Address 3', customerType='BRONZE', balance=100000}', eventTime=1652319479556} Dumping the audit events testEntryProcessorInterceptor-2 AuditEvent{id=F05BC2D3EE, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=2, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479577} AuditEvent{id=7982ADF7F9, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479578} AuditEvent{id=235BC2D3EF, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479584} Three insert events and two entry processor events. One from each storage-enabled node Three entry processor events, one for an individual invoke() on a key and two from the invokeAll as per item 1 testValidatingInterceptor Output testValidatingInterceptor Output <markup lang=\"bash\" >Put was correctly rejected: Failed to execute [put] with arguments [1, Customer{id=1, name='tim', address='123 james street, perth', customerType='BRONZE', balance=2000000}] Update was correctly rejected: Failed to execute [invoke] with arguments [1, UpdaterProcessor(com.oracle.coherence.guides.serverevents.ServerCacheEventsTest$$Lambda$475/0x00000008003da040@783ecb80, GOLD)] testCustomerUppercaseInterceptor Messages from rejected updates testAuditingInterceptor Output <markup lang=\"bash\" >testAuditingInterceptor Dumping the audit events testAuditingInterceptor-1 AuditEvent{id=1D127D270E, target='cache=test-cache', eventType='INSERTED', eventData='key=1, old=null, new=one', eventTime=1652255219418} AuditEvent{id=25D64A90F4, target='cache=test-cache', eventType='INSERTED', eventData='key=2, old=null, new=two', eventTime=1652255219428} AuditEvent{id=A5127D270F, target='cache=test-cache', eventType='UPDATED', eventData='key=1, old=one, new=ONE', eventTime=1652255219432} AuditEvent{id=EF127D2710, target='cache=test-cache', eventType='REMOVED', eventData='key=1, old=ONE, new=null', eventTime=1652255219436} Dumping the audit events testAuditingInterceptor-2 AuditEvent{id=A5127D2711, target='cache=test-customer', eventType='INSERTED', eventData='key=1, old=null, new=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}', eventTime=1652255219456} AuditEvent{id=5BD64A90F5, target='cache=test-customer', eventType='INSERTED', eventData='key=2, old=null, new=Customer{id=2, name='John', address='Address 2', customerType='SILVER', balance=4000}', eventTime=1652255219460} AuditEvent{id=CAD64A90F6, target='cache=test-customer', eventType='REMOVED', eventData='key=2, old=Customer{id=2, name='John', address='Address 2', customerType='SILVER', balance=4000}, new=null', eventTime=1652255219466} AuditEvent{id=27127D2712, target='cache=test-customer', eventType='REMOVED', eventData='key=1, old=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}, new=null', eventTime=1652255219466} Two inserts, one update and a remove Two inserts and two removes as a result of clear() ",
            "title": "Run the Tests"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " In this guide we walked you through how to use server-side events within Coherence to listen for various events on a Coherence NamedMap or NamedCache . ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " Develop Applications using Server Side Events Client Side Events ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " This guide walks you through how to use server-side events, (also known as \"Live Events\"), within Coherence to listen for various events on a Coherence NamedMap or NamedCache . Coherence provides an event programming model that allows extensibility within a cluster when performing operations against a data grid. The model uses events to represent observable occurrences of cluster operations. The events that are currently supported include: Partitioned Cache Events – A set of events that represent the operations being performed against a set of entries in a cache. Partitioned cache events include both entry events and entry processor events. Entry events are related to inserting, removing, and updating entries in a cache. Entry processor events are related to the execution of entry processors. Partitioned Cache Lifecycle Events – A set of events that represent the operations for creating a cache, destroying a cache, and clearing all entries from a cache. Partitioned Service Events – A set of events that represent the operations being performed by a partitioned service. Partitioned service events include both partition transfer events and partition transaction events. Partition transfer events are related to the movement of partitions among cluster members. Partition transaction events are related to changes that may span multiple caches and are performed within the context of a single request. Lifecycle Events – A set of events that represent the activation and disposal of a ConfigurableCacheFactory instance. Federation Events – A set of events that represent the operations being performed by a federation service. Federation events include both Federated connection events and federated change events. Federated connection events are related to the interaction of federated participants and federated change events are related to cache updates. In this example we will not cover Federation Events. Events are registered in the cache configuration against either a cache service or individual caches via cache mappings. The classes are annotated to identify what types of events they will receive. For more information on server-side events, see the Coherence documentation. Please see the Coherence documentation for more information on client events. Table of Contents What You Will Build What You Need Building the Example Code Review the Tests Review the classes Review the cache configuration Run the Tests Summary See Also What You Will Build In this example you will run a number of tests that demonstrate the following features of server-side events including: Understanding where to declare interceptors in your cache config Listening for cache events related to mutations of cache data, and execution of entry processors Listening for transfer events related to partition transfers and loss events Listening for partitioned cache events related to creation, destruction and truncating of caches Listening for lifecycle events for ConfigurableCacheFactory instantiation What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can skip the tests in the initial build by adding the following options: -DskipTests for Maven or -x test for Gradle. Running the Examples This example comprises a number of tests showing various server-side events features. Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Review the Tests The example code comprises a number of classes: Tests ServerCacheEventsTest - tests for various cache events ServerPartitionEventsTest - tests for partition based events AbstractEventsTest - a class used by both tests which starts the clusters Model Customer - represents a fictional customer AuditEvent - represents an audit event Interceptors AuditingInterceptor - creates audit events after inserts, updates or removes on a cache EntryProcessorAuditingInterceptor - creates audit events after entry processor executions UppercaseInterceptor - a mutating interceptor that changes the name and address attributes to uppercase ValidationInterceptor - a mutating interceptor that optionally rejects updates if certain business rules are not met TransferEventsInterceptor - creates audit events after any partition transfers made CacheLifecycleEventsInterceptor - creates audit events after caches are created, truncated or destroyed LifecycleEventsInterceptor - logs a message when ConfigurableCacheFactories are activated or destroyed Review the classes Review the Customer class. Some tests use the Customer class which has the following fields: <markup lang=\"java\" >private int id; private String name; private String address; private String customerType; private long creditLimit; Review the AuditEvent class. Some tests use the AuditEvent class which has the following fields: <markup lang=\"java\" >/** * Unique Id for the audit event. */ private UUID id; /** * The target of the event such as cache, partition, etc. */ private String target; /** * The type of event. */ private String eventType; /** * Specific event data. */ private String eventData; /** * Time of the event. */ private long eventTime; Review the AuditingInterceptor which audits any mutations to caches using post-commit events. See here for details of all Partitioned Cache events. <markup lang=\"java\" >@Interceptor(identifier = \"AuditingInterceptor\", order = Interceptor.Order.HIGH) @EntryEvents({EntryEvent.Type.INSERTED, EntryEvent.Type.UPDATED, EntryEvent.Type.REMOVED}) public class AuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryEvent&lt;?, ?&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;?, ?&gt; event) { String oldValue = null; String newValue = null; EntryEvent.Type eventType = event.getType(); Object key = event.getKey(); if (eventType == EntryEvent.Type.REMOVED || eventType == EntryEvent.Type.UPDATED) { oldValue = event.getOriginalValue().toString(); } if (eventType == EntryEvent.Type.INSERTED || eventType == EntryEvent.Type.UPDATED) { newValue = event.getValue().toString(); } AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), eventType.toString(), String.format(\"key=%s, old=%s, new=%s\", key, oldValue, newValue)); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name and optional order of HIGH or LOW as the priority Identifies the EntryEvents that will be intercepted. INSERTED, UPDATED and REMOVED are raised asynchronously after the event has happened Identifies the type of events, in this case EntryEvents Overrides method to respond to the event Identifies the type of event and sets the payload accordingly Adds the audit event to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link AuditingInterceptor} which will audit any changes to caches * that fall thought and match the '*' cache-mapping. */ @Test public void testAuditingInterceptor() { System.out.println(\"testAuditingInterceptor\"); CoherenceClusterMember member = getMember1(); // create two different caches to be audited which will match to the auditing-scheme NamedCache&lt;Integer, String&gt; cache1 = member.getCache(\"test-cache\"); NamedCache&lt;Integer, Customer&gt; cache2 = member.getCache(\"test-customer\"); cache1.truncate(); cache2.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // generate some mutations that will be audited cache1.put(1, \"one\"); cache1.put(2, \"two\"); cache1.put(1, \"ONE\"); cache1.remove(1); dumpAuditEvents(\"testAuditingInterceptor-1\"); // ensure 3 inserts and 1 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); auditEvents.clear(); // generate new set of mutations for customers cache2.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10000)); cache2.put(2, new Customer(2, \"John\", \"Address 2\", Customer.SILVER, 4000)); cache2.clear(); dumpAuditEvents(\"testAuditingInterceptor-2\"); // ensure 2 insert and 2 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.values().size(), Matchers.is(4)); } Review the EntryProcessorAuditingInterceptor which audits entry processors executions using post-commit events. <markup lang=\"java\" >@Interceptor(identifier = \"EntryProcessorAuditingInterceptor\") @EntryProcessorEvents({EntryProcessorEvent.Type.EXECUTED}) public class EntryProcessorAuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryProcessorEvent&gt;, Serializable { @Override public void onEvent(EntryProcessorEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Entries=%d, processor=%s\", event.getEntrySet().size(), event.getProcessor().toString())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the EntryProcessorEvents that will be intercepted. EXECUTED event is raised asynchronously after the event has happened Identifies the type of events, in this case EntryProcessorEvents Overrides method to respond to the event and add to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link EntryProcessorAuditingInterceptor} which will audit any entry processor * executions on caches that match the '*' cache-mapping. */ @Test public void testEntryProcessorInterceptor() { System.out.println(\"testEntryProcessorInterceptor\"); CoherenceClusterMember member = getMember1(); // create a cache to audit entry processor events on NamedCache&lt;Integer, Customer&gt; cache = member.getCache(\"test-customer\"); cache.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // add some entries cache.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10_000)); cache.put(2, new Customer(2, \"Tom\", \"Address 2\", Customer.SILVER, 10_000)); cache.put(3, new Customer(3, \"Helen\", \"Address 3\", Customer.BRONZE, 10_000)); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(3)); auditEvents.clear(); cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); dumpAuditEvents(\"testEntryProcessorInterceptor-1\"); // up to 3 entry processor events and 3 updates Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"EXECUTED\")).count(), Matchers.lessThanOrEqualTo(3L)); Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"UPDATED\")).count(), Matchers.is(3L)); auditEvents.clear(); // invoke an entry processor across all customers to update credit limit to 100,000 cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); cache.invoke(1, Processors.update(Customer::setCreditLimit, 100_000L)); dumpAuditEvents(\"testEntryProcessorInterceptor-2\"); // ensure up to 4 EXECUTED events are received Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"EXECUTED\")).count(), Matchers.lessThanOrEqualTo(4L)); } Review the UppercaseInterceptor which changes the name and address attributes to uppercase. <markup lang=\"java\" >@Interceptor(identifier = \"UppercaseInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class UppercaseInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customer = entry.getValue(); customer.setName(customer.getName().toUpperCase()); customer.setAddress(customer.getAddress().toUpperCase()); entry.setValue(customer); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Ensures the changes are persisted by calling entry.setValue() This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link UppercaseInterceptor} which is defined on the 'customers' cache only, * to update name and address fields to uppercase. */ @Test public void testCustomerUppercaseInterceptor() { System.out.println(\"testCustomerUppercaseInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // put a new Customer with lowercase names and addresses customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.GOLD, 10000L)); // validate that the name and address are uppercase Customer customer = customers.get(1); assertEquals(customer.getName(), \"TIM\"); assertEquals(customer.getAddress(), \"123 JAMES STREET, PERTH\"); // update a customers name and ensure that it is updated to uppercase customers.invoke(1, Processors.update(Customer::setName, \"timothy\")); assertEquals(customers.get(1).getName(), \"TIMOTHY\"); } Review the ValidationInterceptor which rejects or accepts changes based upon some simple business rules. <markup lang=\"java\" >@Interceptor(identifier = \"ValidationInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class ValidationInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customerOld = entry.getOriginalValue(); Customer customerNew = entry.getValue(); EntryEvent.Type eventType = event.getType(); if (eventType == EntryEvent.Type.INSERTING) { // Rule 1 - New customers cannot have credit limit above 1,000,000 unless they are GOLD if (customerNew.getCreditLimit() &gt;= 1_000_000L &amp;&amp; !customerNew.getCustomerType().equals(Customer.GOLD)) { // reject the update throw new RuntimeException(\"Only gold customers may have credit limits above 1,000,000\"); } } else if (eventType == EntryEvent.Type.UPDATING) { // Rule 2 - Cannot change customer type from BRONZE directly to GOLD, must go BRONZE -&gt; SILVER -&gt; GOLD if (customerNew.getCustomerType().equals(Customer.GOLD) &amp;&amp; customerOld.getCustomerType().equals(Customer.BRONZE)) { // reject the update throw new RuntimeException(\"Cannot update customer directly to GOLD from BRONZE\"); } } // otherwise, continue with update entry.setValue(customerNew); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Validates the first business rule if the event is an insert. If the rule fails, then throw a RuntimeException Validates the second business rule if the event is an update. If the rule fails, then throw a RuntimeException Saves the entry if all the business rules pass This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link ValidationInterceptor} which will reject updates if business rules fail. */ @Test public void testValidatingInterceptor() { System.out.println(\"testValidatingInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // try adding a BRONZE customer with credit limit &gt; 1,000,000 try { customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 2_000_000L)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Put was correctly rejected: %s\", e.getMessage()); } // should be rejected assertEquals(customers.size(), 0); // add a normal BRONZE customer, should succeed with credit limit 10,000 customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 10_000L)); assertEquals(customers.size(), 1); // try and update credit limit to GOLD from BRONZE, should fail try { customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Update was correctly rejected: %s\", e.getMessage()); } assertEquals(customers.get(1).getCustomerType(), Customer.BRONZE); } Review the TransferEventsInterceptor which audits partition transfer events. <markup lang=\"java\" >@Interceptor(identifier = \"TransferEventsInterceptor\") @TransferEvents({TransferEvent.Type.ARRIVED, TransferEvent.Type.DEPARTING, TransferEvent.Type.LOST}) public class TransferEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;TransferEvent&gt;, Serializable { @Override public void onEvent(TransferEvent event) { AuditEvent auditEvent = new AuditEvent(\"partition=\" + event.getPartitionId(), event.getType().toString(), String.format(\"Partitions from remote member %s\", event.getRemoteMember())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the TransferEvents that will be intercepted. Transfer events are raised while holding a lock on the partition being transferred that blocks any operations for the partition. Identifies the type of events, in this case TransferEvents Overrides method to respond to the event This is used in the following test in ServerPartitionEventsTest : <markup lang=\"java\" >@Test public void testPartitionEvents() { System.out.println(\"testPartitionEvents\"); CoherenceClusterMember member1 = getMember1(); CoherenceClusterMember member2 = getMember2(); NamedCache&lt;Integer, String&gt; cache = member1.getCache(\"test-cache\"); for (int i = 0; i &lt; 10; i++) { cache.put(i, \"value-\" + i); } // ensure all audit events are received = 10 insert events plus 2 cache created events Eventually.assertDeferred(()-&gt;auditEvents.size(), Matchers.is(12)); // shutdown the second member member2.close(); // wait for additional partition events to be received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.greaterThan(16)); dumpAuditEvents(\"testPartitionEvents\"); } Review the CacheLifecycleEventsInterceptor which audits cache lifecycle events. <markup lang=\"java\" >@Interceptor(identifier = \"CacheLifecycleEventsInterceptor\") @CacheLifecycleEvents( {CacheLifecycleEvent.Type.CREATED, CacheLifecycleEvent.Type.DESTROYED, CacheLifecycleEvent.Type.TRUNCATED}) public class CacheLifecycleEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;CacheLifecycleEvent&gt;, Serializable { @Override public void onEvent(CacheLifecycleEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Event from service %s\", event.getServiceName())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the CacheLifecycleEvent that will be intercepted. CREATED, DESTROYED and TRUNCATED are raised asynchronously after the operation is completed. Identifies the type of events, in this case CacheLifecycleEvent Overrides method to respond to the event This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >@Test public void testTruncate() { System.out.println(\"testTruncate\"); auditEvents.clear(); NamedCache&lt;Integer, String&gt; cache1 = getMember1().getCache(\"test-cache\"); cache1.truncate(); // ensure we get two events, one from each storage node Eventually.assertDeferred(() -&gt; auditEvents.values().stream().filter(p -&gt; p.getEventType().equals(\"TRUNCATED\")).count(), Matchers.is(2L)); dumpAuditEvents(\"truncate\"); } Review the cache config The interceptors are added via cache config and can be applied at the service or cache level. Review the Cache Scheme Mapping <markup lang=\"xml\" > &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;LifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.LifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;customers&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;UppercaseInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.UppercaseInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;ValidationInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.ValidationInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;!-- cache to store auditing events --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;audit-events&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;!-- any caches other than are defined above will be audited --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; Defines an interceptor called LifecycleEventsInterceptor to log any ConfigurableCacheFactory events. Defines customers cache which has the UppercaseInterceptor and ValidationInterceptor enabled for only this cache Review the Caching Schemes <markup lang=\"xml\" >&lt;!-- Any caches in this scheme will be audited and data put in \"audit-events\" cache. --&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCacheAudit&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;AuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.AuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;EntryProcessorAuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.EntryProcessorAuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;TransferEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.TransferEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;CacheLifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.CacheLifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; Defines auditing-scheme which has the AuditingInterceptor , EntryProcessorAuditingInterceptor , CacheLifecycleEventsInterceptor and TransferEventsInterceptor enabled for any caches using this scheme. Run the Tests Run the examples using the test case below. Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.serverevents.ServerPartitionEventsTest com.oracle.coherence.guides.serverevents.ServerCacheEventsTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the various parts of the test code. Output has been truncated and formatted for easier reading. testPartitions Output <markup lang=\"bash\" >testPartitionEvents Dumping the audit events testPartitionEvents AuditEvent{id=2E1E1FE69E, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255209123} AuditEvent{id=54A54A5CED, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255209129} AuditEvent{id=AAA54A5CEE, target='cache=test-cache', eventType='INSERTED', eventData='key=0, old=null, new=value-0', eventTime=1652255209135} AuditEvent{id=A51E1FE69F, target='cache=test-cache', eventType='INSERTED', eventData='key=1, old=null, new=value-1', eventTime=1652255209141} ... AuditEvent{id=A1A54A5CF3, target='cache=test-cache', eventType='INSERTED', eventData='key=9, old=null, new=value-9', eventTime=1652255209169} ... AuditEvent{id=961E1FE6A3, target='partition=0', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209572} AuditEvent{id=261E1FE6A4, target='partition=1', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209580} ... AuditEvent{id=531E1FE6B1, target='partition=14', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209587} Lifecycle events from creation of cache from two storage nodes Insert events for cache entries Partitions arriving from remove member before shutdown testTruncate Output <markup lang=\"bash\" >testTruncate Dumping the audit events truncate AuditEvent{id=B8127D2701, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218772} AuditEvent{id=6BD64A90EA, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218784} AuditEvent{id=7E127D2702, target='cache=test-cache', eventType='TRUNCATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218802} AuditEvent{id=17D64A90EB, target='cache=test-cache', eventType='TRUNCATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218806} Both CREATED and TRUNCATED events are shown. testEntryProcessorInterceptor Output <markup lang=\"bash\" >testEntryProcessorInterceptor Dumping the audit events testEntryProcessorInterceptor-1 AuditEvent{id=AE5BC2D3EB, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(Customer$setCreditLimit...', eventTime=1652319479550} AuditEvent{id=C25BC2D3EC, target='cache=test-customer', eventType='UPDATED', eventData='key=1, old=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}, new=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=100000}', eventTime=1652319479553} AuditEvent{id=3D82ADF7F7, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=2, processor=UpdaterProcessor(Customer$setCreditLimit...'}}, arguments=[]}}, 100000)', eventTime=1652319479553} AuditEvent{id=4382ADF7F8, target='cache=test-customer', eventType='UPDATED', eventData='key=2, old=Customer{id=2, name='Tom', address='Address 2', customerType='SILVER', balance=10000}, new=Customer{id=2, name='Tom', address='Address 2', customerType='SILVER', balance=100000}', eventTime=1652319479556} AuditEvent{id=575BC2D3ED, target='cache=test-customer', eventType='UPDATED', eventData='key=3, old=Customer{id=3, name='Helen', address='Address 3', customerType='BRONZE', balance=10000}, new=Customer{id=3, name='Helen', address='Address 3', customerType='BRONZE', balance=100000}', eventTime=1652319479556} Dumping the audit events testEntryProcessorInterceptor-2 AuditEvent{id=F05BC2D3EE, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=2, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479577} AuditEvent{id=7982ADF7F9, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479578} AuditEvent{id=235BC2D3EF, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479584} Three insert events and two entry processor events. One from each storage-enabled node Three entry processor events, one for an individual invoke() on a key and two from the invokeAll as per item 1 testValidatingInterceptor Output testValidatingInterceptor Output <markup lang=\"bash\" >Put was correctly rejected: Failed to execute [put] with arguments [1, Customer{id=1, name='tim', address='123 james street, perth', customerType='BRONZE', balance=2000000}] Update was correctly rejected: Failed to execute [invoke] with arguments [1, UpdaterProcessor(com.oracle.coherence.guides.serverevents.ServerCacheEventsTest$$Lambda$475/0x00000008003da040@783ecb80, GOLD)] testCustomerUppercaseInterceptor Messages from rejected updates testAuditingInterceptor Output <markup lang=\"bash\" >testAuditingInterceptor Dumping the audit events testAuditingInterceptor-1 AuditEvent{id=1D127D270E, target='cache=test-cache', eventType='INSERTED', eventData='key=1, old=null, new=one', eventTime=1652255219418} AuditEvent{id=25D64A90F4, target='cache=test-cache', eventType='INSERTED', eventData='key=2, old=null, new=two', eventTime=1652255219428} AuditEvent{id=A5127D270F, target='cache=test-cache', eventType='UPDATED', eventData='key=1, old=one, new=ONE', eventTime=1652255219432} AuditEvent{id=EF127D2710, target='cache=test-cache', eventType='REMOVED', eventData='key=1, old=ONE, new=null', eventTime=1652255219436} Dumping the audit events testAuditingInterceptor-2 AuditEvent{id=A5127D2711, target='cache=test-customer', eventType='INSERTED', eventData='key=1, old=null, new=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}', eventTime=1652255219456} AuditEvent{id=5BD64A90F5, target='cache=test-customer', eventType='INSERTED', eventData='key=2, old=null, new=Customer{id=2, name='John', address='Address 2', customerType='SILVER', balance=4000}', eventTime=1652255219460} AuditEvent{id=CAD64A90F6, target='cache=test-customer', eventType='REMOVED', eventData='key=2, old=Customer{id=2, name='John', address='Address 2', customerType='SILVER', balance=4000}, new=null', eventTime=1652255219466} AuditEvent{id=27127D2712, target='cache=test-customer', eventType='REMOVED', eventData='key=1, old=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}, new=null', eventTime=1652255219466} Two inserts, one update and a remove Two inserts and two removes as a result of clear() Summary In this guide we walked you through how to use server-side events within Coherence to listen for various events on a Coherence NamedMap or NamedCache . See Also Develop Applications using Server Side Events Client Side Events ",
            "title": "Server-Side Events"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Project Maven Configuration Persistence Configuration Listening to JMX Notifications Build and Run the Example Enable Active Persistence Enable a Snapshot Archiver Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " You will review the requirements for running both on-demand and active persistence and carry out the following: Start one or more cache servers with on-demand persistence Start a CohQL session to insert data and create and manage snapshots Start a JMX MBean listener to monitor Persistence operations Change on-demand to active persistence and show this in action Work with archiving snapshots ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " About 20-30 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " The project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. cache-server - Runs a DefaultCacheServer cohql - Runs a CohQL session notifications - Runs a process to subscribe to JMX Notification events cache-server - Runs a DefaultCacheServer <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cache-server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cache-server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.mode=on-demand&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.base.dir=persistence-data&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Set on-demand mode, which is the default Set the base directory for all persistence directories cohql - Runs a CohQL session <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;persistence-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.base.dir&lt;/key&gt; &lt;value&gt;persistence-data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.mode&lt;/key&gt; &lt;value&gt;on-demand&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;127.0.0.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; notifications - Runs a process to subscribe to JMX Notification events <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;notifications&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;notifications&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx128m&lt;/argument&gt; &lt;argument&gt;-Xms128m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.localstorage=false&lt;/argument&gt; &lt;argument&gt;com.oracle.coherence.tutorials.persistence.NotificationWatcher&lt;/argument&gt; &lt;argument&gt;PartitionedCache&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; ",
            "title": "Maven Configuration"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " By default, any partitioned service, including Federated services, will default to on-demand mode. This mode allows you to create and manage snapshots to default directories without any setup. A coherence directory off the users home directory is used to store all persistence-related data. If you wish to enable active persistence mode you can use a system property -Dcoherence.distributed.persistence.mode=active and this will use the default directories as described above. In this example we are also defining the base persistence directory using a system property -Dcoherence.distributed.persistence.base.dir=persistence-data . All other persistence directories will be created below this directory. Please see here for more details on configuring your persistence locations. In this tutorial, Persistence is configured in two files: An operational override file is used to configure non-default persistence environments and archive locations A cache configuration with file the &lt;persistence&gt; element is used to associate services with persistence environments, if you are not using the defaults. (This is initially commented out.) Cache Configuration File <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;!-- initially commented out as we are using system properties &lt;persistence&gt; &lt;environment&gt;default-active&lt;/environment&gt; &lt;archiver&gt;shared-directory-archiver&lt;/archiver&gt; &lt;/persistence&gt; --&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The above cache configuration has the &lt;persistence&gt; element commented out for the first part of this tutorial. Operational Override File <markup lang=\"xml\" >&lt;coherence xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-operational-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-operational-config coherence-operational-config.xsd\"&gt; &lt;cluster-config&gt; &lt;snapshot-archivers&gt; &lt;directory-archiver id=\"shared-directory-archiver\"&gt; &lt;archive-directory system-property=\"coherence.distributed.persistence.archive.dir\"&gt;persistence-data/archives&lt;/archive-directory&gt; &lt;/directory-archiver&gt; &lt;/snapshot-archivers&gt; &lt;/cluster-config&gt; &lt;management-config&gt; &lt;managed-nodes system-property=\"coherence.management\"&gt;all&lt;/managed-nodes&gt; &lt;/management-config&gt; &lt;/coherence&gt; Defines a snapshot archiver to archive to a given directory ",
            "title": "Persistence Configuration"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Persistence operations generate JMX notifications. You can register for these notifications to monitor and understand how long these operations are taking. You can use a tool such as VisualVM with the Coherence VisualVM plugin to monitor and manage persistence. See the VisualVM Plugin project on GitHub . Main entry point <markup lang=\"java\" >public static void main(String[] args) { if (args.length == 0) { System.out.println(\"Please provide a list of services to listen for notifications on\"); System.exit(1); } Set&lt;String&gt; setServices = new HashSet&lt;&gt;(Arrays.asList(args)); System.out.println(\"Getting MBeanServer...\"); Cluster cluster = CacheFactory.ensureCluster(); MBeanServer server = MBeanHelper.findMBeanServer(); Registry registry = cluster.getManagement(); if (server == null) { throw new RuntimeException(\"Unable to find MBeanServer\"); } try { for (String serviceName : setServices) { System.out.println(\"Registering listener for \" + serviceName); String mBeanName = \"Coherence:\" + CachePersistenceHelper.getMBeanName(serviceName); waitForRegistration(registry, mBeanName); ObjectName beanName = new ObjectName(mBeanName); NotificationListener listener = new PersistenceNotificationListener(serviceName); server.addNotificationListener(beanName, listener, null, null); mapListeners.put(beanName, listener); } System.out.println(\"Waiting for notifications. Use CTRL-C to interrupt.\"); Thread.sleep(Long.MAX_VALUE); } catch (Exception e) { e.printStackTrace(); } finally { // unregister all registered notifications mapListeners.forEach((k, v) -&gt; { try { server.removeNotificationListener(k, v); } catch (Exception eIgnore) { // ignore } }); } } Join the cluster and retrieve the MBeanServer and Registry Loop through the services provided as arguments and get the MBean name for the Persistence MBean Ensure the MBean is registered Add a notification listener on the Persistence MBean PersistenceNotificationListener implementation <markup lang=\"java\" >public static class PersistenceNotificationListener implements NotificationListener { Handle the notification <markup lang=\"java\" >@Override public synchronized void handleNotification(Notification notification, Object oHandback) { counter.incrementAndGet(); String userData = notification.getUserData().toString(); String message = notification.getMessage() + \" \" + notification.getUserData(); // default // determine if it's a begin or end notification String type = notification.getType(); if (type.indexOf(BEGIN) &gt; 0) { // handle begin notification and save the start time mapNotify.put(type, notification.getTimeStamp()); message = notification.getMessage(); } else if (type.indexOf(END) &gt; 0) { // handle end notification and try and find the matching begin notification String begin = type.replaceAll(END, BEGIN); Long start = mapNotify.get(begin); if (start != null) { message = \" \" + notification.getMessage() + (userData == null || userData.isEmpty() ? \"\" : userData) + \" (Duration=\" + (notification.getTimeStamp() - start) + \"ms)\"; mapNotify.remove(begin); } } else { message = serviceName + \": \" + type + \"\"; } System.out.println(new Date(notification.getTimeStamp()) + \" : \" + serviceName + \" (\" + type + \") \" + message); } Store the details of the begin notification Handle the end notification and determine the operation length ",
            "title": "Listening to JMX Notifications"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Maven Configuration The project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. cache-server - Runs a DefaultCacheServer cohql - Runs a CohQL session notifications - Runs a process to subscribe to JMX Notification events cache-server - Runs a DefaultCacheServer <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cache-server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cache-server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.mode=on-demand&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.base.dir=persistence-data&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Set on-demand mode, which is the default Set the base directory for all persistence directories cohql - Runs a CohQL session <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;persistence-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.base.dir&lt;/key&gt; &lt;value&gt;persistence-data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.mode&lt;/key&gt; &lt;value&gt;on-demand&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;127.0.0.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; notifications - Runs a process to subscribe to JMX Notification events <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;notifications&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;notifications&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx128m&lt;/argument&gt; &lt;argument&gt;-Xms128m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.localstorage=false&lt;/argument&gt; &lt;argument&gt;com.oracle.coherence.tutorials.persistence.NotificationWatcher&lt;/argument&gt; &lt;argument&gt;PartitionedCache&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Persistence Configuration By default, any partitioned service, including Federated services, will default to on-demand mode. This mode allows you to create and manage snapshots to default directories without any setup. A coherence directory off the users home directory is used to store all persistence-related data. If you wish to enable active persistence mode you can use a system property -Dcoherence.distributed.persistence.mode=active and this will use the default directories as described above. In this example we are also defining the base persistence directory using a system property -Dcoherence.distributed.persistence.base.dir=persistence-data . All other persistence directories will be created below this directory. Please see here for more details on configuring your persistence locations. In this tutorial, Persistence is configured in two files: An operational override file is used to configure non-default persistence environments and archive locations A cache configuration with file the &lt;persistence&gt; element is used to associate services with persistence environments, if you are not using the defaults. (This is initially commented out.) Cache Configuration File <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;!-- initially commented out as we are using system properties &lt;persistence&gt; &lt;environment&gt;default-active&lt;/environment&gt; &lt;archiver&gt;shared-directory-archiver&lt;/archiver&gt; &lt;/persistence&gt; --&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The above cache configuration has the &lt;persistence&gt; element commented out for the first part of this tutorial. Operational Override File <markup lang=\"xml\" >&lt;coherence xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-operational-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-operational-config coherence-operational-config.xsd\"&gt; &lt;cluster-config&gt; &lt;snapshot-archivers&gt; &lt;directory-archiver id=\"shared-directory-archiver\"&gt; &lt;archive-directory system-property=\"coherence.distributed.persistence.archive.dir\"&gt;persistence-data/archives&lt;/archive-directory&gt; &lt;/directory-archiver&gt; &lt;/snapshot-archivers&gt; &lt;/cluster-config&gt; &lt;management-config&gt; &lt;managed-nodes system-property=\"coherence.management\"&gt;all&lt;/managed-nodes&gt; &lt;/management-config&gt; &lt;/coherence&gt; Defines a snapshot archiver to archive to a given directory Listening to JMX Notifications Persistence operations generate JMX notifications. You can register for these notifications to monitor and understand how long these operations are taking. You can use a tool such as VisualVM with the Coherence VisualVM plugin to monitor and manage persistence. See the VisualVM Plugin project on GitHub . Main entry point <markup lang=\"java\" >public static void main(String[] args) { if (args.length == 0) { System.out.println(\"Please provide a list of services to listen for notifications on\"); System.exit(1); } Set&lt;String&gt; setServices = new HashSet&lt;&gt;(Arrays.asList(args)); System.out.println(\"Getting MBeanServer...\"); Cluster cluster = CacheFactory.ensureCluster(); MBeanServer server = MBeanHelper.findMBeanServer(); Registry registry = cluster.getManagement(); if (server == null) { throw new RuntimeException(\"Unable to find MBeanServer\"); } try { for (String serviceName : setServices) { System.out.println(\"Registering listener for \" + serviceName); String mBeanName = \"Coherence:\" + CachePersistenceHelper.getMBeanName(serviceName); waitForRegistration(registry, mBeanName); ObjectName beanName = new ObjectName(mBeanName); NotificationListener listener = new PersistenceNotificationListener(serviceName); server.addNotificationListener(beanName, listener, null, null); mapListeners.put(beanName, listener); } System.out.println(\"Waiting for notifications. Use CTRL-C to interrupt.\"); Thread.sleep(Long.MAX_VALUE); } catch (Exception e) { e.printStackTrace(); } finally { // unregister all registered notifications mapListeners.forEach((k, v) -&gt; { try { server.removeNotificationListener(k, v); } catch (Exception eIgnore) { // ignore } }); } } Join the cluster and retrieve the MBeanServer and Registry Loop through the services provided as arguments and get the MBean name for the Persistence MBean Ensure the MBean is registered Add a notification listener on the Persistence MBean PersistenceNotificationListener implementation <markup lang=\"java\" >public static class PersistenceNotificationListener implements NotificationListener { Handle the notification <markup lang=\"java\" >@Override public synchronized void handleNotification(Notification notification, Object oHandback) { counter.incrementAndGet(); String userData = notification.getUserData().toString(); String message = notification.getMessage() + \" \" + notification.getUserData(); // default // determine if it's a begin or end notification String type = notification.getType(); if (type.indexOf(BEGIN) &gt; 0) { // handle begin notification and save the start time mapNotify.put(type, notification.getTimeStamp()); message = notification.getMessage(); } else if (type.indexOf(END) &gt; 0) { // handle end notification and try and find the matching begin notification String begin = type.replaceAll(END, BEGIN); Long start = mapNotify.get(begin); if (start != null) { message = \" \" + notification.getMessage() + (userData == null || userData.isEmpty() ? \"\" : userData) + \" (Duration=\" + (notification.getTimeStamp() - start) + \"ms)\"; mapNotify.remove(begin); } } else { message = serviceName + \": \" + type + \"\"; } System.out.println(new Date(notification.getTimeStamp()) + \" : \" + serviceName + \" (\" + type + \") \" + message); } Store the details of the begin notification Handle the end notification and determine the operation length ",
            "title": "Review the Project"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P cache-server Start a CohQL session. <markup lang=\"bash\" >./mvnw exec:java -P cohql Start a JMX Listener. <markup lang=\"bash\" >./mvnw exec:exec -P notifications ",
            "title": "Maven"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./gradlew runCacheServer Start a CohQL session. <markup lang=\"bash\" >./gradlew runCohql --console=plain and <markup lang=\"bash\" >./gradlew runNotifications --console=plain ",
            "title": "Gradle"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " In the CohQL session, run the following commands to add data: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Create a snapshot containing this data <markup lang=\"bash\" >CohQL&gt; list snapshots Results \"PartitionedCache\": [] CohQL&gt; create snapshot \"data\" \"PartitionedCache\" Are you sure you want to create a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Creating snapshot 'data' for service 'PartitionedCache' Results \"Success\" CohQL&gt; list snapshots Results \"PartitionedCache\": [\"data\"] You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:03 AWST 2022 : PartitionedCache (create.snapshot.begin) Building snapshot \"data\" Tue Apr 26 10:57:06 AWST 2022 : PartitionedCache (create.snapshot.end) Successfully created snapshot \"data\" (Duration=3445ms) Clear the cache and recover the snapshot <markup lang=\"bash\" >CohQL&gt; delete from test Results CohQL&gt; select count() from test Results 0 CohQL&gt; recover snapshot \"data\" \"PartitionedCache\" Are you sure you want to recover a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Recovering snapshot 'data' for service 'PartitionedCache' 2022-04-11 16:23:06.691/499.700 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been suspended 2022-04-11 16:23:09.247/502.256 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been resumed Results \"Success\" select count() from test Results 3 You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.snapshot.begin) Recovering Snapshot \"data\" Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.begin) Recovering snapshot \"data\" Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.end) Recovery Completed (Duration=623ms) Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.snapshot.end) Successfully recovered snapshot \"data\" (Duration=631ms) You will be able to see the snapshots in the directory persistence-data/snapshots . ",
            "title": "Run the following commands to exercise on-demand Persistence"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Once you have built the project as described earlier in this document, you can run it via Maven or Gradle. Maven Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P cache-server Start a CohQL session. <markup lang=\"bash\" >./mvnw exec:java -P cohql Start a JMX Listener. <markup lang=\"bash\" >./mvnw exec:exec -P notifications Gradle Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./gradlew runCacheServer Start a CohQL session. <markup lang=\"bash\" >./gradlew runCohql --console=plain and <markup lang=\"bash\" >./gradlew runNotifications --console=plain Run the following commands to exercise on-demand Persistence In the CohQL session, run the following commands to add data: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Create a snapshot containing this data <markup lang=\"bash\" >CohQL&gt; list snapshots Results \"PartitionedCache\": [] CohQL&gt; create snapshot \"data\" \"PartitionedCache\" Are you sure you want to create a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Creating snapshot 'data' for service 'PartitionedCache' Results \"Success\" CohQL&gt; list snapshots Results \"PartitionedCache\": [\"data\"] You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:03 AWST 2022 : PartitionedCache (create.snapshot.begin) Building snapshot \"data\" Tue Apr 26 10:57:06 AWST 2022 : PartitionedCache (create.snapshot.end) Successfully created snapshot \"data\" (Duration=3445ms) Clear the cache and recover the snapshot <markup lang=\"bash\" >CohQL&gt; delete from test Results CohQL&gt; select count() from test Results 0 CohQL&gt; recover snapshot \"data\" \"PartitionedCache\" Are you sure you want to recover a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Recovering snapshot 'data' for service 'PartitionedCache' 2022-04-11 16:23:06.691/499.700 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been suspended 2022-04-11 16:23:09.247/502.256 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been resumed Results \"Success\" select count() from test Results 3 You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.snapshot.begin) Recovering Snapshot \"data\" Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.begin) Recovering snapshot \"data\" Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.end) Recovery Completed (Duration=623ms) Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.snapshot.end) Successfully recovered snapshot \"data\" (Duration=631ms) You will be able to see the snapshots in the directory persistence-data/snapshots . ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " After shutting down all running processes, in the file pom.xml change on-demand to active for the cohql and cache-server profiles to enable active persistence. Run the cache-server , cohql and notifications as described above. In the CohQL session, run the following commands to add data <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Shutdown all three processes and restart the cache-server and cohql processes, then continue below. Re-query the test cache <markup lang=\"bash\" >CohQL&gt; select * from test Results \"two\" \"three\" \"one\" You can see that the cache data has automatically been recovered from disk during cluster startup. This active persistence data is stored in the directory persistence-data/active below the persistence tutorial directory. ",
            "title": "Run the following commands to exercise active Persistence"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Run the following commands to exercise active Persistence After shutting down all running processes, in the file pom.xml change on-demand to active for the cohql and cache-server profiles to enable active persistence. Run the cache-server , cohql and notifications as described above. In the CohQL session, run the following commands to add data <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Shutdown all three processes and restart the cache-server and cohql processes, then continue below. Re-query the test cache <markup lang=\"bash\" >CohQL&gt; select * from test Results \"two\" \"three\" \"one\" You can see that the cache data has automatically been recovered from disk during cluster startup. This active persistence data is stored in the directory persistence-data/active below the persistence tutorial directory. ",
            "title": "Enable Active Persistence"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Snapshots can be archived to a central location and then later retrieved and restored. Archiving snapshots requires defining the directory where archives are stored and configuring cache services to use an archive directory. To enable a snapshot archiver in this example, you need to uncomment the &lt;persistence&gt; element in the cache config file src/main/resources/persistence-cache-config.xml , and rebuild the project using Maven or Gradle. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;!-- initially commented out as we are using system properties &lt;persistence&gt; &lt;environment&gt;default-active&lt;/environment&gt; &lt;archiver&gt;shared-directory-archiver&lt;/archiver&gt; &lt;/persistence&gt; --&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Archive the existing data snapshot. <markup lang=\"bash\" >CohQL&gt; archive snapshot \"data\" \"PartitionedCache\" Are you sure you want to archive a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Archiving snapshot 'data' for service 'PartitionedCache' Results \"Success\" Inspect the archive directory contents to determine the cluster name <markup lang=\"bash\" >$ cd persistence-data/archives/ $ ls timmiddleton-s-cluster In the above the cluster name is timmiddleton&#8217;s cluster but a sanitized directory of timmiddleton-s-cluster has been used. View the contents of the archived snapshot directory, substituting your cluster directory. <markup lang=\"bash\" >$ cd timmiddleton-s-cluster/PartitionedCache/data/ $ ls 0-1-18063c8cc4c-1 12-1-18063c8cc4c-1 16-1-18063c8cc4c-1 2-1-18063c8cc4c-1 23-1-18063c8cc4c-1 27-1-18063c8cc4c-1 30-1-18063c8cc4c-1 7-1-18063c8cc4c-1 1-1-18063c8cc4c-1 13-1-18063c8cc4c-1 17-1-18063c8cc4c-1 20-1-18063c8cc4c-1 24-1-18063c8cc4c-1 28-1-18063c8cc4c-1 4-1-18063c8cc4c-1 8-1-18063c8cc4c-1 10-1-18063c8cc4c-1 14-1-18063c8cc4c-1 18-1-18063c8cc4c-1 21-1-18063c8cc4c-1 25-1-18063c8cc4c-1 29-1-18063c8cc4c-1 5-1-18063c8cc4c-1 9-1-18063c8cc4c-1 11-1-18063c8cc4c-1 15-1-18063c8cc4c-1 19-1-18063c8cc4c-1 22-1-18063c8cc4c-1 26-1-18063c8cc4c-1 3-1-18063c8cc4c-1 6-1-18063c8cc4c-1 meta.properties The directory shows 31 different data files and a meta.properties file that contains some metadata. These files are binary files and can only be used by recovering an archived snapshot. Validate the archived snapshot The following command will ensure that the archived snapshot can be retrieved and is valid. You should always use this command to ensure the integrity of your archived snapshots. <markup lang=\"bash\" >CohQL&gt; validate archived snapshot \"data\" \"PartitionedCache\" verbose ... various messages left out ... Results Attribute Value ---------------------------- ------------------------------------------------------------- Partition Count 31 Archived Snapshot Name=data, Service=PartitionedCache Original Storage Format BDB Storage Version 0 Implementation Version 0 Number of Partitions Present 31 Is Complete? true Is Archived Snapshot? true Persistence Version 14 Statistics test Size=3, Bytes=41, Indexes=0, Triggers=0, Listeners=0, Locks=0 Remove the local snapshot In this tutorial, we remove the local snapshot so that we can then retrieve the archived snapshot. A local snapshot of the same name cannot exist already if we want to retrieve an archived snapshot. <markup lang=\"bash\" >CohQL&gt; remove snapshot \"data\" \"PartitionedCache\" Are you sure you want to remove snapshot called 'data' for service 'PartitionedCache'? (y/n): y Removing snapshot 'data' for service 'PartitionedCache' Results \"Success CohQL&gt; list snapshots Results \"PartitionedCache\": [] Retrieve the archived snapshot <markup lang=\"bash\" >CohQL&gt; retrieve archived snapshot \"data\" \"PartitionedCache\" Are you sure you want to retrieve a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Retrieving snapshot 'data' for service 'PartitionedCache' Results \"Success\" CohQL&gt; list snapshots Results \"PartitionedCache\": [\"data\"] Remove all data from the cache and recover the snapshot <markup lang=\"bash\" >CohQL&gt; select count() from test Results 3 CohQL&gt; delete from test Results select count() from test Results 0 CohQL&gt; recover snapshot \"data\" \"PartitionedCache\" Are you sure you want to recover a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Recovering snapshot 'data' for service 'PartitionedCache' 2022-04-26 12:53:26.866/1734.102 Oracle Coherence CE 22.06 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=2): Service PartitionedCache has been suspended 2022-04-26 12:53:28.709/1735.944 Oracle Coherence CE 22.06 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=2): Service PartitionedCache has been resumed Results \"Success\" CohQL&gt; select count() from test Results 3 ",
            "title": "Enable a Snapshot Archiver"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " In this tutorial, you have learnt about Coherence Persistence and how you can use it with the Coherence Query Language (CohQL) to create, recover and managed snapshots, monitor snapshot operations via JMX MBean notifications as well as work with archived snapshots. ",
            "title": "Summary"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Persistence Configuration ",
            "title": "See Also"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " This tutorial walks through Coherence Persistence by using Coherence Query Language (CohQL) to create, recover and manage snapshots, monitor snapshot operations via JMX MBean notifications and work with archived snapshots. Coherence Persistence is a set of tools and technologies that manage the persistence and recovery of Coherence distributed caches. Cached data is persisted so that it can be quickly recovered after a catastrophic failure or after a cluster restart due to planned maintenance. Persistence can operate in two modes: On-Demand persistence mode – a cache service is manually persisted and recovered upon request using the persistence coordinator. The persistence coordinator is exposed as an MBean interface that provides operations for creating, archiving, and recovering snapshots of a cache service. Active persistence mode – In this mode, cache contents are automatically persisted on all mutations and are automatically recovered on cluster/service startup. The persistence coordinator can still be used in active persistence mode to perform on-demand snapshots. For more information on Coherence Persistence, please see the Coherence Documentation . Table of Contents What You Will Build What You Need Building the Example Code Review the Project Maven Configuration Persistence Configuration Listening to JMX Notifications Build and Run the Example Enable Active Persistence Enable a Snapshot Archiver Summary See Also What You Will Build You will review the requirements for running both on-demand and active persistence and carry out the following: Start one or more cache servers with on-demand persistence Start a CohQL session to insert data and create and manage snapshots Start a JMX MBean listener to monitor Persistence operations Change on-demand to active persistence and show this in action Work with archiving snapshots What You Need About 20-30 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Review the Project Maven Configuration The project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. cache-server - Runs a DefaultCacheServer cohql - Runs a CohQL session notifications - Runs a process to subscribe to JMX Notification events cache-server - Runs a DefaultCacheServer <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cache-server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cache-server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.mode=on-demand&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.base.dir=persistence-data&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Set on-demand mode, which is the default Set the base directory for all persistence directories cohql - Runs a CohQL session <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;persistence-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.base.dir&lt;/key&gt; &lt;value&gt;persistence-data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.mode&lt;/key&gt; &lt;value&gt;on-demand&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;127.0.0.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; notifications - Runs a process to subscribe to JMX Notification events <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;notifications&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;notifications&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx128m&lt;/argument&gt; &lt;argument&gt;-Xms128m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.localstorage=false&lt;/argument&gt; &lt;argument&gt;com.oracle.coherence.tutorials.persistence.NotificationWatcher&lt;/argument&gt; &lt;argument&gt;PartitionedCache&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Persistence Configuration By default, any partitioned service, including Federated services, will default to on-demand mode. This mode allows you to create and manage snapshots to default directories without any setup. A coherence directory off the users home directory is used to store all persistence-related data. If you wish to enable active persistence mode you can use a system property -Dcoherence.distributed.persistence.mode=active and this will use the default directories as described above. In this example we are also defining the base persistence directory using a system property -Dcoherence.distributed.persistence.base.dir=persistence-data . All other persistence directories will be created below this directory. Please see here for more details on configuring your persistence locations. In this tutorial, Persistence is configured in two files: An operational override file is used to configure non-default persistence environments and archive locations A cache configuration with file the &lt;persistence&gt; element is used to associate services with persistence environments, if you are not using the defaults. (This is initially commented out.) Cache Configuration File <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;!-- initially commented out as we are using system properties &lt;persistence&gt; &lt;environment&gt;default-active&lt;/environment&gt; &lt;archiver&gt;shared-directory-archiver&lt;/archiver&gt; &lt;/persistence&gt; --&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The above cache configuration has the &lt;persistence&gt; element commented out for the first part of this tutorial. Operational Override File <markup lang=\"xml\" >&lt;coherence xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-operational-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-operational-config coherence-operational-config.xsd\"&gt; &lt;cluster-config&gt; &lt;snapshot-archivers&gt; &lt;directory-archiver id=\"shared-directory-archiver\"&gt; &lt;archive-directory system-property=\"coherence.distributed.persistence.archive.dir\"&gt;persistence-data/archives&lt;/archive-directory&gt; &lt;/directory-archiver&gt; &lt;/snapshot-archivers&gt; &lt;/cluster-config&gt; &lt;management-config&gt; &lt;managed-nodes system-property=\"coherence.management\"&gt;all&lt;/managed-nodes&gt; &lt;/management-config&gt; &lt;/coherence&gt; Defines a snapshot archiver to archive to a given directory Listening to JMX Notifications Persistence operations generate JMX notifications. You can register for these notifications to monitor and understand how long these operations are taking. You can use a tool such as VisualVM with the Coherence VisualVM plugin to monitor and manage persistence. See the VisualVM Plugin project on GitHub . Main entry point <markup lang=\"java\" >public static void main(String[] args) { if (args.length == 0) { System.out.println(\"Please provide a list of services to listen for notifications on\"); System.exit(1); } Set&lt;String&gt; setServices = new HashSet&lt;&gt;(Arrays.asList(args)); System.out.println(\"Getting MBeanServer...\"); Cluster cluster = CacheFactory.ensureCluster(); MBeanServer server = MBeanHelper.findMBeanServer(); Registry registry = cluster.getManagement(); if (server == null) { throw new RuntimeException(\"Unable to find MBeanServer\"); } try { for (String serviceName : setServices) { System.out.println(\"Registering listener for \" + serviceName); String mBeanName = \"Coherence:\" + CachePersistenceHelper.getMBeanName(serviceName); waitForRegistration(registry, mBeanName); ObjectName beanName = new ObjectName(mBeanName); NotificationListener listener = new PersistenceNotificationListener(serviceName); server.addNotificationListener(beanName, listener, null, null); mapListeners.put(beanName, listener); } System.out.println(\"Waiting for notifications. Use CTRL-C to interrupt.\"); Thread.sleep(Long.MAX_VALUE); } catch (Exception e) { e.printStackTrace(); } finally { // unregister all registered notifications mapListeners.forEach((k, v) -&gt; { try { server.removeNotificationListener(k, v); } catch (Exception eIgnore) { // ignore } }); } } Join the cluster and retrieve the MBeanServer and Registry Loop through the services provided as arguments and get the MBean name for the Persistence MBean Ensure the MBean is registered Add a notification listener on the Persistence MBean PersistenceNotificationListener implementation <markup lang=\"java\" >public static class PersistenceNotificationListener implements NotificationListener { Handle the notification <markup lang=\"java\" >@Override public synchronized void handleNotification(Notification notification, Object oHandback) { counter.incrementAndGet(); String userData = notification.getUserData().toString(); String message = notification.getMessage() + \" \" + notification.getUserData(); // default // determine if it's a begin or end notification String type = notification.getType(); if (type.indexOf(BEGIN) &gt; 0) { // handle begin notification and save the start time mapNotify.put(type, notification.getTimeStamp()); message = notification.getMessage(); } else if (type.indexOf(END) &gt; 0) { // handle end notification and try and find the matching begin notification String begin = type.replaceAll(END, BEGIN); Long start = mapNotify.get(begin); if (start != null) { message = \" \" + notification.getMessage() + (userData == null || userData.isEmpty() ? \"\" : userData) + \" (Duration=\" + (notification.getTimeStamp() - start) + \"ms)\"; mapNotify.remove(begin); } } else { message = serviceName + \": \" + type + \"\"; } System.out.println(new Date(notification.getTimeStamp()) + \" : \" + serviceName + \" (\" + type + \") \" + message); } Store the details of the begin notification Handle the end notification and determine the operation length Run the Example Once you have built the project as described earlier in this document, you can run it via Maven or Gradle. Maven Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P cache-server Start a CohQL session. <markup lang=\"bash\" >./mvnw exec:java -P cohql Start a JMX Listener. <markup lang=\"bash\" >./mvnw exec:exec -P notifications Gradle Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./gradlew runCacheServer Start a CohQL session. <markup lang=\"bash\" >./gradlew runCohql --console=plain and <markup lang=\"bash\" >./gradlew runNotifications --console=plain Run the following commands to exercise on-demand Persistence In the CohQL session, run the following commands to add data: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Create a snapshot containing this data <markup lang=\"bash\" >CohQL&gt; list snapshots Results \"PartitionedCache\": [] CohQL&gt; create snapshot \"data\" \"PartitionedCache\" Are you sure you want to create a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Creating snapshot 'data' for service 'PartitionedCache' Results \"Success\" CohQL&gt; list snapshots Results \"PartitionedCache\": [\"data\"] You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:03 AWST 2022 : PartitionedCache (create.snapshot.begin) Building snapshot \"data\" Tue Apr 26 10:57:06 AWST 2022 : PartitionedCache (create.snapshot.end) Successfully created snapshot \"data\" (Duration=3445ms) Clear the cache and recover the snapshot <markup lang=\"bash\" >CohQL&gt; delete from test Results CohQL&gt; select count() from test Results 0 CohQL&gt; recover snapshot \"data\" \"PartitionedCache\" Are you sure you want to recover a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Recovering snapshot 'data' for service 'PartitionedCache' 2022-04-11 16:23:06.691/499.700 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been suspended 2022-04-11 16:23:09.247/502.256 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been resumed Results \"Success\" select count() from test Results 3 You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.snapshot.begin) Recovering Snapshot \"data\" Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.begin) Recovering snapshot \"data\" Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.end) Recovery Completed (Duration=623ms) Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.snapshot.end) Successfully recovered snapshot \"data\" (Duration=631ms) You will be able to see the snapshots in the directory persistence-data/snapshots . Enable Active Persistence Run the following commands to exercise active Persistence After shutting down all running processes, in the file pom.xml change on-demand to active for the cohql and cache-server profiles to enable active persistence. Run the cache-server , cohql and notifications as described above. In the CohQL session, run the following commands to add data <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Shutdown all three processes and restart the cache-server and cohql processes, then continue below. Re-query the test cache <markup lang=\"bash\" >CohQL&gt; select * from test Results \"two\" \"three\" \"one\" You can see that the cache data has automatically been recovered from disk during cluster startup. This active persistence data is stored in the directory persistence-data/active below the persistence tutorial directory. Enable a Snapshot Archiver Snapshots can be archived to a central location and then later retrieved and restored. Archiving snapshots requires defining the directory where archives are stored and configuring cache services to use an archive directory. To enable a snapshot archiver in this example, you need to uncomment the &lt;persistence&gt; element in the cache config file src/main/resources/persistence-cache-config.xml , and rebuild the project using Maven or Gradle. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;!-- initially commented out as we are using system properties &lt;persistence&gt; &lt;environment&gt;default-active&lt;/environment&gt; &lt;archiver&gt;shared-directory-archiver&lt;/archiver&gt; &lt;/persistence&gt; --&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Archive the existing data snapshot. <markup lang=\"bash\" >CohQL&gt; archive snapshot \"data\" \"PartitionedCache\" Are you sure you want to archive a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Archiving snapshot 'data' for service 'PartitionedCache' Results \"Success\" Inspect the archive directory contents to determine the cluster name <markup lang=\"bash\" >$ cd persistence-data/archives/ $ ls timmiddleton-s-cluster In the above the cluster name is timmiddleton&#8217;s cluster but a sanitized directory of timmiddleton-s-cluster has been used. View the contents of the archived snapshot directory, substituting your cluster directory. <markup lang=\"bash\" >$ cd timmiddleton-s-cluster/PartitionedCache/data/ $ ls 0-1-18063c8cc4c-1 12-1-18063c8cc4c-1 16-1-18063c8cc4c-1 2-1-18063c8cc4c-1 23-1-18063c8cc4c-1 27-1-18063c8cc4c-1 30-1-18063c8cc4c-1 7-1-18063c8cc4c-1 1-1-18063c8cc4c-1 13-1-18063c8cc4c-1 17-1-18063c8cc4c-1 20-1-18063c8cc4c-1 24-1-18063c8cc4c-1 28-1-18063c8cc4c-1 4-1-18063c8cc4c-1 8-1-18063c8cc4c-1 10-1-18063c8cc4c-1 14-1-18063c8cc4c-1 18-1-18063c8cc4c-1 21-1-18063c8cc4c-1 25-1-18063c8cc4c-1 29-1-18063c8cc4c-1 5-1-18063c8cc4c-1 9-1-18063c8cc4c-1 11-1-18063c8cc4c-1 15-1-18063c8cc4c-1 19-1-18063c8cc4c-1 22-1-18063c8cc4c-1 26-1-18063c8cc4c-1 3-1-18063c8cc4c-1 6-1-18063c8cc4c-1 meta.properties The directory shows 31 different data files and a meta.properties file that contains some metadata. These files are binary files and can only be used by recovering an archived snapshot. Validate the archived snapshot The following command will ensure that the archived snapshot can be retrieved and is valid. You should always use this command to ensure the integrity of your archived snapshots. <markup lang=\"bash\" >CohQL&gt; validate archived snapshot \"data\" \"PartitionedCache\" verbose ... various messages left out ... Results Attribute Value ---------------------------- ------------------------------------------------------------- Partition Count 31 Archived Snapshot Name=data, Service=PartitionedCache Original Storage Format BDB Storage Version 0 Implementation Version 0 Number of Partitions Present 31 Is Complete? true Is Archived Snapshot? true Persistence Version 14 Statistics test Size=3, Bytes=41, Indexes=0, Triggers=0, Listeners=0, Locks=0 Remove the local snapshot In this tutorial, we remove the local snapshot so that we can then retrieve the archived snapshot. A local snapshot of the same name cannot exist already if we want to retrieve an archived snapshot. <markup lang=\"bash\" >CohQL&gt; remove snapshot \"data\" \"PartitionedCache\" Are you sure you want to remove snapshot called 'data' for service 'PartitionedCache'? (y/n): y Removing snapshot 'data' for service 'PartitionedCache' Results \"Success CohQL&gt; list snapshots Results \"PartitionedCache\": [] Retrieve the archived snapshot <markup lang=\"bash\" >CohQL&gt; retrieve archived snapshot \"data\" \"PartitionedCache\" Are you sure you want to retrieve a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Retrieving snapshot 'data' for service 'PartitionedCache' Results \"Success\" CohQL&gt; list snapshots Results \"PartitionedCache\": [\"data\"] Remove all data from the cache and recover the snapshot <markup lang=\"bash\" >CohQL&gt; select count() from test Results 3 CohQL&gt; delete from test Results select count() from test Results 0 CohQL&gt; recover snapshot \"data\" \"PartitionedCache\" Are you sure you want to recover a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Recovering snapshot 'data' for service 'PartitionedCache' 2022-04-26 12:53:26.866/1734.102 Oracle Coherence CE 22.06 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=2): Service PartitionedCache has been suspended 2022-04-26 12:53:28.709/1735.944 Oracle Coherence CE 22.06 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=2): Service PartitionedCache has been resumed Results \"Success\" CohQL&gt; select count() from test Results 3 Summary In this tutorial, you have learnt about Coherence Persistence and how you can use it with the Coherence Query Language (CohQL) to create, recover and managed snapshots, monitor snapshot operations via JMX MBean notifications as well as work with archived snapshots. See Also Persistence Configuration ",
            "title": "Persistence"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " This guide will build simple examples showing different uses of the Health Check API from application code and in containerized environments. The Basic Health Check API introduces the basic health check APIs. Application Health Checks shows how to add custom application health checks Container Health Checks shows how to add health checks to be used in containers Docker Image Health Checks shows adding health checks to an image built with Docker Buildah Image Health Checks shows adding health checks to an image built with Buildah ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Version 22.06 of Coherence introduced a Health Check API to provide simple checks for the overall health of a Coherence member. This guide shows some ways this API can be used. What You Will Build This guide will build simple examples showing different uses of the Health Check API from application code and in containerized environments. The Basic Health Check API introduces the basic health check APIs. Application Health Checks shows how to add custom application health checks Container Health Checks shows how to add health checks to be used in containers Docker Image Health Checks shows adding health checks to an image built with Docker Buildah Image Health Checks shows adding health checks to an image built with Buildah What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package ",
            "title": "Health Checks"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The first test in BasicHealthIT checks that everything is \"started\". The Coherence instance is obtained (there is only one instance running in this case so the Coherence.getInstance() method can be used). From the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(registry::allHealthChecksStarted, is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksStarted() returns true , which it should as soon as all services are started. At this point Coherence may not be \"ready\" or \"safe\", but it is started. ",
            "title": "Check All Health Checks are Started"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The second test in BasicHealthIT checks that everything is \"ready\". The Coherence instance is obtained and from the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(registry::allHealthChecksStarted, is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksReady() returns true , which it should as soon as all services reach the \"ready\" state. ",
            "title": "Check All Health Checks are Ready"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The third test in BasicHealthIT checks that everything is \"safe\". The Coherence instance is obtained and from the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(registry::allHealthChecksStarted, is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksSafe() returns true , which it should as soon as all services reach the \"safe\" state. ",
            "title": "Check All Health Checks are Safe"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " A Collection of health checks can be obtained using the Registry instances getHealthChecks() method. The example below shows a simple test case that obtains all the registered health checks. There is an assertion that the collection returned is not empty. As the test uses the default Coherence cache configuration file, this will start a distributed cache service named PartitionedCache , so there will be a health check registered with this name. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthChecks() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Collection&lt;HealthCheck&gt; healthChecks = registry.getHealthChecks(); assertThat(healthChecks.isEmpty(), is(false)); HealthCheck healthCheck = healthChecks.stream() .filter(h-&gt;\"PartitionedCache\".equals(h.getName())) .findFirst() .orElse(null); assertThat(healthCheck, is(notNullValue())); } ",
            "title": "Gat All Health Checks"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Instead of getting the collection of all health checks, a single health check can be obtained by using its name. The Registry instances getHealthCheck(String name) method can be used to obtain a health check instance by name. The method returns an Optional that will be empty if there is no health check registered with the specified name. The example below obtains the health check named PartitionedCache , which should exist as the test uses the default Coherence cache configuration file. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthCheckByName() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Optional&lt;HealthCheck&gt; optional = registry.getHealthCheck(\"PartitionedCache\"); assertThat(optional.isPresent(), is(true)); } ",
            "title": "Get a Health Check by Name"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The Registry health check API has methods to obtain instances of the health checks that have been registered on the local member. Gat All Health Checks A Collection of health checks can be obtained using the Registry instances getHealthChecks() method. The example below shows a simple test case that obtains all the registered health checks. There is an assertion that the collection returned is not empty. As the test uses the default Coherence cache configuration file, this will start a distributed cache service named PartitionedCache , so there will be a health check registered with this name. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthChecks() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Collection&lt;HealthCheck&gt; healthChecks = registry.getHealthChecks(); assertThat(healthChecks.isEmpty(), is(false)); HealthCheck healthCheck = healthChecks.stream() .filter(h-&gt;\"PartitionedCache\".equals(h.getName())) .findFirst() .orElse(null); assertThat(healthCheck, is(notNullValue())); } Get a Health Check by Name Instead of getting the collection of all health checks, a single health check can be obtained by using its name. The Registry instances getHealthCheck(String name) method can be used to obtain a health check instance by name. The method returns an Optional that will be empty if there is no health check registered with the specified name. The example below obtains the health check named PartitionedCache , which should exist as the test uses the default Coherence cache configuration file. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthCheckByName() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Optional&lt;HealthCheck&gt; optional = registry.getHealthCheck(\"PartitionedCache\"); assertThat(optional.isPresent(), is(true)); } ",
            "title": "Get Health Check Instances"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The basic health check API includes methods to check the local member&#8217;s health and obtain health check instances. This API is demonstrated using a simple integration test in src/test/java/com/oracle/coherence/guides/health/BasicHealthIT.java The test first bootstraps a Coherence storage member using the Coherence bootstrap API. The test will fail if Coherence takes longer than five minutes to start (it should be up in seconds). <markup lang=\"java\" title=\"BasicHealthIT.java\" > @BeforeAll static void startCoherence() throws Exception { Coherence.clusterMember() .start() .get(5, TimeUnit.MINUTES); } When the tests finish Coherence is shut down. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @AfterAll static void cleanup() { Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } Check All Health Checks are Started The first test in BasicHealthIT checks that everything is \"started\". The Coherence instance is obtained (there is only one instance running in this case so the Coherence.getInstance() method can be used). From the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(registry::allHealthChecksStarted, is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksStarted() returns true , which it should as soon as all services are started. At this point Coherence may not be \"ready\" or \"safe\", but it is started. Check All Health Checks are Ready The second test in BasicHealthIT checks that everything is \"ready\". The Coherence instance is obtained and from the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(registry::allHealthChecksStarted, is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksReady() returns true , which it should as soon as all services reach the \"ready\" state. Check All Health Checks are Safe The third test in BasicHealthIT checks that everything is \"safe\". The Coherence instance is obtained and from the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(registry::allHealthChecksStarted, is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksSafe() returns true , which it should as soon as all services reach the \"safe\" state. Get Health Check Instances The Registry health check API has methods to obtain instances of the health checks that have been registered on the local member. Gat All Health Checks A Collection of health checks can be obtained using the Registry instances getHealthChecks() method. The example below shows a simple test case that obtains all the registered health checks. There is an assertion that the collection returned is not empty. As the test uses the default Coherence cache configuration file, this will start a distributed cache service named PartitionedCache , so there will be a health check registered with this name. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthChecks() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Collection&lt;HealthCheck&gt; healthChecks = registry.getHealthChecks(); assertThat(healthChecks.isEmpty(), is(false)); HealthCheck healthCheck = healthChecks.stream() .filter(h-&gt;\"PartitionedCache\".equals(h.getName())) .findFirst() .orElse(null); assertThat(healthCheck, is(notNullValue())); } Get a Health Check by Name Instead of getting the collection of all health checks, a single health check can be obtained by using its name. The Registry instances getHealthCheck(String name) method can be used to obtain a health check instance by name. The method returns an Optional that will be empty if there is no health check registered with the specified name. The example below obtains the health check named PartitionedCache , which should exist as the test uses the default Coherence cache configuration file. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthCheckByName() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Optional&lt;HealthCheck&gt; optional = registry.getHealthCheck(\"PartitionedCache\"); assertThat(optional.isPresent(), is(true)); } ",
            "title": "The Basic Health Check API"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Application health checks can be automatically registered by Coherence during start-up. When Coherence starts, it will use the Java ServiceLoader to discover any HealthCheck implementations, and automatically register them. To automatically register the example ApplicationHealth class above, create a META-INF/service/com.tangosol.util.HealthCheck file, containing a single line that is the name of the application health check. <markup title=\"META-INF/service/com.tangosol.util.HealthCheck\" >com.oracle.coherence.guides.health.ApplicationHealth; Alternatively, if using a module-info.java file add the health check using the provides clause. <markup lang=\"java\" title=\"module-info.java\" >module coherence.guides.health { provides com.tangosol.util.HealthCheck with com.oracle.coherence.guides.health.ApplicationHealth; } When Coherence starts, it will use ServiceLoader to load HealthCheck instances, which will discover and load an ApplicationHealth instance. ",
            "title": "Health Check Auto-Registration"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Applications can add custom health checks by creating a class that implements the com.tangosol.util.HealthCheck interface, and registering the health check with the Registry . The example ApplicationHealth class below implements the HealthCheck interface. The getName() method returns \"Demo\" , which is a unique name for this health check. In this example, the class does not have any processing in the health check methods. In a real application health check these methods would perform custom application specific checks. <markup lang=\"java\" title=\"ApplicationHealth.java\" >/** * A simple custom health check. */ public class ApplicationHealth implements HealthCheck { /** * The health check name. */ public static final String NAME = \"Demo\"; @Override public String getName() { return NAME; } @Override public boolean isReady() { return true; } @Override public boolean isLive() { return true; } @Override public boolean isStarted() { return true; } @Override public boolean isSafe() { return true; } } The health check can be registered in application code using the Registry.register(HealthCheck hc) method. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); ApplicationHealth healthCheck = new ApplicationHealth(); registry.register(healthCheck); Optional&lt;HealthCheck&gt; optional = registry.getHealthCheck(ApplicationHealth.NAME); assertThat(optional.isPresent(), is(true)); When no longer required, the health check can be unregistered using the Registry.unregister(String name) method. <markup lang=\"java\" > registry.unregister(healthCheck); optional = registry.getHealthCheck(ApplicationHealth.NAME); assertThat(optional.isPresent(), is(false)); Health Check Auto-Registration Application health checks can be automatically registered by Coherence during start-up. When Coherence starts, it will use the Java ServiceLoader to discover any HealthCheck implementations, and automatically register them. To automatically register the example ApplicationHealth class above, create a META-INF/service/com.tangosol.util.HealthCheck file, containing a single line that is the name of the application health check. <markup title=\"META-INF/service/com.tangosol.util.HealthCheck\" >com.oracle.coherence.guides.health.ApplicationHealth; Alternatively, if using a module-info.java file add the health check using the provides clause. <markup lang=\"java\" title=\"module-info.java\" >module coherence.guides.health { provides com.tangosol.util.HealthCheck with com.oracle.coherence.guides.health.ApplicationHealth; } When Coherence starts, it will use ServiceLoader to load HealthCheck instances, which will discover and load an ApplicationHealth instance. ",
            "title": "Application Health Checks"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The example above can be built and tested using a simple Maven command. The command will run a Maven build with the docker profile enabled, which will use Docker to build an image. The name of the image is configured in the properties' section of the example pom.xml to be coherence-health:1.0.0 . <markup lang=\"bash\" >mvn clean package -DskipTests -Pdocker A container can then be run using the image. The normal docker run command is used, in this case the container is given the name test . <markup lang=\"bash\" >docker run -d --name test coherence-health:1.0.0 After starting the container, the set of running containers can be listed using docker ps , which should display the test container: <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 2 seconds (health: starting) test Because the image has a health check configured the status in this case included the current health state Up 2 seconds (health: starting) . At this point the container is still starting, so the Coherence health endpoint ( http://127.0.0.1:6676/ready ) has not returned a 200 response, as Coherence is still starting. Once Coherence has started and te health check reports ready, the container status will change to healthy . <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 4 minutes (healthy) test ",
            "title": "Build and Run the Image"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " To use health checks in Docker, the HEALTHCHECK instruction can be used in the Dockerfile . The format of the HEALTHCHECK instruction is shown below: <markup >HEALTHCHECK [OPTIONS] CMD command The command is typically a simple command line, such as curl or a shell script, or Java command line. For example, if the Coherence health check endpoint is enabled on a fixed port 6676 , then the HEALTHCHECK instruction&#8217;s CMD can be set to curl -f http://127.0.0.1:6676/ready An example of a simple Coherence Dockerfile with a health check is shown below. This example image uses OpenJDK as a base image. Coherence jar is added to the image and the health check port fixed to 6676 using the COHERENCE_HEALTH_HTTP_PORT environment variable. When the image runs, the entry point will just start Coherence . <markup title=\"src/docker/OpenJDK.Dockerfile\" ># Copyright (c) 2022, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at # https://oss.oracle.com/licenses/upl. # FROM openjdk:11-jre ADD coherence.jar /coherence/lib/coherence.jar ENV COHERENCE_HEALTH_HTTP_PORT=6676 HEALTHCHECK --start-period=30s --interval=30s \\ CMD curl -f http://127.0.0.1:6676/ready || exit 1 ENTRYPOINT [\"java\"] CMD [\"-cp\", \"/coherence/lib/*\", \"com.tangosol.net.Coherence\"] Build and Run the Image The example above can be built and tested using a simple Maven command. The command will run a Maven build with the docker profile enabled, which will use Docker to build an image. The name of the image is configured in the properties' section of the example pom.xml to be coherence-health:1.0.0 . <markup lang=\"bash\" >mvn clean package -DskipTests -Pdocker A container can then be run using the image. The normal docker run command is used, in this case the container is given the name test . <markup lang=\"bash\" >docker run -d --name test coherence-health:1.0.0 After starting the container, the set of running containers can be listed using docker ps , which should display the test container: <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 2 seconds (health: starting) test Because the image has a health check configured the status in this case included the current health state Up 2 seconds (health: starting) . At this point the container is still starting, so the Coherence health endpoint ( http://127.0.0.1:6676/ready ) has not returned a 200 response, as Coherence is still starting. Once Coherence has started and te health check reports ready, the container status will change to healthy . <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 4 minutes (healthy) test ",
            "title": "Docker Image Health Checks"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The Podman and Buildah tools are common replacements for Docker when running in Linux. When using Buildah to create images, health checks can be added to an image using the Buildah CLI. To support health checks Buildah must be configured to use \"Docker\" format. The simplest way to do this is to export the BUILDAH_FORMAT environment variable <markup lang=\"bash\" >export BUILDAH_FORMAT=docker Now, the Buildah CLI can be used to create an image. <markup lang=\"bash\" >buildah from --name coherence openjdk:11-jre buildah copy coherence coherence.jar /coherence/lib/coherence.jar buildah config --healthcheck-start-period 10s --healthcheck-interval 10s \\ --healthcheck \"CMD curl -f http://127.0.0.1:6676/ready || exit 1\" coherence buildah config \\ --entrypoint '[\"java\"]' --cmd '-cp /coherence/lib/* com.tangosol.net.Coherence' \\ -e COHERENCE_HEALTH_HTTP_PORT=6676 \\ coherence buildah commit coherence coherence-health:1.0.0 buildah push -f v2s2 coherence-health:1.0.0 docker-daemon:coherence-health:1.0.0 The Buildah commands above build the same image that was built with the Dockerfile in the previous section. The final command above pushes the coherence-health:1.0.0 image built by Buildah into a Docker daemon, so it can be run using Docker. ",
            "title": "Buildah Image Health Checks"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Sometimes a distroless image is used as a base image for applications. These are images that do not contain a Linux distribution. There are various reasons for this such as image size, but mainly security, as the base image does not contain a lot of Linux utilities that may introduce CVEs. The example Coherence images use distroless base images. When using a distroless base image, the curl utility is not present, so it cannot be used as the health check command. In the distroless base images used by Coherence, all that is present is a Linux kernel and Java. This means that the only way to run any health check commands would be to execute a Java command. As part of the Coherence health check API there is a simple http client class com.tangosol.util.HealthCheckClient that can be used to execute a health check as a Java command. The Java command line to execute a health check would be: <markup lang=\"bash\" >java -cp coherence.jar com.tangosol.util.HealthCheckClient http://127.0.0.1:6676/ready This Distroless.Dockerfile in the source code contains an example of using a Java health check command. Because the health check command is running Java and not a simple O/S command, the format of the CMD parameters is slightly different than the previous example. <markup title=\"src/docker/Distroless.Dockerfile\" ># Copyright (c) 2022, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at # https://oss.oracle.com/licenses/upl. # FROM gcr.io/distroless/java17-debian11 ADD coherence.jar /coherence/lib/coherence.jar ENV COHERENCE_HEALTH_HTTP_PORT=6676 HEALTHCHECK --start-period=30s --interval=30s \\ CMD [\"java\", \"-cp\", \"/coherence/lib/coherence.jar\", \"com.tangosol.util.HealthCheckClient\", \"http://127.0.0.1:6676/ready\", \"||\", \"exit\", \"1\"] ENTRYPOINT [\"java\"] CMD [\"-cp\", \"/coherence/lib/*\", \"com.tangosol.net.Coherence\"] The example distroless image can be built using Maven, as before but specifying the distroless Dockerfile. <markup lang=\"bash\" >mvn clean package -DskipTests -Pdocker -Ddocker.file=Distroless.Dockerfile The Maven command builds the same test image with the tag coherence-health:1.0.0 which can be run in the same way as the previous examples. ",
            "title": "Health Checks in Distroless Base Images"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Health checks are extremely useful when running Coherence in containers, as they can signal to the container management system (e.g. Docker, or Kubernetes) that the Coherence container is running and healthy. The OCI specification allows an image to define a command to run to check its health. This is supported by image build tools such as Docker and Buildah. Docker Image Health Checks To use health checks in Docker, the HEALTHCHECK instruction can be used in the Dockerfile . The format of the HEALTHCHECK instruction is shown below: <markup >HEALTHCHECK [OPTIONS] CMD command The command is typically a simple command line, such as curl or a shell script, or Java command line. For example, if the Coherence health check endpoint is enabled on a fixed port 6676 , then the HEALTHCHECK instruction&#8217;s CMD can be set to curl -f http://127.0.0.1:6676/ready An example of a simple Coherence Dockerfile with a health check is shown below. This example image uses OpenJDK as a base image. Coherence jar is added to the image and the health check port fixed to 6676 using the COHERENCE_HEALTH_HTTP_PORT environment variable. When the image runs, the entry point will just start Coherence . <markup title=\"src/docker/OpenJDK.Dockerfile\" ># Copyright (c) 2022, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at # https://oss.oracle.com/licenses/upl. # FROM openjdk:11-jre ADD coherence.jar /coherence/lib/coherence.jar ENV COHERENCE_HEALTH_HTTP_PORT=6676 HEALTHCHECK --start-period=30s --interval=30s \\ CMD curl -f http://127.0.0.1:6676/ready || exit 1 ENTRYPOINT [\"java\"] CMD [\"-cp\", \"/coherence/lib/*\", \"com.tangosol.net.Coherence\"] Build and Run the Image The example above can be built and tested using a simple Maven command. The command will run a Maven build with the docker profile enabled, which will use Docker to build an image. The name of the image is configured in the properties' section of the example pom.xml to be coherence-health:1.0.0 . <markup lang=\"bash\" >mvn clean package -DskipTests -Pdocker A container can then be run using the image. The normal docker run command is used, in this case the container is given the name test . <markup lang=\"bash\" >docker run -d --name test coherence-health:1.0.0 After starting the container, the set of running containers can be listed using docker ps , which should display the test container: <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 2 seconds (health: starting) test Because the image has a health check configured the status in this case included the current health state Up 2 seconds (health: starting) . At this point the container is still starting, so the Coherence health endpoint ( http://127.0.0.1:6676/ready ) has not returned a 200 response, as Coherence is still starting. Once Coherence has started and te health check reports ready, the container status will change to healthy . <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 4 minutes (healthy) test Buildah Image Health Checks The Podman and Buildah tools are common replacements for Docker when running in Linux. When using Buildah to create images, health checks can be added to an image using the Buildah CLI. To support health checks Buildah must be configured to use \"Docker\" format. The simplest way to do this is to export the BUILDAH_FORMAT environment variable <markup lang=\"bash\" >export BUILDAH_FORMAT=docker Now, the Buildah CLI can be used to create an image. <markup lang=\"bash\" >buildah from --name coherence openjdk:11-jre buildah copy coherence coherence.jar /coherence/lib/coherence.jar buildah config --healthcheck-start-period 10s --healthcheck-interval 10s \\ --healthcheck \"CMD curl -f http://127.0.0.1:6676/ready || exit 1\" coherence buildah config \\ --entrypoint '[\"java\"]' --cmd '-cp /coherence/lib/* com.tangosol.net.Coherence' \\ -e COHERENCE_HEALTH_HTTP_PORT=6676 \\ coherence buildah commit coherence coherence-health:1.0.0 buildah push -f v2s2 coherence-health:1.0.0 docker-daemon:coherence-health:1.0.0 The Buildah commands above build the same image that was built with the Dockerfile in the previous section. The final command above pushes the coherence-health:1.0.0 image built by Buildah into a Docker daemon, so it can be run using Docker. Health Checks in Distroless Base Images Sometimes a distroless image is used as a base image for applications. These are images that do not contain a Linux distribution. There are various reasons for this such as image size, but mainly security, as the base image does not contain a lot of Linux utilities that may introduce CVEs. The example Coherence images use distroless base images. When using a distroless base image, the curl utility is not present, so it cannot be used as the health check command. In the distroless base images used by Coherence, all that is present is a Linux kernel and Java. This means that the only way to run any health check commands would be to execute a Java command. As part of the Coherence health check API there is a simple http client class com.tangosol.util.HealthCheckClient that can be used to execute a health check as a Java command. The Java command line to execute a health check would be: <markup lang=\"bash\" >java -cp coherence.jar com.tangosol.util.HealthCheckClient http://127.0.0.1:6676/ready This Distroless.Dockerfile in the source code contains an example of using a Java health check command. Because the health check command is running Java and not a simple O/S command, the format of the CMD parameters is slightly different than the previous example. <markup title=\"src/docker/Distroless.Dockerfile\" ># Copyright (c) 2022, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at # https://oss.oracle.com/licenses/upl. # FROM gcr.io/distroless/java17-debian11 ADD coherence.jar /coherence/lib/coherence.jar ENV COHERENCE_HEALTH_HTTP_PORT=6676 HEALTHCHECK --start-period=30s --interval=30s \\ CMD [\"java\", \"-cp\", \"/coherence/lib/coherence.jar\", \"com.tangosol.util.HealthCheckClient\", \"http://127.0.0.1:6676/ready\", \"||\", \"exit\", \"1\"] ENTRYPOINT [\"java\"] CMD [\"-cp\", \"/coherence/lib/*\", \"com.tangosol.net.Coherence\"] The example distroless image can be built using Maven, as before but specifying the distroless Dockerfile. <markup lang=\"bash\" >mvn clean package -DskipTests -Pdocker -Ddocker.file=Distroless.Dockerfile The Maven command builds the same test image with the tag coherence-health:1.0.0 which can be run in the same way as the previous examples. ",
            "title": "Container Health Checks"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " Coherence MP Metrics provides support for [Eclipse MicroProfile Metrics] ( https://microprofile.io/project/eclipse/microprofile-metrics ) within Coherence cluster members. This is a very simple module that allows you to publish Coherence metrics into MicroProfile Metric Registries available at runtime, and adds Coherence-specific tags to all the metrics published within the process, in order to distinguish them on the monitoring server, such as Prometheus. ",
            "title": "Coherence MicroProfile Metrics"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " In order to use Coherence MP Metrics, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-metrics&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; That&#8217;s it&#8201;&#8212;&#8201;once the module above is in the class path, Coherence will discover MpMetricRegistryAdapter service it provides, and use it to publish all standard Coherence metrics to the vendor registry, and any user-defined application metrics to the application registry. All the metrics will be published as gauges, because they represent point-in-time values of various MBean attributes. ",
            "title": "Usage"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " There could be hundreds of members in a Coherence cluster, with each member publishing potentially the same set of metrics. There could also be many Coherence clusters in the environment, possibly publishing to the same monitoring server instance. In order to distinguish metrics coming from different clusters, as well as from different members of the same cluster, Coherence MP Metrics will automatically add several tags to ALL the metrics published within the process. The tags added are: Tag Name Tag Value cluster the cluster name site the site the member belongs to (if set) machine the machine member is on (if set) member the name of the member (if set) node_id the node ID of the member role the member&#8217;s role This ensures that the metrics published by one member do not collide with and overwrite the metrics published by another members, and allows you to query and aggregate metrics based on the values of the tags above if desired. ",
            "title": "Coherence Global Tags"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " About 15 Minutes IntelliJ IDEA JDK 17 or later ",
            "title": "What You Need"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " If you don’t have IntelliJ IDEA (Ultimate Edition) installed yet, visit the link up above. From there, you can download a copy for your platform. To install it simply unpack the downloaded archive. When you’re done, go ahead and launch IntelliJ IDEA. ",
            "title": "Installing IntelliJ IDEA"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " To import an existing project you need to clone the Coherence CE repository if you have not already done so. <markup lang=\"bash\" >$ git clone https://github.com/oracle/coherence.git ",
            "title": "Clone the Coherence CE Repository"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " With IntelliJ IDEA up and running, click Import Project on the Welcome Screen, or File | New Project From Existing Sources . Navigate to the cloned repository directory, then prj/examples and then to the guide or tutorial directory. For example for Near Caching you would open prj/examples/guides/130-near-caching directory. In the pop-up dialog select either Maven or Gradle as the external model to import from. Once the project is imported, follow the README to build or run the example or guide. ",
            "title": "Importing a Guide or Tutorial"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " IntelliJ IDEA ",
            "title": "See Also"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " You will pick a Coherence guide or example and import it into IntelliJ IDEA. Then you can then read and follow the individual guide or tutorial documentation. What You Need About 15 Minutes IntelliJ IDEA JDK 17 or later Installing IntelliJ IDEA If you don’t have IntelliJ IDEA (Ultimate Edition) installed yet, visit the link up above. From there, you can download a copy for your platform. To install it simply unpack the downloaded archive. When you’re done, go ahead and launch IntelliJ IDEA. Clone the Coherence CE Repository To import an existing project you need to clone the Coherence CE repository if you have not already done so. <markup lang=\"bash\" >$ git clone https://github.com/oracle/coherence.git Importing a Guide or Tutorial With IntelliJ IDEA up and running, click Import Project on the Welcome Screen, or File | New Project From Existing Sources . Navigate to the cloned repository directory, then prj/examples and then to the guide or tutorial directory. For example for Near Caching you would open prj/examples/guides/130-near-caching directory. In the pop-up dialog select either Maven or Gradle as the external model to import from. Once the project is imported, follow the README to build or run the example or guide. See Also IntelliJ IDEA ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " This guide walks you through importing one of the Coherence guides or tutorials into IntelliJ IDEA. What You Will Build You will pick a Coherence guide or example and import it into IntelliJ IDEA. Then you can then read and follow the individual guide or tutorial documentation. What You Need About 15 Minutes IntelliJ IDEA JDK 17 or later Installing IntelliJ IDEA If you don’t have IntelliJ IDEA (Ultimate Edition) installed yet, visit the link up above. From there, you can download a copy for your platform. To install it simply unpack the downloaded archive. When you’re done, go ahead and launch IntelliJ IDEA. Clone the Coherence CE Repository To import an existing project you need to clone the Coherence CE repository if you have not already done so. <markup lang=\"bash\" >$ git clone https://github.com/oracle/coherence.git Importing a Guide or Tutorial With IntelliJ IDEA up and running, click Import Project on the Welcome Screen, or File | New Project From Existing Sources . Navigate to the cloned repository directory, then prj/examples and then to the guide or tutorial directory. For example for Near Caching you would open prj/examples/guides/130-near-caching directory. In the pop-up dialog select either Maven or Gradle as the external model to import from. Once the project is imported, follow the README to build or run the example or guide. See Also IntelliJ IDEA ",
            "title": "Import a Project Into IntelliJ IDEA"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence MP Config provides support for Eclipse MicroProfile Config within Coherence cluster members. It allows you both to configure various Coherence parameters from the values specified in any of the supported config sources, and to use Coherence cache as another, mutable config source. ",
            "title": "Coherence MicroProfile Config"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " In order to use Coherence MP Config, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-config&lt;/artifactId&gt; &lt;version&gt;24.09.1&lt;/version&gt; &lt;/dependency&gt; You will also need an implementation of the Eclipse MP Config specification as a dependency. For example, if you are using Helidon , add the following to your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.config&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-config&lt;/artifactId&gt; &lt;version&gt;4.1.5&lt;/version&gt; &lt;/dependency&gt; &lt;!-- optional: add it if you want YAML config file support --&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.config&lt;/groupId&gt; &lt;artifactId&gt;helidon-config-yaml&lt;/artifactId&gt; &lt;version&gt;4.1.5&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Usage"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence provides a number of configuration properties that can be specified by the users in order to define certain attributes or to customize cluster member behavior at runtime. For example, attributes such as cluster and role name, as well as whether a cluster member should or should not store data, can be specified via system properties: <markup lang=\"xml\" >-Dcoherence.cluster=MyCluster -Dcoherence.role=Proxy -Dcoherence.distributed.localstorage=false Most of these attributes can also be defined within the operational or cache configuration file. For example, you could define first two attributes, cluster name and role, within the operational config override file: <markup lang=\"xml\" > &lt;cluster-config&gt; &lt;member-identity&gt; &lt;cluster-name&gt;MyCluster&lt;/cluster-name&gt; &lt;role-name&gt;Proxy&lt;/role-name&gt; &lt;/member-identity&gt; &lt;/cluster-config&gt; While these two options are more than enough in most cases, there are some issues with them being the only way to configure Coherence: When you are using one of Eclipse MicroProfile implementations, such as Helidon as the foundation of your application, it would be nice to define some of Coherence configuration parameters along with your other configuration parameters, and not in the separate file or via system properties. In some environments, such as Kubernetes, Java system properties are cumbersome to use, and environment variables are a preferred way of passing configuration properties to containers. Unfortunately, neither of the two use cases above is supported out of the box, but that&#8217;s the gap Coherence MP Config is designed to fill. As long as you have coherence-mp-config and an implementation of Eclipse MP Config specification to your class path, Coherence will use any of the standard or custom config sources to resolve various configuration options it understands. Standard config sources in MP Config include META-INF/microprofile-config.properties file, if present in the class path, environment variables, and system properties (in that order, with the properties in the latter overriding the ones from the former). That will directly address problem #2 above, and allow you to specify Coherence configuration options via environment variables within Kubernetes YAML files, for example: <markup lang=\"yaml\" > containers: - name: my-app image: my-company/my-app:1.0.0 env: - name: COHERENCE_CLUSTER value: \"MyCluster\" - name: COHERENCE_ROLE value: \"Proxy\" - name: COHERENCE_DISTRIBUTED_LOCALSTORAGE value: \"false\" Of course, the above is just an example&#8201;&#8212;&#8201;if you are running your Coherence cluster in Kubernetes, you should really be using Coherence Operator instead, as it will make both the configuration and the operation of your Coherence cluster much easier. You will also be able to specify Coherence configuration properties along with the other configuration properties of your application, which will allow you to keep everything in one place, and not scattered across many files. For example, if you are writing a Helidon application, you can simply add coherence section to your application.yaml : <markup lang=\"yaml\" >coherence: cluster: MyCluster role: Proxy distributed: localstorage: false ",
            "title": "Configuring Coherence using MP Config"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence MP Config also provides an implementation of Eclipse MP Config ConfigSource interface, which allows you to store configuration parameters in a Coherence cache. This has several benefits: Unlike pretty much all of the default configuration sources, which are static, configuration options stored in a Coherence cache can be modified without forcing you to rebuild your application JARs or Docker images. You can change the value in one place, and it will automatically be visible and up to date on all the members. While the features above give you incredible amount of flexibility, we also understand that such flexibility is not always desired, and the feature is disabled by default. If you want to enable it, you need to do so explicitly, by registering CoherenceConfigSource as a global interceptor in your cache configuration file: <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;class-name&gt;com.oracle.coherence.mp.config.CoherenceConfigSource&lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;!-- your cache mappings and schemes... --&gt; &lt;/cache-config&gt; Once you do that, CoherenceConfigSource will be activated as soon as your cache factory is initialized, and injected into the list of available config sources for your application to use via standard MP Config APIs. By default, it will be configured with a priority (ordinal) of 500, making it higher priority than all the standard config sources, thus allowing you to override the values provided via config files, environment variables and system properties. However, you have full control over that behavior and can specify different ordinal via coherence.mp.config.source.ordinal configuration property. Note It should be obvious, but it&#8217;s worth pointing out that you cannot use Coherence cache as a config source for properties such as the one above, or for any other configuration property required during Coherence startup and initialization. This feature is primarily intended for easier management of application configuration options that are a) not needed during application startup, and b) would benefit from being mutable at runtime. ",
            "title": "Using Coherence Cache as a Config Source"
        }
 ]
}